[[answer]The speed of light in vacuum, commonly denoted , is a universal physical constant important in many areas of physics. Its value is exactly 299,792,458 metres per second, a figure that is exact because the length of the metre is defined from this constant and the international standard for time. This is, to three significant figures, 186,000 miles per second, or about 671 million miles per hour. According to special relativity, is the maximum speed at which all energy, matter, and information in the universe can travel. It is the speed at which all massless particles and associated fields (including electromagnetic radiation such as light) travel in vacuum. It is also the speed of gravity (i.e. of gravitational waves) predicted by current theories. Such particles and waves travel at regardless of the motion of the source or the inertial frame of reference of the observer. In the theory of relativity, interrelates space and time, and also appears in the famous equation of mass–energy equivalence  = . The speed at which light propagates through transparent materials, such as glass or air, is less than . The ratio between and the speed at which light travels in a material is called the refractive index of the material ( =  / ). For example, for visible light the refractive index of glass is typically around 1.5, meaning that light in glass travels at ; the refractive index of air for visible light is 1.000293, so the speed of light in air is or about slower than . In most practical cases, light and other electromagnetic waves can be thought of as moving "instantaneously", but for long distances and very sensitive measurements their finite speed has noticeable effects. For example, in videos of an intense lightning storm on the Earth's surface taken from the International Space Station, the expansion of light wavefronts from individual flashes of lightning is clearly visible, and allows estimates of the speed of light to be made from frame-to-frame analysis of the position of the light wavefront. This is not surprising, as the time for light to propagate completely around the Earth is of the order of 140 milliseconds. This transit time is what causes the Schumann resonance. In communicating with distant space probes, it can take minutes to hours for a message to get from Earth to the spacecraft, or vice versa. The light we see from stars left them many years ago, allowing us to study the history of the universe by looking at distant objects. The finite speed of light also limits the theoretical maximum speed of computers, since information must be sent within the computer from chip to chip. Finally, the speed of light can be used with time of flight measurements to measure large distances to high precision. Ole Rømer first demonstrated in 1676 that light travelled at a finite speed (as opposed to instantaneously) by studying the apparent motion of Jupiter's moon Io. In 1865, James Clerk Maxwell proposed that light was an electromagnetic wave, and therefore travelled at the speed appearing in his theory of electromagnetism. In 1905, Albert Einstein postulated that the speed of light with respect to any inertial frame is independent of the motion of the light source, and explored the consequences of that postulate by deriving the special theory of relativity and showing that the parameter had relevance outside of the context of light and electromagnetism. After centuries of increasingly precise measurements, in 1975 the speed of light was known to be with a measurement uncertainty of 4 parts per billion. In 1983, the metre was redefined in the International System of Units (SI) as the distance travelled by light in vacuum in 1/299,792,458 of a second. As a result, the numerical value of in metres per second is now fixed exactly by the definition of the metre. Numerical value, notation, and units. The speed of light in vacuum is usually denoted by "c", for "constant" or the Latin (meaning "swiftness"). (Capital C is the SI unit for coulomb of electric charge.) Originally, the symbol "V" was used for the speed of light, introduced by James Clerk Maxwell in 1865. In 1856, Wilhelm Eduard Weber and Rudolf Kohlrausch had used "c" for a different constant later shown to equal times the speed of light in vacuum. In 1894, Paul Drude redefined "c" with its modern meaning. Einstein used "V" in his original German-language papers on special relativity in 1905, but in 1907 he switched to "c", which by then had become the standard symbol. Sometimes "c" is used for the speed of waves in "any" material medium, and "c"0 for the speed of light in vacuum. This subscripted notation, which is endorsed in official SI literature, has the same form as other related constants: namely, "μ"0 for the vacuum permeability or magnetic constant, "ε"0 for the vacuum permittivity or electric constant, and "Z"0 for the impedance of free space. This article uses "c" exclusively for the speed of light in vacuum. Since 1983, the metre has been defined in the International System of Units (SI) as the distance light travels in vacuum in 1/ of a second. This definition fixes the speed of light in vacuum at exactly . As a dimensional physical constant, the numerical value of "c" is different for different unit systems. In branches of physics in which "c" appears often, such as in relativity, it is common to use systems of natural units of measurement or the geometrized unit system where . Using these units, "c" does not appear explicitly because multiplication or division by 1 does not affect the result. Fundamental role in physics. The speed at which light waves propagate in vacuum is independent both of the motion of the wave source and of the inertial frame of reference of the observer. This invariance of the speed of light was postulated by Einstein in 1905, after being motivated by Maxwell's theory of electromagnetism and the lack of evidence for the luminiferous aether; it has since been consistently confirmed by many experiments. It is only possible to verify experimentally that the two-way speed of light (for example, from a source to a mirror and back again) is frame-independent, because it is impossible to measure the one-way speed of light (for example, from a source to a distant detector) without some convention as to how clocks at the source and at the detector should be synchronized. However, by adopting Einstein synchronization for the clocks, the one-way speed of light becomes equal to the two-way speed of light by definition. The special theory of relativity explores the consequences of this invariance of "c" with the assumption that the laws of physics are the same in all inertial frames of reference. One consequence is that "c" is the speed at which all massless particles and waves, including light, must travel in vacuum. Special relativity has many counterintuitive and experimentally verified implications. These include the equivalence of mass and energy , length contraction (moving objects shorten), and time dilation (moving clocks run more slowly). The factor "γ" by which lengths contract and times dilate is known as the Lorentz factor and is given by , where "v" is the speed of the object. The difference of "γ" from 1 is negligible for speeds much slower than "c", such as most everyday speeds—in which case special relativity is closely approximated by Galilean relativity—but it increases at relativistic speeds and diverges to infinity as "v" approaches "c". The results of special relativity can be summarized by treating space and time as a unified structure known as spacetime (with "c" relating the units of space and time), and requiring that physical theories satisfy a special symmetry called Lorentz invariance, whose mathematical formulation contains the parameter "c". Lorentz invariance is an almost universal assumption for modern physical theories, such as quantum electrodynamics, quantum chromodynamics, the Standard Model of particle physics, and general relativity. As such, the parameter "c" is ubiquitous in modern physics, appearing in many contexts that are unrelated to light. For example, general relativity predicts that "c" is also the speed of gravity and of gravitational waves. In non-inertial frames of reference (gravitationally curved space or accelerated reference frames), the "local" speed of light is constant and equal to "c", but the speed of light along a trajectory of finite length can differ from "c", depending on how distances and times are defined. It is generally assumed that fundamental constants such as "c" have the same value throughout spacetime, meaning that they do not depend on location and do not vary with time. However, it has been suggested in various theories that the speed of light may have changed over time. No conclusive evidence for such changes has been found, but they remain the subject of ongoing research. It also is generally assumed that the speed of light is isotropic, meaning that it has the same value regardless of the direction in which it is measured. Observations of the emissions from nuclear energy levels as a function of the orientation of the emitting nuclei in a magnetic field (see Hughes–Drever experiment), and of rotating optical resonators (see Resonator experiments) have put stringent limits on the possible two-way anisotropy. Upper limit on speeds. According to special relativity, the energy of an object with rest mass "m" and speed "v" is given by , where "γ" is the Lorentz factor defined above. When "v" is zero, "γ" is equal to one, giving rise to the famous formula for mass-energy equivalence. The "γ" factor approaches infinity as "v" approaches "c", and it would take an infinite amount of energy to accelerate an object with mass to the speed of light. The speed of light is the upper limit for the speeds of objects with positive rest mass. This is experimentally established in many tests of relativistic energy and momentum. More generally, it is normally impossible for information or energy to travel faster than "c". One argument for this follows from the counter-intuitive implication of special relativity known as the relativity of simultaneity. If the spatial distance between two events A and B is greater than the time interval between them multiplied by "c" then there are frames of reference in which A precedes B, others in which B precedes A, and others in which they are simultaneous. As a result, if something were travelling faster than "c" relative to an inertial frame of reference, it would be travelling backwards in time relative to another frame, and causality would be violated. In such a frame of reference, an "effect" could be observed before its "cause". Such a violation of causality has never been recorded, and would lead to paradoxes such as the tachyonic antitelephone. Faster-than-light observations and experiments. There are situations in which it may seem that matter, energy, or information travels at speeds greater than "c", but they do not. For example, as is discussed in the propagation of light in a medium section below, many wave velocities can exceed "c". For example, the phase velocity of X-rays through most glasses can routinely exceed "c", but phase velocity does not determine the velocity at which waves convey information. If a laser beam is swept quickly across a distant object, the spot of light can move faster than "c", although the initial movement of the spot is delayed because of the time it takes light to get to the distant object at the speed "c". However, the only physical entities that are moving are the laser and its emitted light, which travels at the speed "c" from the laser to the various positions of the spot. Similarly, a shadow projected onto a distant object can be made to move faster than "c", after a delay in time. In neither case does any matter, energy, or information travel faster than light. The rate of change in the distance between two objects in a frame of reference with respect to which both are moving (their closing speed) may have a value in excess of "c". However, this does not represent the speed of any single object as measured in a single inertial frame. Certain quantum effects appear to be transmitted instantaneously and therefore faster than "c", as in the EPR paradox. An example involves the quantum states of two particles that can be entangled. Until either of the particles is observed, they exist in a superposition of two quantum states. If the particles are separated and one particle's quantum state is observed, the other particle's quantum state is determined instantaneously (i.e., faster than light could travel from one particle to the other). However, it is impossible to control which quantum state the first particle will take on when it is observed, so information cannot be transmitted in this manner. Another quantum effect that predicts the occurrence of faster-than-light speeds is called the Hartman effect; under certain conditions the time needed for a virtual particle to tunnel through a barrier is constant, regardless of the thickness of the barrier. This could result in a virtual particle crossing a large gap faster-than-light. However, no information can be sent using this effect. So-called superluminal motion is seen in certain astronomical objects, such as the relativistic jets of radio galaxies and quasars. However, these jets are not moving at speeds in excess of the speed of light: the apparent superluminal motion is a projection effect caused by objects moving near the speed of light and approaching Earth at a small angle to the line of sight: since the light which was emitted when the jet was farther away took longer to reach the Earth, the time between two successive observations corresponds to a longer time between the instants at which the light rays were emitted. In models of the expanding universe, the farther galaxies are from each other, the faster they drift apart. This receding is not due to motion "through" space, but rather to the expansion of space itself. For example, galaxies far away from Earth appear to be moving away from the Earth with a speed proportional to their distances. Beyond a boundary called the Hubble sphere, the rate at which their distance from Earth increases becomes greater than the speed of light. In September 2011, physicists working on the OPERA experiment published results that suggested beams of neutrinos had travelled from CERN (in Geneva, Switzerland) to LNGS (at the Gran Sasso, Italy) faster than the speed of light. These findings, sometimes referred to as the faster-than-light neutrino anomaly, were subsequently determined—subject to further confirmation—to be the result of a measurement error. Propagation of light. In classical physics, light is described as a type of electromagnetic wave. The classical behaviour of the electromagnetic field is described by Maxwell's equations, which predict that the speed "c" with which electromagnetic waves (such as light) propagate through the vacuum is related to the electric constant "ε"0 and the magnetic constant "μ"0 by the equation . In modern quantum physics, the electromagnetic field is described by the theory of quantum electrodynamics (QED). In this theory, light is described by the fundamental excitations (or quanta) of the electromagnetic field, called photons. In QED, photons are massless particles and thus, according to special relativity, they travel at the speed of light in vacuum. Extensions of QED in which the photon has a mass have been considered. In such a theory, its speed would depend on its frequency, and the invariant speed "c" of special relativity would then be the upper limit of the speed of light in vacuum. No variation of the speed of light with frequency has been observed in rigorous testing, putting stringent limits on the mass of the photon. The limit obtained depends on the model used: if the massive photon is described by Proca theory, the experimental upper bound for its mass is about 10−57 grams; if photon mass is generated by a Higgs mechanism, the experimental upper limit is less sharp,   (roughly 2 × 10−47 g). Another reason for the speed of light to vary with its frequency would be the failure of special relativity to apply to arbitrarily small scales, as predicted by some proposed theories of quantum gravity. In 2009, the observation of the spectrum of gamma-ray burst GRB 090510 did not find any difference in the speeds of photons of different energies, confirming that Lorentz invariance is verified at least down to the scale of the Planck length ("l"P =  ≈ ) divided by 1.2. In a medium. In a medium, light usually does not propagate at a speed equal to "c"; further, different types of light wave will travel at different speeds. The speed at which the individual crests and troughs of a plane wave (a wave filling the whole space, with only one frequency) propagate is called the phase velocity "v"p. An actual physical signal with a finite extent (a pulse of light) travels at a different speed. The largest part of the pulse travels at the group velocity "v"g, and its earliest part travels at the front velocity "v"f. The phase velocity is important in determining how a light wave travels through a material or from one material to another. It is often represented in terms of a "refractive index". The refractive index of a material is defined as the ratio of "c" to the phase velocity "v"p in the material: larger indices of refraction indicate lower speeds. The refractive index of a material may depend on the light's frequency, intensity, polarization, or direction of propagation; in many cases, though, it can be treated as a material-dependent constant. The refractive index of air is approximately 1.0003. Denser media, such as water, glass, and diamond, have refractive indexes of around 1.3, 1.5 and 2.4, respectively, for visible light. In exotic materials like Bose–Einstein condensates near absolute zero, the effective speed of light may be only a few metres per second. However, this represents absorption and re-radiation delay between atoms, as do all slower-than-c speeds in material substances. As an extreme example of this, light "slowing" in matter, two independent teams of physicists claimed to bring light to a "complete standstill" by passing it through a Bose–Einstein Condensate of the element rubidium, one team at Harvard University and the Rowland Institute for Science in Cambridge, Mass., and the other at the Harvard–Smithsonian Center for Astrophysics, also in Cambridge. However, the popular description of light being "stopped" in these experiments refers only to light being stored in the excited states of atoms, then re-emitted at an arbitrarily later time, as stimulated by a second laser pulse. During the time it had "stopped," it had ceased to be light. This type of behaviour is generally microscopically true of all transparent media which "slow" the speed of light. In transparent materials, the refractive index generally is greater than 1, meaning that the phase velocity is less than "c". In other materials, it is possible for the refractive index to become smaller than 1 for some frequencies; in some exotic materials it is even possible for the index of refraction to become negative. The requirement that causality is not violated implies that the real and imaginary parts of the dielectric constant of any material, corresponding respectively to the index of refraction and to the attenuation coefficient, are linked by the Kramers–Kronig relations. In practical terms, this means that in a material with refractive index less than 1, the absorption of the wave is so quick that no signal can be sent faster than "c". A pulse with different group and phase velocities (which occurs if the phase velocity is not the same for all the frequencies of the pulse) smears out over time, a process known as dispersion. Certain materials have an exceptionally low (or even zero) group velocity for light waves, a phenomenon called slow light, which has been confirmed in various experiments. The opposite, group velocities exceeding "c", has also been shown in experiment. It should even be possible for the group velocity to become infinite or negative, with pulses travelling instantaneously or backwards in time. None of these options, however, allow information to be transmitted faster than "c". It is impossible to transmit information with a light pulse any faster than the speed of the earliest part of the pulse (the front velocity). It can be shown that this is (under certain assumptions) always equal to "c". It is possible for a particle to travel through a medium faster than the phase velocity of light in that medium (but still slower than "c"). When a charged particle does that in a dielectric material, the electromagnetic equivalent of a shock wave, known as Cherenkov radiation, is emitted. Practical effects of finiteness. The speed of light is of relevance to communications: the one-way and round-trip delay time are greater than zero. This applies from small to astronomical scales. On the other hand, some techniques depend on the finite speed of light, for example in distance measurements. Small scales. In supercomputers, the speed of light imposes a limit on how quickly data can be sent between processors. If a processor operates at 1 gigahertz, a signal can only travel a maximum of about in a single cycle. Processors must therefore be placed close to each other to minimize communication latencies; this can cause difficulty with cooling. If clock frequencies continue to increase, the speed of light will eventually become a limiting factor for the internal design of single chips. Large distances on Earth. For example, given the equatorial circumference of the Earth is about and "c" about , the theoretical shortest time for a piece of information to travel half the globe along the surface is about 67 milliseconds. When light is travelling around the globe in an optical fibre, the actual transit time is longer, in part because the speed of light is slower by about 35% in an optical fibre, depending on its refractive index "n". Furthermore, straight lines rarely occur in global communications situations, and delays are created when the signal passes through an electronic switch or signal regenerator. Spaceflights and astronomy. Similarly, communications between the Earth and spacecraft are not instantaneous. There is a brief delay from the source to the receiver, which becomes more noticeable as distances increase. This delay was significant for communications between ground control and Apollo 8 when it became the first manned spacecraft to orbit the Moon: for every question, the ground control station had to wait at least three seconds for the answer to arrive. The communications delay between Earth and Mars can vary between five and twenty minutes depending upon the relative positions of the two planets. As a consequence of this, if a robot on the surface of Mars were to encounter a problem, its human controllers would not be aware of it until at least five minutes later, and possibly up to twenty minutes later; it would then take a further five to twenty minutes for instructions to travel from Earth to Mars. NASA must wait several hours for information from a probe orbiting Jupiter, and if it needs to correct a navigation error, the fix will not arrive at the spacecraft for an equal amount of time, creating a risk of the correction not arriving in time. Receiving light and other signals from distant astronomical sources can even take much longer. For example, it has taken 13 billion (13) years for light to travel to Earth from the faraway galaxies viewed in the Hubble Ultra Deep Field images. Those photographs, taken today, capture images of the galaxies as they appeared 13 billion years ago, when the universe was less than a billion years old. The fact that more distant objects appear to be younger, due to the finite speed of light, allows astronomers to infer the evolution of stars, of galaxies, and of the universe itself. Astronomical distances are sometimes expressed in light-years, especially in popular science publications and media. A light-year is the distance light travels in one year, around 9461 billion kilometres, 5879 billion miles, or 0.3066 parsecs. In round figures, a light year is nearly 10 trillion kilometres or nearly 6 trillion miles. Proxima Centauri, the closest star to Earth after the Sun, is around 4.2 light-years away. Distance measurement. Radar systems measure the distance to a target by the time it takes a radio-wave pulse to return to the radar antenna after being reflected by the target: the distance to the target is half the round-trip transit time multiplied by the speed of light. A Global Positioning System (GPS) receiver measures its distance to GPS satellites based on how long it takes for a radio signal to arrive from each satellite, and from these distances calculates the receiver's position. Because light travels about 300,000 kilometres (186,000 miles) in one second, these measurements of small fractions of a second must be very precise. The Lunar Laser Ranging Experiment, radar astronomy and the Deep Space Network determine distances to the Moon, planets and spacecraft, respectively, by measuring round-trip transit times. Measurement. There are different ways to determine the value of "c". One way is to measure the actual speed at which light waves propagate, which can be done in various astronomical and earth-based setups. However, it is also possible to determine "c" from other physical laws where it appears, for example, by determining the values of the electromagnetic constants "ε"0 and "μ"0 and using their relation to "c". Historically, the most accurate results have been obtained by separately determining the frequency and wavelength of a light beam, with their product equalling "c". In 1983 the metre was defined as "the length of the path travelled by light in vacuum during a time interval of 1⁄299,792,458 of a second", fixing the value of the speed of light at by definition, as described below. Consequently, accurate measurements of the speed of light yield an accurate realization of the metre rather than an accurate value of "c". Astronomical measurements. Outer space is a natural setting for measuring the speed of light because of its large scale and nearly perfect vacuum. Typically, one measures the time needed for light to traverse some reference distance in the solar system, such as the radius of the Earth's orbit. Historically, such measurements could be made fairly accurately, compared to how accurately the length of the reference distance is known in Earth-based units. It is customary to express the results in astronomical units (AU) per day. An astronomical unit is approximately the average distance between the Earth and Sun; it is not based on the International System of Units. Because the AU determines an actual length, and is not based upon time-of-flight like the SI units, modern measurements of the speed of light in astronomical units per day can be compared with the defined value of "c" in the International System of Units. Ole Christensen Rømer used an astronomical measurement to make the first quantitative estimate of the speed of light.Translated in (As reproduced in )<br> The account published in "Journal des sçavans" was based on a report that Rømer read to the French Academy of Sciences in November 1676 (Cohen, 1940, p. 346).</ref> When measured from Earth, the periods of moons orbiting a distant planet are shorter when the Earth is approaching the planet than when the Earth is receding from it. The distance travelled by light from the planet (or its moon) to Earth is shorter when the Earth is at the point in its orbit that is closest to its planet than when the Earth is at the farthest point in its orbit, the difference in distance being the diameter of the Earth's orbit around the Sun. The observed change in the moon's orbital period is actually the difference in the time it takes light to traverse the shorter or longer distance. Rømer observed this effect for Jupiter's innermost moon Io and deduced that light takes 22 minutes to cross the diameter of the Earth's orbit. Another method is to use the aberration of light, discovered and explained by James Bradley in the 18th century. This effect results from the vector addition of the velocity of light arriving from a distant source (such as a star) and the velocity of its observer (see diagram on the right). A moving observer thus sees the light coming from a slightly different direction and consequently sees the source at a position shifted from its original position. Since the direction of the Earth's velocity changes continuously as the Earth orbits the Sun, this effect causes the apparent position of stars to move around. From the angular difference in the position of stars (maximally 20.5 arcseconds) it is possible to express the speed of light in terms of the Earth's velocity around the Sun, which with the known length of a year can be converted to the time needed to travel from the Sun to the Earth. In 1729, Bradley used this method to derive that light travelled 10,210 times faster than the Earth in its orbit (the modern figure is 10,066 times faster) or, equivalently, that it would take light 8 minutes 12 seconds to travel from the Sun to the Earth. Nowadays, the "light time for unit distance"—the inverse of "c", expressed in seconds per astronomical unit—is measured by comparing the time for radio signals to reach different spacecraft in the Solar System, with their position calculated from the gravitational effects of the Sun and various planets. By combining many such measurements, a best fit value for the light time per unit distance is obtained. , the best estimate, as approved by the International Astronomical Union (IAU), is: The relative uncertainty in these measurements is 0.02 parts per billion (2), equivalent to the uncertainty in Earth-based measurements of length by interferometry. Since the metre is defined to be the length travelled by light in a certain time interval, the measurement of the light time for unit distance can also be interpreted as measuring the length of an AU in metres. Time of flight techniques. A method of measuring the speed of light is to measure the time needed for light to travel to a mirror at a known distance and back. This is the working principle behind the Fizeau–Foucault apparatus developed by Hippolyte Fizeau and Léon Foucault. The setup as used by Fizeau consists of a beam of light directed at a mirror away. On the way from the source to the mirror, the beam passes through a rotating cogwheel. At a certain rate of rotation, the beam passes through one gap on the way out and another on the way back, but at slightly higher or lower rates, the beam strikes a tooth and does not pass through the wheel. Knowing the distance between the wheel and the mirror, the number of teeth on the wheel, and the rate of rotation, the speed of light can be calculated. The method of Foucault replaces the cogwheel by a rotating mirror. Because the mirror keeps rotating while the light travels to the distant mirror and back, the light is reflected from the rotating mirror at a different angle on its way out than it is on its way back. From this difference in angle, the known speed of rotation and the distance to the distant mirror the speed of light may be calculated. Nowadays, using oscilloscopes with time resolutions of less than one nanosecond, the speed of light can be directly measured by timing the delay of a light pulse from a laser or an LED reflected from a mirror. This method is less precise (with errors of the order of 1%) than other modern techniques, but it is sometimes used as a laboratory experiment in college physics classes. Electromagnetic constants. An option for deriving "c" that does not directly depend on a measurement of the propagation of electromagnetic waves is to use the relation between "c" and the vacuum permittivity "ε"0 and vacuum permeability "μ"0 established by Maxwell's theory: "c"2 = 1/("ε"0"μ"0). The vacuum permittivity may be determined by measuring the capacitance and dimensions of a capacitor, whereas the value of the vacuum permeability is fixed at exactly through the definition of the ampere. Rosa and Dorsey used this method in 1907 to find a value of . Cavity resonance. Another way to measure the speed of light is to independently measure the frequency "f" and wavelength "λ" of an electromagnetic wave in vacuum. The value of "c" can then be found by using the relation "c" = "fλ". One option is to measure the resonance frequency of a cavity resonator. If the dimensions of the resonance cavity are also known, these can be used determine the wavelength of the wave. In 1946, Louis Essen and A.C. Gordon-Smith established the frequency for a variety of normal modes of microwaves of a microwave cavity of precisely known dimensions. The dimensions were established to an accuracy of about ±0.8 μm using gauges calibrated by interferometry. As the wavelength of the modes was known from the geometry of the cavity and from electromagnetic theory, knowledge of the associated frequencies enabled a calculation of the speed of light. The Essen–Gordon-Smith result, , was substantially more precise than those found by optical techniques. By 1950, repeated measurements by Essen established a result of . A household demonstration of this technique is possible, using a microwave oven and food such as marshmallows or margarine: if the turntable is removed so that the food does not move, it will cook the fastest at the antinodes (the points at which the wave amplitude is the greatest), where it will begin to melt. The distance between two such spots is half the wavelength of the microwaves; by measuring this distance and multiplying the wavelength by the microwave frequency (usually displayed on the back of the oven, typically 2450 MHz), the value of "c" can be calculated, "often with less than 5% error". Interferometry. Interferometry is another method to find the wavelength of electromagnetic radiation for determining the speed of light. A coherent beam of light (e.g. from a laser), with a known frequency ("f"), is split to follow two paths and then recombined. By adjusting the path length while observing the interference pattern and carefully measuring the change in path length, the wavelength of the light ("λ") can be determined. The speed of light is then calculated using the equation "c" = "λf". Before the advent of laser technology, coherent radio sources were used for interferometry measurements of the speed of light. However interferometric determination of wavelength becomes less precise with wavelength and the experiments were thus limited in precision by the long wavelength (~0.4 cm) of the radiowaves. The precision can be improved by using light with a shorter wavelength, but then it becomes difficult to directly measure the frequency of the light. One way around this problem is to start with a low frequency signal of which the frequency can be precisely measured, and from this signal progressively synthesize higher frequency signals whose frequency can then be linked to the original signal. A laser can then be locked to the frequency, and its wavelength can be determined using interferometry. This technique was due to a group at the National Bureau of Standards (NBS) (which later became NIST). They used it in 1972 to measure the speed of light in vacuum with a fractional uncertainty of . History. Until the early modern period, it was not known whether light travelled instantaneously or at a very fast finite speed. The first extant recorded examination of this subject was in ancient Greece. The ancient Greeks, Muslim scholars and classical European scientists long debated this until Rømer provided the first calculation of the speed of light. Einstein's Theory of Special Relativity concluded that the speed of light is constant regardless of one's frame of reference. Since then, scientists have provided increasingly accurate measurements. Early history. Empedocles was the first to claim that light has a finite speed. He maintained that light was something in motion, and therefore must take some time to travel. Aristotle argued, to the contrary, that "light is due to the presence of something, but it is not a movement". Euclid and Ptolemy advanced the emission theory of vision, where light is emitted from the eye, thus enabling sight. Based on that theory, Heron of Alexandria argued that the speed of light must be infinite because distant objects such as stars appear immediately upon opening the eyes. Early Islamic philosophers initially agreed with the Aristotelian view that light had no speed of travel. In 1021, Alhazen (Ibn al-Haytham) published the "Book of Optics", in which he presented a series of arguments dismissing the emission theory in favour of the now accepted intromission theory of vision, in which light moves from an object into the eye. This led Alhazen to propose that light must have a finite speed, and that the speed of light is variable, decreasing in denser bodies. He argued that light is substantial matter, the propagation of which requires time, even if this is hidden from our senses. Also in the 11th century, Abū Rayhān al-Bīrūnī agreed that light has a finite speed, and observed that the speed of light is much faster than the speed of sound. In the 13th century, Roger Bacon argued that the speed of light in air was not infinite, using philosophical arguments backed by the writing of Alhazen and Aristotle. In the 1270s, Witelo considered the possibility of light travelling at infinite speed in vacuum, but slowing down in denser bodies. In the early 17th century, Johannes Kepler believed that the speed of light was infinite, since empty space presents no obstacle to it. René Descartes argued that if the speed of light were finite, the Sun, Earth, and Moon would be noticeably out of alignment during a lunar eclipse. Since such misalignment had not been observed, Descartes concluded the speed of light was infinite. Descartes speculated that if the speed of light were found to be finite, his whole system of philosophy might be demolished. In Descartes' derivation of Snell's law, he assumed that even though the speed of light was instantaneous, the more dense the medium, the faster was light's speed. Pierre de Fermat derived Snell's law using the opposing assumption, the more dense the medium the slower light traveled. Fermat also argued in support of a finite speed of light. First measurement attempts. In 1629, Isaac Beeckman proposed an experiment in which a person observes the flash of a cannon reflecting off a mirror about one mile (1.6 km) away. In 1638, Galileo Galilei proposed an experiment, with an apparent claim to having performed it some years earlier, to measure the speed of light by observing the delay between uncovering a lantern and its perception some distance away. He was unable to distinguish whether light travel was instantaneous or not, but concluded that if it were not, it must nevertheless be extraordinarily rapid. Galileo's experiment was carried out by the Accademia del Cimento of Florence, Italy, in 1667, with the lanterns separated by about one mile, but no delay was observed. The actual delay in this experiment would have been about 11 microseconds. The first quantitative estimate of the speed of light was made in 1676 by Rømer (see Rømer's determination of the speed of light). From the observation that the periods of Jupiter's innermost moon Io appeared to be shorter when the Earth was approaching Jupiter than when receding from it, he concluded that light travels at a finite speed, and estimated that it takes light 22 minutes to cross the diameter of Earth's orbit. Christiaan Huygens combined this estimate with an estimate for the diameter of the Earth's orbit to obtain an estimate of speed of light of , 26% lower than the actual value. In his 1704 book "Opticks", Isaac Newton reported Rømer's calculations of the finite speed of light and gave a value of "seven or eight minutes" for the time taken for light to travel from the Sun to the Earth (the modern value is 8 minutes 19 seconds). Newton queried whether Rømer's eclipse shadows were coloured; hearing that they were not, he concluded the different colours travelled at the same speed. In 1729, James Bradley discovered the aberration of light. From this effect he determined that light must travel 10,210 times faster than the Earth in its orbit (the modern figure is 10,066 times faster) or, equivalently, that it would take light 8 minutes 12 seconds to travel from the Sun to the Earth. Connections with electromagnetism. In the 19th century Hippolyte Fizeau developed a method to determine the speed of light based on time-of-flight measurements on Earth and reported a value of . His method was improved upon by Léon Foucault who obtained a value of in 1862. In the year 1856, Wilhelm Eduard Weber and Rudolf Kohlrausch measured the ratio of the electromagnetic and electrostatic units of charge, 1/√"ε"0"μ"0, by discharging a Leyden jar, and found that its numerical value was very close to the speed of light as measured directly by Fizeau. The following year Gustav Kirchhoff calculated that an electric signal in a resistanceless wire travels along the wire at this speed. In the early 1860s, Maxwell showed that, according to the theory of electromagnetism he was working on, electromagnetic waves propagate in empty space at a speed equal to the above Weber/Kohrausch ratio, and drawing attention to the numerical proximity of this value to the speed of light as measured by Fizeau, he proposed that light is in fact an electromagnetic wave. "Luminiferous aether". It was thought at the time that empty space was filled with a background medium called the luminiferous aether in which the electromagnetic field existed. Some physicists thought that this aether acted as a preferred frame of reference for the propagation of light and therefore it should be possible to measure the motion of the Earth with respect to this medium, by measuring the isotropy of the speed of light. Beginning in the 1880s several experiments were performed to try to detect this motion, the most famous of which is the experiment performed by Albert Michelson and Edward Morley in 1887. The detected motion was always less than the observational error. Modern experiments indicate that the two-way speed of light is isotropic (the same in every direction) to within 6 nanometres per second. Because of this experiment Hendrik Lorentz proposed that the motion of the apparatus through the aether may cause the apparatus to contract along its length in the direction of motion, and he further assumed, that the time variable for moving systems must also be changed accordingly ("local time"), which led to the formulation of the Lorentz transformation. Based on Lorentz's aether theory, Henri Poincaré (1900) showed that this local time (to first order in v/c) is indicated by clocks moving in the aether, which are synchronized under the assumption of constant light speed. In 1904, he speculated that the speed of light could be a limiting velocity in dynamics, provided that the assumptions of Lorentz's theory are all confirmed. In 1905, Poincaré brought Lorentz's aether theory into full observational agreement with the principle of relativity. Special relativity. In 1905 Einstein postulated from the outset that the speed of light in vacuum, measured by a non-accelerating observer, is independent of the motion of the source or observer. Using this and the principle of relativity as a basis he derived the special theory of relativity, in which the speed of light in vacuum "c" featured as a fundamental constant, also appearing in contexts unrelated to light. This made the concept of the stationary aether (to which Lorentz and Poincaré still adhered) useless and revolutionized the concepts of space and time. Increased accuracy of "c" and redefinition of the metre and second. In the second half of the 20th century much progress was made in increasing the accuracy of measurements of the speed of light, first by cavity resonance techniques and later by laser interferometer techniques. These were aided by new, more precise, definitions of the metre and second. In 1960, the metre was redefined in terms of the wavelength of a particular spectral line of krypton-86, and, in 1967, the second was redefined in terms of the hyperfine transition frequency of the ground state of caesium-133. In 1972, using the laser interferometer method and the new definitions, a group at NBS in Boulder, Colorado determined the speed of light in vacuum to be "c" = . This was 100 times less uncertain than the previously accepted value. The remaining uncertainty was mainly related to the definition of the metre. As similar experiments found comparable results for "c", the 15th Conférence Générale des Poids et Mesures (CGPM) in 1975 recommended using the value for the speed of light. Defining the speed of light as an explicit constant. In 1983 the 17th CGPM found that wavelengths from frequency measurements and a given value for the speed of light are more reproducible than the previous standard. They kept the 1967 standard for time, so the Caesium hyperfine frequency would now determine both the second and the metre. To do this, they redefined the metre thus, "The metre is the length of the path travelled by light in vacuum during a time interval of 1/299 792 458 of a second." As a result of this definition, the value of the speed of light in vacuum is exactly and has become a defined constant in the SI system of units. Improved experimental techniques that prior to 1983 would have measured the speed of light, no longer affect the value of the speed of light in SI units, but instead allow a more precise realization of the metre by more accurately measuring the wavelength of Krypton-86 and other light sources. In 2011, the CGPM said it intends to define all seven SI base units using what it calls "the explicit-constant formulation", where each "unit is defined indirectly by specifying explicitly an exact value for a well-recognized fundamental constant", as was done for the speed of light. It proposed a new, but completely equivalent, wording of the metre's definition: "The metre, symbol m, is the unit of length; its magnitude is set by fixing the numerical value of the speed of light in vacuum to be equal to exactly 299 792 458 when it is expressed in the SI unit m s–1 [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@244efc35 [docID]28736 [hitPos]0 [correct]false [extraScores][F@418becc5 , [answer]When using the term 'the speed of light' it is sometimes necessary to make the distinction between its one-way speed and its two-way speed. The "one-way" speed of light from a source to a detector, cannot be measured independently of a convention as to how to synchronize the clocks at the source and the detector. What can however be experimentally measured is the round-trip speed (or "two-way" speed of light) from the source to the detector and back again. Albert Einstein chose a synchronization convention (see Einstein synchronization) that made the one-way speed equal to the two-way speed. The constancy of the one-way speed in any given inertial frame, is the basis of his special theory of relativity although all experimentally verifiable predictions of this theory do not depend on that convention. Experiments that attempted to directly probe the one-way speed of light independent of synchronization have been proposed, but none has succeeded in doing so. Those experiments directly establish that synchronization with slow clock-transport is equivalent to Einstein synchronization, which is an important feature of special relativity. Though those experiments don't directly establish the isotropy of the one-way speed of light, because it was shown that slow clock-transport, the laws of motion, and the way inertial reference frames are defined, already involve the assumption of isotropic one-way speeds and thus are conventional as well. In general, it was shown that these experiments are consistent with anisotropic one-way light speed as long as the two-way light speed is isotropic. The 'speed of light' in this article refers to the speed of all electromagnetic radiation in vacuum. The two-way speed. The two-way speed of light is the average speed of light from one point, such as a source, to a mirror and back again. Because the light starts and finishes in the same place only one clock is needed to measure the total time, thus this speed can be experimentally determined independently of any clock synchronization scheme. Any measurement in which the light follows a closed path is considered a two-way speed measurement. Many tests of special relativity such as the Michelson–Morley experiment and the Kennedy–Thorndike experiment have shown within tight limits that in an inertial frame the two-way speed of light is isotropic and independent of the closed path considered. Isotropy experiments of the Michelson-Morley type do not use an external clock to directly measure the speed of light, but rather compare two internal frequencies or clocks. Therefore such experiments are sometimes called "clock anisotropy experiments", since every arm of a Michelson interferometer can be seen as a light clock having a specific rate, whose relative orientation dependences can be tested. Since 1983 the meter has been "defined" as the distance traveled by light in vacuum in second. This means that the speed of light can no longer be experimentally measured in SI units, but the length of a meter can be compared experimentally against some other standard of length. The one-way speed. Although the average speed over a two-way path can be measured, the one-way speed in one direction or the other is undefined (and not simply unknown), unless one can define what is "the same time" in two different locations. To measure the time that the light has taken to travel from one place to another it is necessary to know the start and finish times as measured on the same time scale. This requires either two synchronized clocks, one at the start and one at the finish, or some means of sending a signal instantaneously from the start to the finish. No instantaneous means of transmitting information is known. Thus the measured value of the average one-way speed is dependent on the method used to synchronize the start and finish clocks. This is a matter of convention. The Lorentz transformation is defined such that the one-way speed of light will be measured to be independent of the inertial frame chosen. On the other hand, some authors such as Mansouri and Sexl (1977) as well as Will (1992) argued that this problem doesn't affect measurements of the isotropy of the one-way speed of light, for instance, due to direction dependent changes relative to a "preferred" (aether) frame Σ. They based their analysis on a specific interpretation of the RMS test theory in relation to experiments in which light follows a unidirectional path and to slow clock-transport experiments. Will agreed that it is impossible to measure the one-way speed between two clocks using a time-of-flight method without synchronization scheme, though he argued: ""...a test of the isotropy of the speed between the same two clocks as the orientation of the propagation path varies relative to Σ should not depend on how they were synchronized..."". He added that aether theories can only be made consistent with relativity by introducing ad-hoc hypotheses. In more recent papers (2005, 2006) Will referred to those experiments as measuring the ""isotropy of light speed using one-way propagation"". However, others such as Zhang (1995, 1997) and Anderson "et al". (1998) objected to this interpretation of RMS. For instance, Anderson "et al." pointed out that the conventionality of simultaneity must already be considered in the preferred frame, so all assumptions concerning the isotropy of the one-way speed of light and other velocities in this frame are conventional as well. Therefore, RMS remains a useful test theory to analyze tests of Lorentz invariance and the two-way speed of light, though not of the one-way speed of light. They concluded :""...one cannot hope even to test the isotropy of the speed of light without, in the course of the same experiment, deriving a one-way numerical value at least in principle, which then would contradict the conventionality of synchrony."" Using generalizations of Lorentz transformations with anisotropic one-way speeds, Zhang and Anderson pointed out that all events and experimental results compatible with the Lorentz transformation and the isotropic one-way speed of light, must also be compatible with transformations preserving two-way light speed constancy and isotropy, while allowing anisotropic one-way speeds. Synchronization conventions. The way in which distant clocks are synchronized can have an effect on all time-related measurements over distance, such as speed or acceleration measurements. In isotropy experiments, simultaneity conventions are often not explicitly stated but are implicitly present in the way coordinates are defined or in the laws of physics employed. Einstein convention. This method synchronizes distant clocks in such a way that the one-way speed of light becomes equal to the two-way speed of light. If a signal sent from A at time formula_1 is arriving at B at time formula_2 and coming back to A at time formula_3, then the following convention applies: The details of this method, and the conditions that assure its consistency are discussed in Einstein synchronization. Slow clock-transport. It is easily demonstrated that if two clocks are brought together and synchronized, then one clock is moved rapidly away and back again, the two clocks will no longer be synchronized due to time dilation. This was measured in a variety of tests and is related to the twin paradox. If however one clock is moved away slowly in frame S and returned the two clocks will be very nearly synchronized when they are back together again. The clocks can remain synchronized to an arbitrary accuracy by moving them sufficiently slowly. If it is taken that, if moved slowly, the clocks remain synchronized at all times, even when separated, this method can be used to synchronize two spatially separated clocks. In the limit as the speed of transport tends to zero, this method is experimentally and theoretically equivalent to the Einstein convention. Though the effect of time dilation on those clocks cannot be neglected anymore when analyzed in another relatively moving frame S'. This explains why the clocks remain synchronized in S, whereas they are not synchronized anymore from the viewpoint of S', establishing relativity of simultaneity in agreement with Einstein synchronization. Therefore testing the equivalence between these clock synchronization schemes is important for special relativity, and some experiments in which light follows a unidirectional path have proven this equivalence to high precision. Non-standard synchronizations. As demonstrated by Hans Reichenbach and Adolf Grünbaum, Einstein synchronization is only a special case of a more broader synchronization scheme, which leaves the two-way speed of light invariant, but allows for different one-way speeds. The formula for Einstein synchronization is modified by replacing ½ with ε: ε can have values between 0 and 1. It was shown that this scheme can be used for observationally equivalent reformulations of the Lorentz transformation, see Generalizations of Lorentz transformations with anisotropic one-way speeds. As required by the experimentally proven equivalence between Einstein synchronization and slow clock-transport synchronization, which requires knowledge of time dilation of moving clocks, the same non-standard synchronisations must also affect time dilation. It was indeed pointed out that time dilation of moving clocks depends on the convention for the one-way velocities used in its formula. That is, time dilation can be measured by synchronizing two stationary clocks A and B, and then the readings of a moving clock C are compared with them. Changing the convention of synchronization for A and B makes the value for time dilation (like the one-way speed of light) directional dependent. The same conventionality also applies to the influence of time dilation on the Doppler effect. Only when time dilation is measured on closed paths, it is not conventional and can unequivocally be measured like the two-way speed of light. Time dilation on closed paths was measured in the Hafele–Keating experiment and in experiments on the Time dilation of moving particles such as Bailey "et al". (1977). Thus the so-called twin paradox occurs in all transformations preserving the constancy of the two-way speed of light. Inertial frames and dynamics. It was argued against the conventionality of the one-way speed of light that this concept is closely related to dynamics, the laws of motion and inertial reference frames. Salmon described some variations of this argument using momentum conservation, from which it follows that two equal bodies at the same place which are equally accelerated in opposite directions, should move with the same one-way velocity. Similarly, Ohanian argued that inertial reference frames are defined so that Newton's laws of motion hold in first approximation. Therefore, since the laws of motion predict isotropic one-way speeds of moving bodies with equal acceleration, and because of the experiments demonstrating the equivalence between Einstein synchronization and slow clock-transport synchronization, it appears to be required and directly measured that the one-way speed of light is isotropic in inertial frames. Otherwise, both the concept of inertial reference frames and the laws of motion must be replaced by much more complicated ones involving anisotropic coordinates. However, it was shown by others that this is principally not in contradiction with the conventionality of the one-way speed of light. Salmon argued that momentum conservation in its standard form assumes isotropic one-way speed of moving bodies from the outset. So it involves practically the same convention as in the case of isotropic one-way speed of light, thus using this as an argument against light speed conventionality would be circular. And in response to Ohanian, both Macdonald and Martinez argued that even though the laws of physics become more complicated with non-standard synchrony, they still are a consistent way to describe the phenomena. They also argued that it's not necessary to define inertial frames in terms of Newton's laws of motion, because other methods are possible as well. In addition, Iyer and Prabhu distinguished between "isotropic inertial frames" with standard synchrony and "anisotropic inertial frames" with non-standard synchrony. Experiments which appear to measure the one-way speed of light. Experiments which claimed to use a one-way light signal but which actually made a round trip measurement. The Greaves, Rodriguez and Ruiz-Camacho experiment. In the October 2009 issue of the American Journal of Physics Greaves, Rodriguez and Ruiz-Camacho reported a measurement of the one-way speed of light. J. Finkelstein showed that this experiment actually measures the round trip (two-way) speed of light. Experiments in which light follows a unidirectional path. Many experiments intended to measure the one-way speed of light, or its variation with direction, have been (and occasionally still are) performed in which light follows a unidirectional path. Claims have been made that those experiments have measured the one-way speed of light independently of any clock synchronisation convention, but they have all been shown to actually measure the two-way speed, because they are consistent with generalized Lorentz transformations including synchronizations with different one-way speeds on the basis of isotropic two-way speed of light (see sections the one-way speed and generalized Lorentz transformations). These experiments also confirm agreement between clock synchronization by slow transport and Einstein synchronization. Even though some authors argued that this is sufficient to demonstrate the isotropy of the one-way speed of light, it has been shown that such experiments cannot, in any meaningful way, measure the (an)isotropy of the one way speed of light unless inertial frames and coordinates are defined from the outset so that space and time coordinates as well as slow clock-transport are described isotropically (see sections inertial frames and dynamics and the one-way speed). Regardless of those different interpretations, the observed agreement between those synchronization schemes is an important prediction of special relativity, because this requires that transported clocks undergo time dilation (which itself is synchronization dependent) when viewed from another frame (see sections Slow clock-transport and Non-standard synchronizations). The JPL experiment. This experiment, carried out in 1990 by the NASA Jet Propulsion Laboratory, measured the time of flight of light signals through a fibre optic link between two hydrogen maser clocks. In 1992 the experimental results were analysed by Clifford Will who concluded that the experiment did actually measure the one-way speed of light. In 1997 the experiment was re-analysed by Zhang who showed that, in fact, only the two-way speed had been measured. Rømer's measurement. The first experimental determination of the speed of light was made by Ole Christensen Rømer. It may seem that this experiment measures the time for light to traverse part of the Earth's orbit and thus determines its one-way speed, however, this experiment was carefully re-analysed by Zhang, who showed that the measurement does not measure the speed independently of a clock synchronization scheme but actually used the Jupiter system as a slowly-transported clock to measure the light transit times. Experiments that can be done on the one-way speed of light. Although experiments cannot be done in which the one-way speed of light is measured independently of any clock synchronization scheme, it is possible to carry out experiments that measure a change in the one-way speed of light due, for example, to the motion of the source. Such experiments are the De Sitter double star experiment (1913), conclusively repeated in the x-ray spectrum by K. Brecher in 1977; or the terrestrial experiment by Alväger, "et al". (1963); they show that, when measured in an inertial frame, the one-way speed of light is independent of the motion of the source within the limits of experimental accuracy. In such experiments the clocks may be synchronized in any convenient way, since it is only a change of speed that is being measured. Observations of the arrival of radiation from distant astronomical events have shown that the one-way speed of light does not vary with frequency, that is, there is no vacuum dispersion of light. Similarly, differences in the one-way propagation between left- and right-handed photons, leading to vacuum birefringence, were excluded by observation of the simultaneous arrival of distant star light. For current limits on both effects, often analyzed with the Standard-Model Extension, see Vacuum dispersion and Vacuum birefringence. Experiments on two-way and one-way speeds using the Standard-Model Extension. While the experiments above were analyzed using generalized Lorentz transformations as in the Robertson-Mansouri-Sexl test theory, many modern tests are based on the Standard-Model Extension (SME). This test theory includes all possible Lorentz violations not only of special relativity, but of the Standard Model and General relativity as well. Regarding the isotropy of the speed of light, both two-way and one-way limits are described using coefficients (3x3 matrices): A series of experiments have been (and still are) performed since 2002 testing all of those coefficients using, for instance, symmetric and asymmetric optical resonators. No Lorentz violations have been observed as of 2013, providing current upper limits for Lorentz violations: formula_9, formula_10, and formula_11. For details and sources see Modern searches for Lorentz violation#Speed of light. However, the partially conventional character of those quantities was demonstrated by Kostelecky "et al", pointing out that such variations in the speed of light can be removed by suitable coordinate transformations and field redefinitions. Though this doesn't remove the Lorentz violation "per se", since such a redefinition only transfers the Lorentz violation form the photon sector to the matter sector of SME, thus those experiments remain valid tests of Lorentz invariance violation. There are one-way coefficients of the SME that cannot be redefined into other sectors, since different light rays from the same distance location are directly compared with each other, see the previous section. Theories in which the one-way speed of light is not equal to the two-way speed. Theories equivalent to special relativity. Lorentz ether theory. In the theory, the one-way speed of light is principally only equal to the two-way speed in the aether frame, though not in other frames due to the motion of the observer through the aether. However, the difference between the one-way and two-way speeds of light can never be observed due to the action of the aether on the clocks and lengths. Therefore, the Poincaré-Einstein convention is also employed in this model, making the one-way speed of light isotropic in all frames of reference. Even though this theory is experimentally indistinguishable from special relativity, Lorentz' theory is no longer used for reasons of philosophical preference and because of the development of general relativity. Generalizations of Lorentz transformations with anisotropic one-way speeds. The Reichenbach-Grünbaum ε-synchronization was further developed by authors such as Edwards (1963), Winnie (1970), Anderson and Stedman (1977), who reformulated the Lorentz transformation without changing its physical predictions. For instance, Edwards replaced Einstein's postulate that the one-way speed of light is constant when measured in an inertial frame with the postulate: So the average speed for the round trip remains the experimentally verifiable two-way speed, whereas the one-way speed of light is allowed to take the form in opposite directions: κ can have values between 0 and 1. In the extreme as κ approaches 1, light might propagate in one direction instantaneously, provided it takes the entire round-trip time to travel in the opposite direction. Following Edwards and Winnie, Anderson "et al." formulated generalized Lorentz transformations for arbitrary boosts of the form: (with κ and κ' being the synchrony vectors in frames S and S', respectively). This transformation indicates the one-way speed of light is conventional in all frames, leaving the two-way speed invariant. κ=0 means Einstein synchronization which results in the standard Lorentz transformation. As shown by Edwards, Winnie and Mansouri-Sexl, by suitable rearrangement of the synchrony parameters even some sort of "absolute simultaneity" can be achieved, in order to simulate the basic assumption of Lorentz ether theory. That is, in one frame the one-way speed of light is chosen to be isotropic, while all other frames take over the values of this "preferred" frame by "external synchronization". All predictions derived from such a transformation are experimentally indistinguishable from those of the standard Lorentz transformation; the difference is only that the defined clock time varies from Einstein's according to the distance in a specific direction. Theories not equivalent to special relativity. Test theories. A number of theories have been developed to allow assessment of the degree to which experimental results differ from the predictions of relativity. These are known as test theories and include the Robertson and Mansouri-Sexl (RMS) theories. This test theory uses Einstein synchronization in a "preferred" frame, while all other frames take over the value of this "preferred" frame by "external synchronization". However, this alone does not provide any testable deviation from special relativity, therefore they included additional parameters, making the two-way speed of light anisotropic in this model. To date, all experimental results agree with special relativity within the experimental uncertainty. Another test theory is the Standard-Model Extension (SME). It employs a broad variety of coefficients indicating Lorentz symmetry violations in special relativity, general relativity, and the Standard Model. Some of those parameters indicate anisotropies of the two-way and one-way speed of light. However, it was pointed out that such variations in the speed of light can be removed by suitable redefinitions of the coordinates and fields employed. Though this doesn't remove Lorentz violations "per se", it only shifts their appearance from the photon sector into the matter sector of SME (see above Experiments on two-way and one-way speeds using the Standard-Model Extension. Aether theories. Before 1887 it was generally believed that light travelled at a constant speed relative to the hypothesised medium of the aether. For an observer in motion with respect to the aether, this would result in slightly different two-way speeds of light in different directions. In 1887, the Michelson–Morley experiment showed that the two-way speed of light was constant regardless of direction or motion through the aether. Preferred reference frame. A preferred reference frame is a reference frame in which the laws of physics take on a special form. The ability to make measurements which show the one-way speed of light to be different from its two-way speed would, in principle, enable a preferred reference frame to be determined. This would be the reference frame in which the two-way speed of light was equal to the one-way speed. In Einstein's special theory of relativity, all inertial frames of reference are equivalent and there is no preferred frame. There are theories, such as Lorentz ether theory that are experimentally and mathematically equivalent to special relativity but have a preferred reference frame. In order for these theories to be compatible with experimental results the preferred frame must be undetectable. In other words it is a preferred frame in principle only, in practice all inertial frames must be equivalent, as in special relativity [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@12925ecb [docID]24372574 [hitPos]0 [correct]false [extraScores][F@c616598 , [answer]In physics, natural units are physical units of measurement based only on universal physical constants. For example the elementary charge "e" is a natural unit of electric charge, and the speed of light "c" is a natural unit of speed. A purely natural system of units is defined in such a way that some set of selected universal physical constants are each normalized to unity; that is, their numerical values in terms of these units are exactly 1. While this has the advantage of simplicity, there is a potential disadvantage in terms of loss of clarity and understanding, as these constants are then omitted from mathematical expressions of physical laws. Introduction. Natural units are intended to elegantly simplify particular algebraic expressions appearing in the laws of physics or to normalize some chosen physical quantities that are properties of universal elementary particles and are reasonably believed to be constant. However there is a choice of the set of natural units chosen, and quantities which are set to unity in one system may take a different value or even assumed to vary in another natural unit system. Natural units are "natural" because the origin of their definition comes only from properties of nature and not from any human construct. Planck units are often, without qualification, called "natural units", although they constitute only one of several systems of natural units, albeit the best known such system. Planck units (up to a simple multiplier for each unit) might be considered one of the most "natural" systems in that the set of units is not based on properties of any prototype, object, or particle but are solely derived from the properties of free space. As with other systems of units, the base units of a set of natural units will include definitions and values for length, mass, time, temperature, and electric charge (in lieu of electric current). Some physicists do not recognize temperature as a fundamental physical quantity, since it expresses the energy per degree of freedom of a particle, which can be expressed in terms of energy (or mass, length, and time). Virtually every system of natural units normalizes Boltzmann's constant "k"B to 1, which can be thought of as simply a way of defining the unit temperature. In the SI unit system, electric charge is a separate fundamental dimension of physical quantity, but in natural unit systems charge is expressed in terms of the mechanical units of mass, length, and time, similarly to cgs. There are two common ways to relate charge to mass, length, and time: In Lorentz–Heaviside units (also called "rationalized"), Coulomb's law is , and in Gaussian units (also called "non-rationalized"), Coulomb's law is . Both possibilities are incorporated into different natural unit systems. Notation and use. Natural units are most commonly used by "setting the units to one". For example, many natural unit systems include the equation in the unit-system definition, where "c" is the speed of light. If a velocity "v" is half the speed of light, then as and , hence . The equation means "the velocity "v" has the value one-half when measured in Planck units", or "the velocity "v" is one-half the Planck unit of velocity". The equation " can be plugged in anywhere else. For example, Einstein's equation can be rewritten in Planck units as . This equation means "The energy of a particle, measured in Planck units of energy, equals the mass of the particle, measured in Planck units of mass." Advantages and disadvantages. Compared to SI or other unit systems, natural units have both advantages and disadvantages: Choosing constants to normalize. Out of the many physical constants, the designer of a system of natural unit systems must choose a few of these constants to normalize (set equal to 1). It is not possible to normalize just "any" set of constants. For example, the mass of a proton and the mass of an electron cannot both be normalized: if the mass of an electron is defined to be 1, then the mass of a proton has to be ≈1836. In a less trivial example, the fine-structure constant, α≈1/137, cannot be set to 1, because it is a dimensionless number. The fine-structure constant is related to other fundamental constants where "k"e is the Coulomb constant, "e" is the elementary charge, ℏ is the reduced Planck constant, and "c" is the speed of light. Therefore it is not possible to simultaneously normalize all four of the constants "c", ℏ, "e", and "k"e. Electromagnetism units. In SI units, electric charge is expressed in coulombs, a separate unit which is additional to the "mechanical" units (mass, length, time), even though the traditional definition of the ampere refers to some of these other units. In natural unit systems, however, electric charge has units of . There are two main natural unit systems for electromagnetism: Of these, Heaviside-Lorentz is somewhat more common, mainly because Maxwell's equations are simpler in Lorentz-Heaviside units than they are in Gaussian units. In the two unit systems, the elementary charge "e" satisfies: where ℏ is the reduced Planck constant, "c" is the speed of light, and α≈1/137 is the fine-structure constant. In a natural unit system where , Lorentz-Heaviside units can be derived from SI units by setting . Gaussian units can be derived from SI units by a more complicated set of transformations, such as multiplying all electric fields by (4πε0)-1/2, multiplying all magnetic susceptibilities by 4π, and so on. Systems of natural units. Planck units. Planck units are defined by where "c" is the speed of light, "G" is the gravitational constant, ℏ is the reduced Planck constant, and "k"B is the Boltzmann constant. Planck units are a system of natural units that is not defined in terms of properties of any prototype, physical object, or even elementary particle. They only refer to the basic structure of the laws of physics: "c" and "G" are part of the structure of spacetime in general relativity, and ℏ captures the relationship between energy and frequency which is at the foundation of quantum mechanics. This makes Planck units particularly useful and common in theories of quantum gravity, including string theory. Planck units may be considered "more natural" even than other natural unit systems discussed below, as Planck units are not based on any arbitrarily chosen prototype object or particle. For example, some other systems use the mass of an electron as a parameter to be normalized. But the electron is just one of 16 known massive elementary particles, all with different masses, and there is no compelling reason, within fundamental physics, to emphasize the electron mass over some other elementary particle's mass. Like the other systems (see above), the electromagnetism units in Planck units can be based on either Lorentz–Heaviside units or Gaussian units. The unit of charge is different in each. "Natural units" (particle physics and cosmology). In particle physics and cosmology, the phrase "natural units" generally means: where formula_6 is the reduced Planck constant, "c" is the speed of light, and "k"B is the Boltzmann constant. Like the other systems (see above), the electromagnetism units in Planck units can be based on either Lorentz–Heaviside units or Gaussian units. The unit of charge is different in each. Finally, one more unit is needed. Most commonly, electron-volt (eV) is used, despite the fact that this is not a "natural" unit in the sense discussed above – it is defined by a natural property, the elementary charge, and the anthropogenic unit of electric potential, the volt. (The SI prefixed multiples of eV are used as well: keV, MeV, GeV, etc.) With the addition of eV (or any other auxiliary unit), any quantity can be expressed. For example, a distance of 1 cm can be expressed in terms of eV, in natural units, as: Stoney units. Stoney units are defined by: where "c" is the speed of light, "G" is the gravitational constant, "e" is the elementary charge, "k"B is the Boltzmann constant, ℏ is the reduced Planck constant, and α is the fine-structure constant. George Johnstone Stoney was the first physicist to introduce the concept of natural units. He presented the idea in a lecture entitled "On the Physical Units of Nature" delivered to the British Association in 1874. Stoney units differ from Planck units by fixing the elementary charge at 1, instead of Planck's constant (only discovered after Stoney's proposal). Stoney units are rarely used in modern physics for calculations, but they are of historical interest. Atomic units. There are two types of atomic units, closely related. Hartree atomic units: Rydberg atomic units: These units are designed to simplify atomic and molecular physics and chemistry, especially the hydrogen atom, and are widely used in these fields. The Hartree units were first proposed by Douglas Hartree, and are more common than the Rydberg units. The units are designed especially to characterize the behavior of an electron in the ground state of a hydrogen atom. For example, using the Hartree convention, in the Bohr model of the hydrogen atom, an electron in the ground state has orbital velocity = 1, orbital radius = 1, angular momentum = 1, ionization energy = , etc. The unit of energy is called the Hartree energy in the Hartree system and the Rydberg energy in the Rydberg system. They differ by a factor of 2. The speed of light is relatively large in atomic units (137 in Hartree or 274 in Rydberg), which comes from the fact that an electron in hydrogen tends to move much slower than the speed of light. The gravitational constant is extremely small in atomic units (around 10−45), which comes from the fact that the gravitational force between two electrons is far weaker than the Coulomb force. The unit length, "l"A, is the Bohr radius, "a"0. The values of "c" and "e" shown above imply that "e"=(αℏ"c")1/2, as in Gaussian units, "not" Lorentz–Heaviside units. However, hybrids of the Gaussian and Lorentz–Heaviside units are sometimes used, leading to inconsistent conventions for magnetism-related units. Quantum chromodynamics (QCD) system of units. The electron mass is replaced with that of the proton. "Strong units" are "convenient for work in QCD and nuclear physics, where quantum mechanics and relativity are omnipresent and the proton is an object of central interest". Geometrized units. The geometrized unit system, used in general relativity, is not a completely defined system. In this system, the base physical units are chosen so that the speed of light and the gravitational constant are set equal to unity. Other units may be treated however desired. By normalizing other appropriate units, geometrized units become identical to Planck units. Summary table. where [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@15ab6789 [docID]33710742 [hitPos]0 [correct]false [extraScores][F@15cf07f1 , [answer]In physics, the relativity of simultaneity is the concept that "distant simultaneity" – whether two spatially separated events occur at the same time – is not absolute, but depends on the observer's reference frame. According to the special theory of relativity, it is impossible to say in an "absolute" sense whether two distinct events occur at the same time if those events are separated in space, such as a car crash in London and another in New York. The question of whether the events are simultaneous is "relative": in some reference frames the two accidents may happen at the same time, in other frames (in a different state of motion relative to the events) the crash in London may occur first, and in still other frames the New York crash may occur first. However, if the two events are causally connected ("event A causes event B"), the causal order is preserved (i.e., "event A precedes event B") in all frames of reference. If we imagine one reference frame assigns precisely the same time to two events that are at different points in space, a reference frame that is moving relative to the first will generally assign different times to the two events. This is illustrated in the ladder paradox, a thought experiment which uses the example of a ladder moving at high speed through a garage. A mathematical form of the relativity of simultaneity ("local time") was introduced by Hendrik Lorentz in 1892, and physically interpreted (to first order in "v/c") as the result of a synchronization using light signals by Henri Poincaré in 1900. However, both Lorentz and Poincaré based their conceptions on the aether as a preferred but undetectable frame of reference, and continued to distinguish between "true time" (in the aether) and "apparent" times for moving observers. It was Albert Einstein in 1905 who abandoned the (classical) aether and emphasized the significance of relativity of simultaneity to our understanding of space and time. He deduced the failure of absolute simultaneity from two stated assumptions: The train-and-platform thought experiment. A popular picture for understanding this idea is provided by a thought experiment consisting of one observer midway inside a speeding traincar and another observer standing on a platform as the train moves past. It is similar to thought experiments suggested by Daniel Frost Comstock in 1910 and Einstein in 1917. A flash of light is given off at the center of the traincar just as the two observers pass each other. The observer on board the train sees the front and back of the traincar at fixed distances from the source of light and as such, according to this observer, the light will reach the front and back of the traincar at the same time. The observer standing on the platform, on the other hand, sees the rear of the traincar moving (catching up) toward the point at which the flash was given off and the front of the traincar moving away from it. As the speed of light is finite and the same in all directions for all observers, the light headed for the back of the train will have less distance to cover than the light headed for the front. Thus, the flashes of light will strike the ends of the traincar at different times. Spacetime diagrams. It may be helpful to visualize this situation using spacetime diagrams. For a given observer, the "t"-axis is defined to be a point traced out in time by the origin of the spatial coordinate "x", and is drawn vertically. The "x"-axis is defined as the set of all points in space at the time "t" = 0, and is drawn horizontally. The statement that the speed of light is the same for all observers is represented by drawing a light ray as a 45° line, regardless of the speed of the source relative to the speed of the observer. In the first diagram, we see the two ends of the train drawn as grey lines. Because the ends of the train are stationary with respect to the observer on the train, these lines are just vertical lines, showing their motion through time but not space. The flash of light is shown as the 45° red lines. We see that the points at which the two light flashes hit the ends of the train are at the same level in the diagram. This means that the events are simultaneous. In the second diagram, we see the two ends of the train moving to the right, shown by parallel lines. The flash of light is given off at a point exactly halfway between the two ends of the train, and again form two 45° lines, expressing the constancy of the speed of light. In this picture, however, the points at which the light flashes hit the ends of the train are "not" at the same level; they are "not" simultaneous. The dashed grey line between the events of the light beams hitting the ends of the trains identifies a volume of simultaneity for the observer on the train, i.e., those events which he calculates occur at the same instant of time (these form a flat 3-dimensional surface). Note that for the observer on the platform, each point on that line (identifying a plane where "y" and "z" coordinates are the same) is on a different level. So each point on that dashed grey line exists at a different time for the station observer, and at the same time for the observer on the train. That is the essence of the relativity of simultaneity. Lorentz transformations. The relativity of simultaneity can be calculated using Lorentz transformations, which relate the coordinates used by one observer to coordinates used by another in uniform relative motion with respect to the first. Assume that the first observer uses coordinates labeled "t, x, y," and "z", while the second observer uses coordinates labeled "t', x', y'," and "z'". Now suppose that the first observer sees the second moving in the "x"-direction at a velocity "v". And suppose that the observer's coordinate axes are parallel and that they have the same origin. Then, the Lorentz transformations show that the coordinates are related by the equations: where "c" is the speed of light. If two events happen at the same time in the frame of the first observer, they will have identical values of the "t"-coordinate. However, if they have different values of the "x"-coordinate (different positions in the "x"-direction), we see that they will have different values of the "t"' coordinate; they will happen at different times in that frame. The term that accounts for the failure of absolute simultaneity is that "v x/c"2. The equation "t' "= constant defines a "line of simultaneity" in the ("x', t' ") coordinate system for the second (moving) observer, just as the equation "t" = constant defines the "line of simultaneity" for the first (stationary) observer in the ("x, t") coordinate system. We can see from the above equations for the Lorentz transform that "t'" is constant if and only if "t – v x/c"2 = constant. Thus the set of points that make "t" constant are different from the set of points that makes "t' "constant. That is, the set of events which are regarded as simultaneous depends on the frame of reference used to make the comparison. Graphically, this can be represented on a space-time diagram by the fact that a plot of the set of points regarded as simultaneous generates a line which depends on the observer. In the space-time diagram at the right, the dashed line represents a set of points considered to be simultaneous with the origin by an observer moving with a velocity "v" of one-quarter of the speed of light. The dotted horizontal line represents the set of points regarded as simultaneous with the origin by a stationary observer. This diagram is drawn using the ("x, t") coordinates of the stationary observer, and is scaled so that the speed of light is one, i.e., so that a ray of light would be represented by a line with a 45° angle from the "x" axis. From our previous analysis, given that "v" = 0.25 and "c" = 1, the equation of the dashed line of simultaneity is "t" – 0.25"x" = 0 and with "v" = 0, the equation of the dotted line of simultaneity is "t" = 0. It should also be mentioned that Lorentz came up with his ideas based on the assumption that the Aether existed. History. In 1892 and 1895, Hendrik Lorentz used a mathematical tool called "local time" "t' = t – v x/c"2 for explaining the negative aether drift experiments. However, Lorentz gave no physical explanation of this effect. This was done by Henri Poincaré who already in 1898 emphasized the conventional nature of simultaneity and who argued that it is convenient to postulate the constancy of the speed of light in all directions. However, this paper does not contain any discussion of Lorentz's theory or the possible difference in defining simultaneity for observers in different states of motion. This was done in 1900, when he derived local time by assuming that within the aether the speed of light is invariant. Due to the "Principle of relative motion" also moving observers within the aether assume that they are at rest and that the speed of light is constant in all directions ("only" to first order in "v/c"). So if they synchronize their clocks by using light signals, they will only consider the transit time for the signals, but not their motion in respect to the aether. So the moving clocks are not synchronous and do not indicate the "true" time. Poincaré calculated that this synchronization error corresponds to Lorentz's local time. Also in 1904 Poincaré emphasized the connection between the principle of relativity, "local time", and light speed invariance, however, the reasoning in that paper was presented in a qualitative and conjectural manner. Albert Einstein in 1905 used a similar method to derive the time transformation for "all" orders in "v/c", i.e., the complete Lorentz transformation (also Poincaré got the full transformation in 1905 but in those papers he did not mention his synchronization procedure). This derivation was completely based on light speed invariance and the relativity principle, so Einstein noted that for the electrodynamics of moving bodies the aether is superfluous. Thus the separation into "true" and "local" times of Lorentz and Poincaré vanishes – all times are equally valid and therefore the relativity of length and time is a natural consequence. In 1908 Hermann Minkowski introduced the concept of a world line of a particle in his model of the cosmos called Minkowski space. The mathematical model of spacetime is an affine geometry equipped with a quadratic form that measures intervals between events. (When the events are connected by light, the interval is zero). In Minkowski's system there is a simultaneous hyperplane determined by the quadratic form at each event along a world line. This simultaneous hyperplane depends on the velocity of the particle, and thus is relative to velocity. Einstein's train thought experiment. Einstein's version of the experiment presumed slightly different conditions, where a train moving past the standing observer is struck by two bolts of lightning simultaneously, but at different positions along the axis of train movement (back and front of the traincar). In the inertial frame of the standing observer, there are three events which are spatially dislocated, but simultaneous: event of the standing observer facing the moving observer (i.e., the center of the train), event of lightning striking the front of the traincar, and the event of lightning striking the back of the car. Since the events are placed along the axis of train movement, their time coordinates become projected to different time coordinates in the moving train's inertial frame. Events which occurred at space coordinates in the direction of train movement (in the stationary frame), happen earlier than events at coordinates opposite to the direction of train movement. In the moving train's inertial frame, this means that lightning will strike the front of the traincar before two observers align (face each other [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@58777339 [docID]3406142 [hitPos]0 [correct]false [extraScores][F@16aed091 , [answer]In physics, the electron volt (symbol eV; also written electronvolt) is a unit of energy equal to approximately joule (symbol J). By definition, it is the amount of energy gained (or lost) by the charge of a single electron moved across an electric potential difference of one volt. Thus it is 1 volt (1 joule per coulomb, ) multiplied by the elementary charge ("e", or ). Therefore, one electron volt is equal to . Historically, the electron volt was devised as a standard unit of measure through its usefulness in electrostatic particle accelerator sciences because a particle with charge "q" has an energy after passing through the potential "V"; if "q" is quoted in integer units of the elementary charge and the terminal bias in volts, one gets an energy in eV. The electron volt is not an SI unit, and thus its value in SI units must be obtained experimentally. Like the elementary charge on which it is based, it is not an independent quantity but is equal to . It is a common unit of energy within physics, widely used in solid state, atomic, nuclear, and particle physics. It is commonly used with the SI prefixes milli-, kilo-, mega-, giga-, tera-, peta- or exa- (meV, keV, MeV, GeV, TeV, PeV and EeV respectively). Thus meV stands for milli-electron volt. In some older documents, and in the name Bevatron, the symbol BeV is used, which stands for billion electron volts; it is equivalent to the GeV. Mass. By mass–energy equivalence, the electronvolt is also a unit of mass. It is common in particle physics, where mass and energy are often interchanged, to express mass in units of eV/"c"2, where "c" is the speed of light in vacuum (from Mass–energy equivalence). It is common to simply express mass in terms of "eV" as a unit of mass, effectively using a system of natural units with "c" set to 1. The Mass equivalent of 1 eV is . For example, an electron and a positron, each with a mass of , can annihilate to yield of energy. The proton has a mass of . In general, the masses of all hadrons are of the order of , which makes the GeV (gigaelectronvolt) a convenient unit of mass for particle physics: The atomic mass unit, 1 gram divided by Avogadro's number, is almost the mass of a hydrogen atom, which is mostly the mass of the proton. To convert to megaelectronvolts, use the formula: Momentum. In high-energy physics, the electron volt is often used as a unit of momentum. A potential difference of 1 volt causes an electron to gain an amount of energy (i.e., ). This gives rise to usage of eV (and keV, MeV, GeV or TeV) as units of momentum, for the energy supplied results in acceleration of the particle. The dimensions of momentum units are . The dimensions of energy units are . Then, dividing the units of energy (such as eV) by a fundamental constant that has units of velocity (), facilitates the required conversion of using energy units to describe momentum. In the field of high-energy particle physics, the fundamental velocity unit is the speed of light in vacuum "c". Thus, dividing energy in eV by the speed of light, one can describe the momentum of an electron in units of eV/"c". The fundamental velocity constant "c" is often "dropped" from the units of momentum by way of defining units of length such that the value of "c" is unity. For example, if the momentum "p" of an electron is said to be , then the conversion to MKS can be achieved by: Distance. In particle physics, a system of "natural units" in which the speed of light in vacuum "c" and the reduced Planck constant "ħ" are dimensionless and equal to unity is widely used: . In these units, both distances and times are expressed in inverse energy units (while energy and mass are expressed in the same units, see mass–energy equivalence). In particular, particle scattering lengths are often presented in units of inverse particle masses. Outside this system of units, the conversion factors between electronvolt, second, and nanometer are the following: The above relations also allow expressing the mean lifetime "τ" of an unstable particle (in seconds) in terms of its decay width "Γ" (in eV) via . For example, the B0 meson has a lifetime of 1.530(9) picoseconds, mean decay length is , or a decay width of . Conversely, the tiny meson mass differences responsible for meson oscillations are often expressed in the more convenient inverse picoseconds. Temperature. In certain fields, such as plasma physics, it is convenient to use the electronvolt as a unit of temperature. The conversion to kelvin is defined by using "k"B, the Boltzmann constant: For example, a typical magnetic confinement fusion plasma is , or 170 megakelvin. As an approximation: "k"B"T" is about (≈ ) at a temperature of . Properties. The energy "E", frequency "v", and wavelength λ of a photon are related by where "h" is the Planck constant, "c" is the speed of light. This reduces to A photon with a wavelength of (green light) would have an energy of approximately . Similarly, would correspond to an infrared photon of wavelength , and so on. Scattering experiments. In a low-energy nuclear scattering experiment, it is conventional to refer to the nuclear recoil energy in units of eVr, keVr, etc. This distinguishes the nuclear recoil energy from the "electron equivalent" recoil energy (eVee, keVee, etc.) measured by scintillation light. For example, the yield of a phototube is measured in phe/keVee (photoelectrons per keV electron-equivalent energy). The relationship between eV, eVr, and eVee depends on the medium the scattering takes place in, and must be established empirically for each material [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@15ab6789 [docID]9598 [hitPos]1 [correct]false [extraScores][F@46e0896c , [answer]Visible light (commonly referred to simply as light) is electromagnetic radiation that is visible to the human eye, and is responsible for the sense of sight. Visible light is usually defined as having a wavelength in the range of 400 nanometres (nm), or 400×10−9 m, to 700 nanometres – between the infrared, with longer wavelengths and the ultraviolet, with shorter wavelengths. These numbers do not represent the absolute limits of human vision, but the approximate range within which most people can see reasonably well under most circumstances. Various sources define visible light as narrowly as 420 to 680 to as broadly as 380 to 800 nm. Under ideal laboratory conditions, people can see infrared up to at least 1050 nm, children and young adults ultraviolet down to about 310 to 313 nm. Primary properties of visible light are intensity, propagation direction, frequency or wavelength spectrum, and polarisation, while its speed in a vacuum, 299,792,458 meters per second, is one of the fundamental constants of nature. Visible light, as with all types of electromagnetic radiation (EMR), is experimentally found to always move at this speed in vacuum. In common with all types of EMR, visible light is emitted and absorbed in tiny "packets" called photons, and exhibits properties of both waves and particles. This property is referred to as the wave–particle duality. The study of light, known as optics, is an important research area in modern physics. In physics, the term "light" sometimes refers to electromagnetic radiation of any wavelength, whether visible or not. This article focuses on visible light. See the electromagnetic radiation article for the general term. Speed of light. The speed of light in a vacuum is defined to be exactly 299,792,458 m/s (approximately 186,282 miles per second). The fixed value of the speed of light in SI units results from the fact that the metre is now defined in terms of the speed of light. All forms of electromagnetic radiation move at exactly this same speed in vacuum. Different physicists have attempted to measure the speed of light throughout history. Galileo attempted to measure the speed of light in the seventeenth century. An early experiment to measure the speed of light was conducted by Ole Rømer, a Danish physicist, in 1676. Using a telescope, Rømer observed the motions of Jupiter and one of its moons, Io. Noting discrepancies in the apparent period of Io's orbit, he calculated that light takes about 22 minutes to traverse the diameter of Earth's orbit. However, its size was not known at that time. If Rømer had known the diameter of the Earth's orbit, he would have calculated a speed of 227,000,000 m/s. Another, more accurate, measurement of the speed of light was performed in Europe by Hippolyte Fizeau in 1849. Fizeau directed a beam of light at a mirror several kilometers away. A rotating cog wheel was placed in the path of the light beam as it traveled from the source, to the mirror and then returned to its origin. Fizeau found that at a certain rate of rotation, the beam would pass through one gap in the wheel on the way out and the next gap on the way back. Knowing the distance to the mirror, the number of teeth on the wheel, and the rate of rotation, Fizeau was able to calculate the speed of light as 313,000,000 m/s. Léon Foucault used an experiment which used rotating mirrors to obtain a value of 298,000,000 m/s in 1862. Albert A. Michelson conducted experiments on the speed of light from 1877 until his death in 1931. He refined Foucault's methods in 1926 using improved rotating mirrors to measure the time it took light to make a round trip from Mt. Wilson to Mt. San Antonio in California. The precise measurements yielded a speed of 299,796,000 m/s. The effective velocity of light in various transparent substances containing ordinary matter, is less than in vacuum. For example the speed of light in water is about 3/4 of that in vacuum. However, the slowing process in matter is thought to result not from actual slowing of particles of light, but rather from their absorption and re-emission from charged particles in matter. As an extreme example of the nature of light-slowing in matter, two independent teams of physicists were able to bring light to a "complete standstill" by passing it through a Bose-Einstein Condensate of the element rubidium, one team at Harvard University and the Rowland Institute for Science in Cambridge, Mass., and the other at the Harvard-Smithsonian Center for Astrophysics, also in Cambridge. However, the popular description of light being "stopped" in these experiments refers only to light being stored in the excited states of atoms, then re-emitted at an arbitrary later time, as stimulated by a second laser pulse. During the time it had "stopped" it had ceased to be light. Electromagnetic spectrum and visible light. Generally, EM radiation, or EMR (the designation 'radiation' excludes static electric and magnetic and near fields) is classified by wavelength into radio, microwave, infrared, the visible region that we perceive as light, ultraviolet, X-rays and gamma rays. The behaviour of EMR depends on its wavelength. Higher frequencies have shorter wavelengths, and lower frequencies have longer wavelengths. When EMR interacts with single atoms and molecules, its behaviour depends on the amount of energy per quantum it carries. EMR in the visible light region consists of quanta (called photons) that are at the lower end of the energies that are capable of causing electronic excitation within molecules, which lead to changes in the bonding or chemistry of the molecule. At the lower end of the visible light spectrum, EMR becomes invisible to humans (infrared) because its photons no longer have enough individual energy to cause a lasting molecular change (a change in conformation) in the visual molecule retinal in the human retina, which change triggers the sensation of vision. There exist animals that are sensitive to various types of infrared, but not by means of quantum-absorption. Infrared sensing in snakes depends on a kind of natural thermal imaging, in which tiny packets of cellular water are raised in temperature by the infrared radiation. EMR in this range causes molecular vibration and heating effects, which is how these animals detect it. Above the range of visible light, ultraviolet light becomes invisible to humans, mostly because it is absorbed by the cornea below 360 nanometers and the internal lens below 400. Furthermore, the rods and cones located in the retina of the human eye cannot detect the very short (below 360 nm.) ultraviolet wavelengths, and are in fact damaged by ultraviolet. Many animals with eyes that do not require lenses (such as insects and shrimp) are able to detect ultraviolet, by quantum photon-absorption mechanisms, in much the same chemical way that humans detect visible light. Optics. The study of light and the interaction of light and matter is termed optics. The observation and study of optical phenomena such as rainbows and the aurora borealis offer many clues as to the nature of light. Refraction. Refraction is the bending of light rays when passing through a surface between one transparent material and another. It is described by Snell's Law: where formula_2 is the angle between the ray and the surface normal in the first medium, formula_3 is the angle between the ray and the surface normal in the second medium, and n1 and n2 are the indices of refraction, "n" = 1 in a vacuum and "n" > 1 in a transparent substance. When a beam of light crosses the boundary between a vacuum and another medium, or between two different media, the wavelength of the light changes, but the frequency remains constant. If the beam of light is not orthogonal (or rather normal) to the boundary, the change in wavelength results in a change in the direction of the beam. This change of direction is known as refraction. The refractive quality of lenses is frequently used to manipulate light in order to change the apparent size of images. Magnifying glasses, spectacles, contact lenses, microscopes and refracting telescopes are all examples of this manipulation. Light sources. There are many sources of light. The most common light sources are thermal: a body at a given temperature emits a characteristic spectrum of black-body radiation. A simple thermal source is sunlight, the radiation emitted by the chromosphere of the Sun at around 6,000 Kelvin peaks in the visible region of the electromagnetic spectrum when plotted in wavelength units and roughly 44% of sunlight energy that reaches the ground is visible. Another example is incandescent light bulbs, which emit only around 10% of their energy as visible light and the remainder as infrared. A common thermal light source in history is the glowing solid particles in flames, but these also emit most of their radiation in the infrared, and only a fraction in the visible spectrum. The peak of the blackbody spectrum is in the deep infrared, at about 10 micrometer wavelength, for relatively cool objects like human beings. As the temperature increases, the peak shifts to shorter wavelengths, producing first a red glow, then a white one, and finally a blue-white colour as the peak moves out of the visible part of the spectrum and into the ultraviolet. These colours can be seen when metal is heated to "red hot" or "white hot". Blue-white thermal emission is not often seen, except in stars (the commonly seen pure-blue colour in a gas flame or a welder's torch is in fact due to molecular emission, notably by CH radicals (emitting a wavelength band around 425 nm, and is not seen in stars or pure thermal radiation). Atoms emit and absorb light at characteristic energies. This produces "emission lines" in the spectrum of each atom. Emission can be spontaneous, as in light-emitting diodes, gas discharge lamps (such as neon lamps and neon signs, mercury-vapor lamps, etc.), and flames (light from the hot gas itself—so, for example, sodium in a gas flame emits characteristic yellow light). Emission can also be stimulated, as in a laser or a microwave maser. Deceleration of a free charged particle, such as an electron, can produce visible radiation: cyclotron radiation, synchrotron radiation, and bremsstrahlung radiation are all examples of this. Particles moving through a medium faster than the speed of light in that medium can produce visible Cherenkov radiation. Certain chemicals produce visible radiation by chemoluminescence. In living things, this process is called bioluminescence. For example, fireflies produce light by this means, and boats moving through water can disturb plankton which produce a glowing wake. Certain substances produce light when they are illuminated by more energetic radiation, a process known as fluorescence. Some substances emit light slowly after excitation by more energetic radiation. This is known as phosphorescence. Phosphorescent materials can also be excited by bombarding them with subatomic particles. Cathodoluminescence is one example. This mechanism is used in cathode ray tube television sets and computer monitors. Certain other mechanisms can produce light: When the concept of light is intended to include very-high-energy photons (gamma rays), additional generation mechanisms include: Units and measures. Light is measured with two main alternative sets of units: radiometry consists of measurements of light power at all wavelengths, while photometry measures light with wavelength weighted with respect to a standardised model of human brightness perception. Photometry is useful, for example, to quantify Illumination (lighting) intended for human use. The SI units for both systems are summarised in the following tables. The photometry units are different from most systems of physical units in that they take into account how the human eye responds to light. The cone cells in the human eye are of three types which respond differently across the visible spectrum, and the cumulative response peaks at a wavelength of around 555 nm. Therefore, two sources of light which produce the same intensity (W/m2) of visible light do not necessarily appear equally bright. The photometry units are designed to take this into account, and therefore are a better representation of how "bright" a light appears to be than raw intensity. They relate to raw power by a quantity called luminous efficacy, and are used for purposes like determining how to best achieve sufficient illumination for various tasks in indoor and outdoor settings. The illumination measured by a photocell sensor does not necessarily correspond to what is perceived by the human eye, and without filters which may be costly, photocells and charge-coupled devices (CCD) tend to respond to some infrared, ultraviolet or both. Light pressure. Light exerts physical pressure on objects in its path, a phenomenon which can be deduced by Maxwell's equations, but can be more easily explained by the particle nature of light: photons strike and transfer their momentum. Light pressure is equal to the power of the light beam divided by "c", the speed of light. Due to the magnitude of "c", the effect of light pressure is negligible for everyday objects. For example, a one-milliwatt laser pointer exerts a force of about 3.3 piconewtons on the object being illuminated; thus, one could lift a U.S. penny with laser pointers, but doing so would require about 30 billion 1-mW laser pointers. However, in nanometer-scale applications such as NEMS, the effect of light pressure is more significant, and exploiting light pressure to drive NEMS mechanisms and to flip nanometer-scale physical switches in integrated circuits is an active area of research. At larger scales, light pressure can cause asteroids to spin faster, acting on their irregular shapes as on the vanes of a windmill. The possibility of making solar sails that would accelerate spaceships in space is also under investigation. Although the motion of the Crookes radiometer was originally attributed to light pressure, this interpretation is incorrect; the characteristic Crookes rotation is the result of a partial vacuum. This should not be confused with the Nichols radiometer, in which the (slight) motion caused by torque (though not enough for full rotation against friction) "is" directly caused by light pressure. Historical theories about light, in chronological order. Classical Greece and Hellenism. In the fifth century BC, Empedocles postulated that everything was composed of four elements; fire, air, earth and water. He believed that Aphrodite made the human eye out of the four elements and that she lit the fire in the eye which shone out from the eye making sight possible. If this were true, then one could see during the night just as well as during the day, so Empedocles postulated an interaction between rays from the eyes and rays from a source such as the sun. In about 300 BC, Euclid wrote "Optica", in which he studied the properties of light. Euclid postulated that light travelled in straight lines and he described the laws of reflection and studied them mathematically. He questioned that sight is the result of a beam from the eye, for he asks how one sees the stars immediately, if one closes one's eyes, then opens them at night. Of course if the beam from the eye travels infinitely fast this is not a problem. In 55 BC, Lucretius, a Roman who carried on the ideas of earlier Greek atomists, wrote: ""The light & heat of the sun; these are composed of minute atoms which, when they are shoved off, lose no time in shooting right across the interspace of air in the direction imparted by the shove."" – "On the nature of the Universe" Despite being similar to later particle theories, Lucretius's views were not generally accepted. Ptolemy (c. 2nd century) wrote about the refraction of light in his book "Optics". Classical India. In ancient India, the Hindu schools of Samkhya and Vaisheshika, from around the early centuries CE developed theories on light. According to the Samkhya school, light is one of the five fundamental "subtle" elements ("tanmatra") out of which emerge the gross elements. The atomicity of these elements is not specifically mentioned and it appears that they were actually taken to be continuous. On the other hand, the Vaisheshika school gives an atomic theory of the physical world on the non-atomic ground of ether, space and time. (See "Indian atomism".) The basic atoms are those of earth ("prthivi"), water ("pani"), fire ("agni"), and air ("vayu") Light rays are taken to be a stream of high velocity of "tejas" (fire) atoms. The particles of light can exhibit different characteristics depending on the speed and the arrangements of the "tejas" atoms. The "Vishnu Purana" refers to sunlight as "the seven rays of the sun". The Indian Buddhists, such as Dignāga in the 5th century and Dharmakirti in the 7th century, developed a type of atomism that is a philosophy about reality being composed of atomic entities that are momentary flashes of light or energy. They viewed light as being an atomic entity equivalent to energy. Descartes. René Descartes (1596–1650) held that light was a mechanical property of the luminous body, rejecting the "forms" of Ibn al-Haytham and Witelo as well as the "species" of Bacon, Grosseteste, and Kepler. In 1637 he published a theory of the refraction of light that assumed, incorrectly, that light travelled faster in a denser medium than in a less dense medium. Descartes arrived at this conclusion by analogy with the behaviour of sound waves. Although Descartes was incorrect about the relative speeds, he was correct in assuming that light behaved like a wave and in concluding that refraction could be explained by the speed of light in different media. Descartes is not the first to use the mechanical analogies but because he clearly asserts that light is only a mechanical property of the luminous body and the transmitting medium, Descartes' theory of light is regarded as the start of modern physical optics. Particle theory. Pierre Gassendi (1592–1655), an atomist, proposed a particle theory of light which was published posthumously in the 1660s. Isaac Newton studied Gassendi's work at an early age, and preferred his view to Descartes' theory of the "plenum". He stated in his "Hypothesis of Light" of 1675 that light was composed of corpuscles (particles of matter) which were emitted in all directions from a source. One of Newton's arguments against the wave nature of light was that waves were known to bend around obstacles, while light travelled only in straight lines. He did, however, explain the phenomenon of the diffraction of light (which had been observed by Francesco Grimaldi) by allowing that a light particle could create a localised wave in the aether. Newton's theory could be used to predict the reflection of light, but could only explain refraction by incorrectly assuming that light accelerated upon entering a denser medium because the gravitational pull was greater. Newton published the final version of his theory in his "Opticks" of 1704. His reputation helped the particle theory of light to hold sway during the 18th century. The particle theory of light led Laplace to argue that a body could be so massive that light could not escape from it. In other words it would become what is now called a black hole. Laplace withdrew his suggestion later, after a wave theory of light became firmly established as the model for light (as has been explained, neither a particle or wave theory is fully correct). A translation of Newton's essay on light appears in "The large scale structure of space-time," by Stephen Hawking and George F. R. Ellis. Wave theory. To explain the origin of colors, Robert Hooke (1635-1703) developed a "pulse theory" and compared the spreading of light to that of waves in water in his 1665 Micrographia ("Observation XI"). In 1672 Hooke suggested that light's vibrations could be perpendicular to the direction of propagation. Christiaan Huygens (1629-1695) worked out a mathematical wave theory of light in 1678, and published it in his "Treatise on light" in 1690. He proposed that light was emitted in all directions as a series of waves in a medium called the "Luminiferous ether". As waves are not affected by gravity, it was assumed that they slowed down upon entering a denser medium. The wave theory predicted that light waves could interfere with each other like sound waves (as noted around 1800 by Thomas Young), and that light could be polarised, as if it were a transverse wave. Young showed by means of a diffraction experiment that light behaved as waves. He also proposed that different colours were caused by different wavelengths of light, and explained colour vision in terms of three-coloured receptors in the eye. Another supporter of the wave theory was Leonhard Euler. He argued in "Nova theoria lucis et colorum" (1746) that diffraction could more easily be explained by a wave theory. Later, Augustin-Jean Fresnel independently worked out his own wave theory of light, and presented it to the Académie des Sciences in 1817. Siméon Denis Poisson added to Fresnel's mathematical work to produce a convincing argument in favour of the wave theory, helping to overturn Newton's corpuscular theory. By the year 1821, Fresnel was able to show via mathematical methods that polarisation could be explained only by the wave theory of light and only if light was entirely transverse, with no longitudinal vibration whatsoever. The weakness of the wave theory was that light waves, like sound waves, would need a medium for transmission. The existence of the hypothetical substance "luminiferous aether" proposed by Huygens in 1678 was cast into strong doubt in the late nineteenth century by the Michelson–Morley experiment. Newton's corpuscular theory implied that light would travel faster in a denser medium, while the wave theory of Huygens and others implied the opposite. At that time, the speed of light could not be measured accurately enough to decide which theory was correct. The first to make a sufficiently accurate measurement was Léon Foucault, in 1850. His result supported the wave theory, and the classical particle theory was finally abandoned, only to partly re-emerge in the 20th century. Electromagnetic theory as explanation for all types of visible light and all EM radiation. In 1845, Michael Faraday discovered that the plane of polarisation of linearly polarised light is rotated when the light rays travel along the magnetic field direction in the presence of a transparent dielectric, an effect now known as Faraday rotation. This was the first evidence that light was related to electromagnetism. In 1846 he speculated that light might be some form of disturbance propagating along magnetic field lines. Faraday proposed in 1847 that light was a high-frequency electromagnetic vibration, which could propagate even in the absence of a medium such as the ether. Faraday's work inspired James Clerk Maxwell to study electromagnetic radiation and light. Maxwell discovered that self-propagating electromagnetic waves would travel through space at a constant speed, which happened to be equal to the previously measured speed of light. From this, Maxwell concluded that light was a form of electromagnetic radiation: he first stated this result in 1862 in "On Physical Lines of Force". In 1873, he published "A Treatise on Electricity and Magnetism", which contained a full mathematical description of the behaviour of electric and magnetic fields, still known as Maxwell's equations. Soon after, Heinrich Hertz confirmed Maxwell's theory experimentally by generating and detecting radio waves in the laboratory, and demonstrating that these waves behaved exactly like visible light, exhibiting properties such as reflection, refraction, diffraction, and interference. Maxwell's theory and Hertz's experiments led directly to the development of modern radio, radar, television, electromagnetic imaging, and wireless communications. In the quantum theory, photons are seen as wave packets of the waves described in the classical theory of Maxwell. The quantum theory was needed to explain effects even with visual light that Maxwell's classical theory could not (such as spectral lines). Quantum theory. In 1900 Max Planck, attempting to explain black body radiation suggested that although light was a wave, these waves could gain or lose energy only in finite amounts related to their frequency. Planck called these "lumps" of light energy "quanta" (from a Latin word for "how much"). In 1905, Albert Einstein used the idea of light quanta to explain the photoelectric effect, and suggested that these light quanta had a "real" existence. In 1923 Arthur Holly Compton showed that the wavelength shift seen when low intensity X-rays scattered from electrons (so called Compton scattering) could be explained by a particle-theory of X-rays, but not a wave theory. In 1926 Gilbert N. Lewis named these liqht quanta particles photons. Eventually the modern theory of quantum mechanics came to picture light as (in some sense) "both" a particle and a wave, and (in another sense), as a phenomenon which is "neither" a particle nor a wave (which actually are macroscopic phenomena, such as baseballs or ocean waves). Instead, modern physics sees light as something that can be described sometimes with mathematics appropriate to one type of macroscopic metaphor (particles), and sometimes another macroscopic metaphor (water waves), but is actually something that cannot be fully imagined. As in the case for radio waves and the X-rays involved in Compton scattering, physicists have noted that electromagnetic radiation tends to behave more like a classical wave at lower frequencies, but more like a classical particle at higher frequencies, but never completely loses all qualities of one or the other. Visible light, which occupies a middle ground in frequency, can easily be shown in experiments to be describable using either a wave or particle model, or sometimes both [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@244efc35 [docID]17939 [hitPos]2 [correct]false [extraScores][F@45a7a256 , [answer]Faster-than-light (also superluminal or FTL) communications and travel refer to the propagation of information or matter faster than the speed of light. Under the special theory of relativity, a particle (that has rest mass) with subluminal velocity needs infinite energy to accelerate to the speed of light, although special relativity does not forbid the existence of particles that travel faster than light at all times (tachyons). On the other hand, what some physicists refer to as "apparent" or "effective" FTL depends on the hypothesis that unusually distorted regions of spacetime might permit matter to reach distant locations in less time than light could in normal or undistorted spacetime. Although according to current theories matter is still required to travel subluminally with respect to the locally distorted spacetime region, "apparent" FTL is not excluded by general relativity. Examples of FTL proposals are the Alcubierre drive and the traversable wormhole, although their physical plausibility is uncertain. FTL travel of non-information. In the context of this article, FTL is the transmission of information or matter faster than "c", a constant equal to the speed of light in a vacuum, which is 299,792,458 meters per second (by definition) or about 186,282.4 miles per second. This is not quite the same as traveling faster than light, since: Neither of these phenomena violates special relativity or creates problems with causality, and thus neither qualifies as "FTL" as described here. In the following examples, certain influences may appear to travel faster than light, but they do not convey energy or information faster than light, so they do not violate special relativity. Daily sky motion. For an earthbound observer, objects in the sky complete one revolution around the Earth in 1 day. Proxima Centauri, which is the nearest star outside the solar system, is about 4 light-years away. On a geostationary view Proxima Centauri has a speed many times greater than "c" as the rim speed of an object moving in a circle is a product of the radius and angular speed. It is also possible on a geostatic view for objects such as comets to vary their speed from subluminal to superluminal and vice versa simply because the distance from the Earth varies. Comets may have orbits which take them out to more than 1000 AU. The circumference of a circle with a radius of 1000 AU is greater than one light day. In other words, a comet at such a distance is superluminal in a geostatic —and therefore non-inertial— frame. Light spots and shadows. If a laser is swept across a distant object, the spot of laser light can easily be made to move across the object at a speed greater than "c". Similarly, a shadow projected onto a distant object can be made to move across the object faster than "c". In neither case does the light travel from the source to the object faster than "c", nor does any information travel faster than light. Apparent FTL propagation of static field effects. Since there is no "retardation" (or aberration) of the apparent position of the source of a gravitational or electric static field when the source moves with constant velocity, the static field "effect" may seem at first glance to be "transmitted" faster than the speed of light. However, uniform motion of the static source may be removed with a change in reference frame, causing the direction of the static field to change immediately, at all distances. This is not a change of position which "propagates", and thus this change cannot be used to transmit information from the source. No information or matter can be FTL-transmitted or propagated from source to receiver/observer by an electromagnetic field. Closing speeds. The rate at which two objects in motion in a single frame of reference get closer together is called the mutual or closing speed. This may approach twice the speed of light, as in the case of two particles travelling at close to the speed of light in opposite directions with respect to the reference frame. Imagine two fast-moving particles approaching each other from opposite sides of a particle accelerator of the collider type. The closing speed would be the rate at which the distance between the two particles is decreasing. From the point of view of an observer standing at rest relative to the accelerator, this rate will be slightly less than twice the speed of light. Special relativity does not prohibit this. It tells us that it is wrong to use Galilean relativity to compute the velocity of one of the particles, as would be measured by an observer traveling alongside the other particle. That is, special relativity gives the right formula for computing such relative velocity. It is instructive to compute the relative velocity of particles moving at "v" and -"v" in accelerator frame, which corresponds to the closing speed of 2"v" > "c". Expressing the speeds in units of "c", β = "v"/"c": Proper speeds. If a spaceship travels to a planet one light-year (as measured in the Earth's rest frame) away from Earth at high speed, the time taken to reach that planet could be less than one year as measured by the traveller's clock (although it will always be more than one year as measured by a clock on Earth). The value obtained by dividing the distance traveled, as determined in the Earth's frame, by the time taken, measured by the traveller's clock, is known as a proper speed or a proper velocity. There is no limit on the value of a proper speed as a proper speed does not represent a speed measured in a single inertial frame. A light signal that left the Earth at the same time as the traveller would always get to the destination before the traveller. How far can one travel from the Earth? Since one might not travel faster than light, one might conclude that a human can never travel further from the earth than 40 light-years if the traveler is active between the age of 20 and 60. A traveler would then never be able to reach more than the very few star systems which exist within the limit of 20-40 light-years from the Earth. This is a mistaken conclusion: because of time dilation, the traveler can travel thousands of light-years during their 40 active years. If the spaceship accelerates at a constant 1 g (in its own changing frame of reference), it will, after 354 days, reach speeds a little under the speed of light (for an observer on Earth), and time dilation will increase their lifespan to thousands of Earth years, seen from the reference system of the Solar System, but the traveler's subjective lifespan will not thereby change. If the traveler returns to the Earth, they will land thousands of years into the future. Their speed will not be seen as higher than the speed of light by observers on Earth, and the traveler will not measure their speed as being higher than the speed of light, but will see a length contraction of the universe in their direction of travel. And as the traveler turns around to return, the Earth will seem to experience much more time than the traveler does. So, although their (ordinary) speed cannot exceed "c", the four-velocity (distance as seen by Earth divided by their proper, i.e. subjective, time) can be much greater than "c". This is seen in statistical studies of muons traveling much further than "c" times their half-life (at rest), if traveling close to "c". Phase velocities above "c". The phase velocity of an electromagnetic wave, when traveling through a medium, can routinely exceed "c", the vacuum velocity of light. For example, this occurs in most glasses at X-ray frequencies. However, the phase velocity of a wave corresponds to the propagation speed of a theoretical single-frequency (purely monochromatic) component of the wave at that frequency. Such a wave component must be infinite in extent and of constant amplitude (otherwise it is not truly monochromatic), and so cannot convey any information. Thus a phase velocity above "c" does not imply the propagation of signals with a velocity above "c". Group velocities above "c". The group velocity of a wave (e.g., a light beam) may also exceed "c" in some circumstances. In such cases, which typically at the same time involve rapid attenuation of the intensity, the maximum of the envelope of a pulse may travel with a velocity above "c". However, even this situation does not imply the propagation of signals with a velocity above "c", even though one may be tempted to associate pulse maxima with signals. The latter association has been shown to be misleading, basically because the information on the arrival of a pulse can be obtained before the pulse maximum arrives. For example, if some mechanism allows the full transmission of the leading part of a pulse while strongly attenuating the pulse maximum and everything behind (distortion), the pulse maximum is effectively shifted forward in time, while the information on the pulse does not come faster than "c" without this effect. Universal expansion. The expansion of the universe causes distant galaxies to recede from us faster than the speed of light, if comoving distance and cosmological time are used to calculate the speeds of these galaxies. However, in general relativity, velocity is a local notion, so velocity calculated using comoving coordinates does not have any simple relation to velocity calculated locally. (See "comoving distance" for a discussion of different notions of 'velocity' in cosmology.) Rules that apply to relative velocities in special relativity, such as the rule that relative velocities cannot increase past the speed of light, do not apply to relative velocities in comoving coordinates, which are often described in terms of the "expansion of space" between galaxies. This expansion rate is thought to have been at its peak during the inflationary epoch thought to have occurred in a tiny fraction of the second after the Big Bang (models suggest the period would have been from around 10−36 seconds after the Big Bang to around 10−33 seconds), when the universe may have rapidly expanded by a factor of around 1020 to 1030. There are many galaxies visible in telescopes with red shift numbers of 1.4 or higher. All of these are currently traveling away from us at speeds greater than the speed of light. Because the Hubble parameter is decreasing with time, there can actually be cases where a galaxy that is receding from us faster than light does manage to emit a signal which reaches us eventually. However, because the expansion of the universe is accelerating, it is projected that most galaxies will eventually cross a type of cosmological event horizon where any light they emit past that point will never be able to reach us at any time in the infinite future, because the light never reaches a point where its "peculiar velocity" towards us exceeds the expansion velocity away from us (these two notions of velocity are also discussed in Comoving distance#Uses of the proper distance). The current distance to this cosmological event horizon is about 16 billion light-years, meaning that a signal from an event happening "at present" would eventually be able to reach us in the future if the event was less than 16 billion light-years away, but the signal would never reach us if the event was more than 16 billion light-years away. Astronomical observations. Apparent superluminal motion is observed in many radio galaxies, blazars, quasars and recently also in microquasars. The effect was predicted before it was observed by Martin Rees and can be explained as an optical illusion caused by the object partly moving in the direction of the observer, when the speed calculations assume it does not. The phenomenon does not contradict the theory of special relativity. Interestingly, corrected calculations show these objects have velocities close to the speed of light (relative to our reference frame). They are the first examples of large amounts of mass moving at close to the speed of light. Earth-bound laboratories have only been able to accelerate small numbers of elementary particles to such speeds. Quantum mechanics. Certain phenomena in quantum mechanics, such as quantum entanglement, might give the superficial impression of allowing communication information faster than light. According to the no-communication theorem these phenomena do not allow true communication; they only let two observers in different locations see the same system simultaneously, without any way of controlling what either sees. Wavefunction collapse can be viewed as an epiphenomenon of quantum decoherence, which in turn is nothing more than an effect of the underlying local time evolution of the wavefunction of a system and "all" of its environment. Since the "underlying" behaviour doesn't violate local causality or allow FTL it follows that neither does the additional effect of wavefunction collapse, whether real "or" apparent. The uncertainty principle implies that individual photons may travel for short distances at speeds somewhat faster (or slower) than "c", even in a vacuum; this possibility must be taken into account when enumerating Feynman diagrams for a particle interaction. It has since been proven that a single photon may not travel faster than "c". In quantum mechanics, virtual particles may travel faster than light, and this phenomenon is related to the fact that static field effects (which are mediated by virtual particles in quantum terms) may travel faster than light (see section on static fields above). However, macroscopically these fluctuations average out, so that photons do travel in straight lines over long (i.e., non-quantum) distances, and they do travel at the speed of light on average. Therefore, this does not imply the possibility of superluminal information transmission. There have been various reports in the popular press of experiments on faster-than-light transmission in optics—most often in the context of a kind of quantum tunnelling phenomenon. Usually, such reports deal with a phase velocity or group velocity faster than the vacuum velocity of light. However, as stated above, a superluminal "phase velocity" cannot be used for faster-than-light transmission of information. There has sometimes been confusion concerning the latter point. Additionally a channel that permits such propagation cannot be laid out faster than the speed of light. Quantum teleportation transmits quantum information at whatever speed is used to transmit the same amount of classical information, likely the speed of light. This quantum information may theoretically be used in ways that classical information can not, such as in quantum computations involving quantum information only available to the recipient. Hartman effect. The Hartman effect is the tunnelling effect through a barrier where the tunnelling time tends to a constant for large barriers. This was first described by Thomas Hartman in 1962. This could, for instance, be the gap between two prisms. When the prisms are in contact, the light passes straight through, but when there is a gap, the light is refracted. There is a nonzero probability that the photon will tunnel across the gap rather than follow the refracted path. For large gaps between the prisms the tunnelling time approaches a constant and thus the photons appear to have crossed with a superluminal speed. However, an analysis by Herbert G. Winful from the University of Michigan suggests that the Hartman effect cannot actually be used to violate relativity by transmitting signals faster than "c", because the tunnelling time "should not be linked to a velocity since evanescent waves do not propagate". The evanescent waves in the Hartman effect are due to virtual particles and a non-propagating static field, as mentioned in the sections above for gravity and electromagnetism. Casimir effect. In physics, the Casimir effect or Casimir-Polder force is a physical force exerted between separate objects due to resonance of vacuum energy in the intervening space between the objects. This is sometimes described in terms of virtual particles interacting with the objects, owing to the mathematical form of one possible way of calculating the strength of the effect. Because the strength of the force falls off rapidly with distance, it is only measurable when the distance between the objects is extremely small. Because the effect is due to virtual particles mediating a static field effect, it is subject to the comments about static fields discussed above. EPR Paradox. The EPR paradox refers to a famous thought experiment of Einstein, Podolski and Rosen that was realized experimentally for the first time by Alain Aspect in 1981 and 1982 in the Aspect experiment. In this experiment, the measurement of the state of one of the quantum systems of an entangled pair apparently instantaneously forces the other system (which may be distant) to be measured in the complementary state. However, no information can be transmitted this way; the answer to whether or not the measurement actually affects the other quantum system comes down to which interpretation of quantum mechanics one subscribes to. An experiment performed in 1997 by Nicolas Gisin at the University of Geneva has demonstrated non-local quantum correlations between particles separated by over 10 kilometers. But as noted earlier, the non-local correlations seen in entanglement cannot actually be used to transmit classical information faster than light, so that relativistic causality is preserved; see no-communication theorem for further information. A 2008 quantum physics experiment also performed by Nicolas Gisin and his colleagues in Geneva, Switzerland has determined that in any hypothetical non-local hidden-variables theory the speed of the "quantum non-local connection" (what Einstein called "spooky action at a distance") is at least 10,000 times the speed of light. Delayed choice quantum eraser. Delayed choice quantum eraser (an experiment of Marlan Scully) is a version of the EPR paradox in which the observation or not of interference after the passage of a photon through a double slit experiment depends on the conditions of observation of a second photon entangled with the first. The characteristic of this experiment is that the observation of the second photon can take place at a later time than the observation of the first photon, which may give the impression that the measurement of the later photons "retroactively" determines whether the earlier photons show interference or not, although the interference pattern can only be seen by correlating the measurements of both members of every pair and so it can't be observed until both photons have been measured, ensuring that an experimenter watching only the photons going through the slit does not obtain information about the other photons in an FTL or backwards-in-time manner. FTL communication possibility. Faster-than-light communication is, by Einstein's theory of relativity, equivalent to time travel. According to Einstein's theory of special relativity, what we measure as the speed of light in a vacuum is actually the fundamental physical constant "c". This means that all inertial observers, regardless of their relative velocity, will always measure zero-mass particles such as photons traveling at "c" in a vacuum. This result means that measurements of time and velocity in different frames are no longer related simply by constant shifts, but are instead related by Poincaré transformations. These transformations have important implications: Justifications. Faster light (Casimir vacuum and quantum tunnelling). Raymond Y. Chiao was first to measure the quantum tunnelling time, which was found to be between 1.5 to 1.7 times the speed of light. Einstein's equations of special relativity postulate that the speed of light in a vacuum is invariant in inertial frames. That is, it will be the same from any frame of reference moving at a constant speed. The equations do not specify any particular value for the speed of the light, which is an experimentally determined quantity for a fixed unit of length. Since 1983, the SI unit of length (the meter) has been defined using the speed of light. The experimental determination has been made in vacuum. However, the vacuum we know is not the only possible vacuum which can exist. The vacuum has energy associated with it, unsurprisingly called the vacuum energy. This vacuum energy can perhaps be changed in certain cases. When vacuum energy is lowered, light itself has been predicted to go faster than the standard value "c". This is known as the Scharnhorst effect. Such a vacuum can be produced by bringing two perfectly smooth metal plates together at near atomic diameter spacing. It is called a Casimir vacuum. Calculations imply that light will go faster in such a vacuum by a minuscule amount: a photon traveling between two plates that are 1 micrometer apart would increase the photon's speed by only about one part in 1036. Accordingly there has as yet been no experimental verification of the prediction. A recent analysis argued that the Scharnhorst effect cannot be used to send information backwards in time with a single set of plates since the plates' rest frame would define a "preferred frame" for FTL signalling. However, with multiple pairs of plates in motion relative to one another the authors noted that they had no arguments that could "guarantee the total absence of causality violations", and invoked Hawking's speculative chronology protection conjecture which suggests that feedback loops of virtual particles would create "uncontrollable singularities in the renormalized quantum stress-energy" on the boundary of any potential time machine, and thus would require a theory of quantum gravity to fully analyze. Other authors argue that Scharnhorst's original analysis which seemed to show the possibility of faster-than-"c" signals involved approximations which may be incorrect, so that it is not clear whether this effect could actually increase signal speed at all. The physicists Günter Nimtz and Alfons Stahlhofen, of the University of Cologne, claim to have violated relativity experimentally by transmitting photons faster than the speed of light. They say they have conducted an experiment in which microwave photons—relatively low energy packets of light—travelled "instantaneously" between a pair of prisms that had been moved up to apart. Their experiment involved an optical phenomenon known as "evanescent modes", and they claim that since evanescent modes have an imaginary wave number, they represent a "mathematical analogy" to quantum tunnelling. Nimtz has also claimed that "evanescent modes are not fully describable by the Maxwell equations and quantum mechanics have to be taken into consideration." Other scientists such as Herbert G. Winful and Robert Helling have argued that in fact there is nothing quantum-mechanical about Nimtz's experiments, and that the results can be fully predicted by the equations of classical electromagnetism (Maxwell's equations). Nimtz told "New Scientist" magazine: "For the time being, this is the only violation of special relativity that I know of." However, other physicists say that this phenomenon does not allow information to be transmitted faster than light. Aephraim Steinberg, a quantum optics expert at the University of Toronto, Canada, uses the analogy of a train traveling from Chicago to New York, but dropping off train cars at each station along the way, so that the center of the ever shrinking main train moves forward at each stop; in this way, the speed of the center of the train exceeds the speed of any of the individual cars. Herbert G. Winful argues that the train analogy is a variant of the "reshaping argument" for superluminal tunneling velocities, but he goes on to say that this argument is not actually supported by experiment or simulations, which actually show that the transmitted pulse has the same length and shape as the incident pulse. Instead, Winful argues that the group delay in tunneling is not actually the transit time for the pulse (whose spatial length must be greater than the barrier length in order for its spectrum to be narrow enough to allow tunneling), but is instead the lifetime of the energy stored in a standing wave which forms inside the barrier. Since the stored energy in the barrier is less than the energy stored in a barrier-free region of the same length due to destructive interference, the group delay for the energy to escape the barrier region is shorter than it would be in free space, which according to Winful is the explanation for apparently superluminal tunneling. A number of authors have published papers disputing Nimtz's claim that Einstein causality is violated by his experiments, and there are many other papers in the literature discussing why quantum tunneling is not thought to violate causality. It was later claimed by the Keller group in Switzerland that particle tunneling does indeed occur in zero real time. Their tests involved tunneling electrons, where the group argued a relativistic prediction for tunneling time should be 500-600 attoseconds (an attosecond is one quintillionth (10−18) of a second). All that could be measured was 24 attoseconds, which is the limit of the test accuracy. Again, though, other physicists believe that tunneling experiments in which particles appear to spend anomalously short times inside the barrier are in fact fully compatible with relativity, although there is disagreement about whether the explanation involves reshaping of the wave packet or other effects. Give up causality. Another approach is to accept special relativity, but to posit that mechanisms allowed by general relativity (e.g., wormholes) will allow traveling between two points without going through the intervening space. While this gets around the infinite acceleration problem, it still would lead to closed timelike curves (i.e., time travel) and causality violations. Causality is not required by special or general relativity, but is nonetheless generally considered a basic property of the universe that cannot be sensibly dispensed with. Because of this, most physicists expect that quantum gravity effects will preclude this option. An alternative is to conjecture that, while time travel is possible, it never leads to paradoxes; this is the Novikov self-consistency principle. Give up (absolute) relativity. Because of the strong empirical support for special relativity, any modifications to it must necessarily be quite subtle and difficult to measure. The best-known attempt is doubly special relativity, which posits that the Planck length is also the same in all reference frames, and is associated with the work of Giovanni Amelino-Camelia and João Magueijo. One consequence of this theory is a variable speed of light, where photon speed would vary with energy, and some zero-mass particles might possibly travel faster than "c". However, even if this theory is accurate, it is still very unclear whether it would allow information to be communicated, and appears not in any case to allow massive particles to exceed "c". There are speculative theories that claim inertia is produced by the combined mass of the universe (e.g., Mach's principle), which implies that the rest frame of the universe might be "preferred" by conventional measurements of natural law. If confirmed, this would imply special relativity is an approximation to a more general theory, but since the relevant comparison would (by definition) be outside the observable universe, it is difficult to imagine (much less construct) experiments to test this hypothesis. Space-time distortion. Although the theory of special relativity forbids objects to have a relative velocity greater than light speed, and general relativity reduces to special relativity in a local sense (in small regions of spacetime where curvature is negligible), general relativity does allow the space between distant objects to expand in such a way that they have a "recession velocity" which exceeds the speed of light, and it is thought that galaxies which are at a distance of more than about 14 billion light-years from us today have a recession velocity which is faster than light. Miguel Alcubierre theorized that it would be possible to create an Alcubierre drive, in which a ship would be enclosed in a "warp bubble" where the space at the front of the bubble is rapidly contracting and the space at the back is rapidly expanding, with the result that the bubble can reach a distant destination much faster than a light beam moving outside the bubble, but without objects inside the bubble locally traveling faster than light. However, several objections raised against the Alcubierre drive appear to rule out the possibility of actually using it in any practical fashion. Another possibility predicted by general relativity is the traversable wormhole, which could create a shortcut between arbitrarily distant points in space. As with the Alcubierre drive, travelers moving through the wormhole would not "locally" move faster than light which travels through the wormhole alongside them, but they would be able to reach their destination (and return to their starting location) faster than light traveling outside the wormhole. Dr. Gerald Cleaver, associate professor of physics at Baylor University, and Richard Obousy, a Baylor graduate student, theorize that by manipulating the extra spatial dimensions of string theory around a spaceship with an extremely large amount of energy, it would create a "bubble" that could cause the ship to travel faster than the speed of light. To create this bubble, the physicists believe manipulating the 10th spatial dimension would alter the dark energy in three large spatial dimensions: height, width and length. Cleaver said positive dark energy is currently responsible for speeding up the expansion rate of our universe as time moves on. Heim theory. In 1977, a paper on Heim theory theorized that it may be possible to travel faster than light by using magnetic fields to enter a higher-dimensional space. MiHsC/Quantised inertia. A new theory has been proposed that Modifies inertia by assuming it is due to Unruh radiation subject to a Hubble scale Casimir effect (MiHsC, or quantised inertia). MiHsC predicts a minimum possible acceleration even at light speed, implying that this speed can be exceeded. Lorentz symmetry violation. The possibility that Lorentz symmetry may be violated has been seriously considered in the last two decades, particularly after the development of a realistic effective field theory that describes this possible violation, the so-called Standard-Model Extension. This general framework has allowed experimental searches by ultra-high energy cosmic-ray experiments and a wide variety of experiments in gravity, electrons, protons, neutrons, neutrinos, mesons, and photons. The breaking of rotation and boost invariance causes direction dependence in the theory as well as unconventional energy dependence that introduces novel effects, including Lorentz-violating neutrino oscillations and modifications to the dispersion relations of different particle species, which naturally could make particles move faster than light. In some models of broken Lorentz symmetry, it is postulated that the symmetry is still built into the most fundamental laws of physics, but that spontaneous symmetry breaking of Lorentz invariance shortly after the Big Bang could have left a "relic field" throughout the universe which causes particles to behave differently depending on their velocity relative to the field; however, there are also some models where Lorentz symmetry is broken in a more fundamental way. If Lorentz symmetry can cease to be a fundamental symmetry at Planck scale or at some other fundamental scale, it is conceivable that particles with a critical speed different from the speed of light be the ultimate constituents of matter. In current models of Lorentz symmetry violation, the phenomenological parameters are expected to be energy-dependent. Therefore, as widely recognized, existing low-energy bounds cannot be applied to high-energy phenomena; however, many searches for Lorentz violation at high energies have been carried out using the Standard-Model Extension. Lorentz symmetry violation is expected to become stronger as one gets closer to the fundamental scale. Another recent theory (see EPR paradox above) resulting from the analysis of an EPR communication set up, has the simple device based on removing the effective retarded time terms in the Lorentz transform to yield a preferred absolute reference frame. This frame cannot be used to do physics (i.e., compute the influence of light-speed limited signals) but it provides an objective, absolute frame all could agree upon, if superluminal communication is possible. If this sounds indulgent, it allows simultaneity, absolute space and time and a deterministic universe (along with decoherence theory) whilst the status-quo permits time travel/causality paradoxes, subjectivity in the measurement process and multiple universes. Superfluid theories of physical vacuum. In this approach the physical vacuum is viewed as the quantum superfluid which is essentially non-relativistic whereas the Lorentz symmetry is not an exact symmetry of nature but rather the approximate description valid only for the small fluctuations of the superfluid background. Within the framework of the approach a theory was proposed in which the physical vacuum is conjectured to be the quantum Bose liquid whose ground-state wavefunction is described by the logarithmic Schrödinger equation. It was shown that the relativistic gravitational interaction arises as the small-amplitude collective excitation mode whereas relativistic elementary particles can be described by the particle-like modes in the limit of low momenta. The important fact is that at very high velocities the behavior of the particle-like modes becomes distinct from the relativistic one - they can reach the speed of light limit at finite energy; also the faster-than-light propagation is possible without requiring moving objects to have imaginary mass. Time of flight of neutrinos. MINOS experiment. In 2007 MINOS collaboration reported results measuring the flight-time of 3 GeV neutrinos yielding a speed exceeding that of light by 1.8-sigma significance. However, those measurements were considered to be statistically consistent with neutrinos traveling at the speed of light. After the detectors for the project were upgraded in 2012, MINOS corrected their initial result and found agreement with the speed of light. Further measurements are going to be conducted. OPERA neutrino anomaly. On September 22, 2011, a paper from the OPERA Collaboration indicated detection of 17 and 28 GeV muon neutrinos, sent 730 kilometers (454 miles) from CERN near Geneva, Switzerland to the Gran Sasso National Laboratory in Italy, traveling faster than light by a factor of 2.48×10−5 (approximately 1 in 40,000), a statistic with 6.0-sigma significance. On 18 November 2011, a second follow-up experiment by OPERA scientists confirmed their initial results. However, scientists were skeptical about the results of these experiments, the significance of which was disputed. In March 2012, the ICARUS collaboration failed to reproduce the OPERA results with their equipment, detecting neutrino travel time from CERN to the Gran Sasso National Laboratory indistinguishable from the speed of light. Later the OPERA team reported two flaws in their equipment set-up that had caused errors far outside of their original confidence interval: a fiber optic cable attached improperly, which caused the apparently faster-than-light measurements, and a clock oscillator ticking too fast. Tachyons. In special relativity, it is impossible to accelerate an object "to" the speed of light, or for a massive object to move "at" the speed of light. However, it might be possible for an object to exist which "always" moves faster than light. The hypothetical elementary particles with this property are called tachyonic particles. Attempts to quantize them failed to produce faster-than-light particles, and instead illustrated that their presence leads to an instability. Various theorists have suggested that the neutrino might have a tachyonic nature, while others have disputed the possibility. General relativity. General relativity was developed after special relativity to include concepts like gravity. It maintains the principle that no object can accelerate to the speed of light in the reference frame of any coincident observer. However, it permits distortions in spacetime that allow an object to move faster than light from the point of view of a distant observer. One such distortion is the Alcubierre drive, which can be thought of as producing a ripple in spacetime that carries an object along with it. Another possible system is the wormhole, which connects two distant locations as though by a shortcut. Both distortions would need to create a very strong curvature in a highly localized region of space-time and their gravity fields would be immense. To counteract the unstable nature, and prevent the distortions from collapsing under their own 'weight', one would need to introduce hypothetical exotic matter or negative energy. General relativity also recognizes that any means of faster-than-light travel could also be used for time travel. This raises problems with causality. Many physicists believe that the above phenomena are impossible and that future theories of gravity will prohibit them. One theory states that stable wormholes are possible, but that any attempt to use a network of wormholes to violate causality would result in their decay. In string theory, Eric G. Gimon and Petr Hořava have argued that in a supersymmetric five-dimensional Gödel universe, quantum corrections to general relativity effectively cut off regions of spacetime with causality-violating closed timelike curves. In particular, in the quantum theory a smeared supertube is present that cuts the spacetime in such a way that, although in the full spacetime a closed timelike curve passed through every point, no complete curves exist on the interior region bounded by the tube. Variable speed of light. In conventional physics, the speed of light in a vacuum is assumed to be a constant. However, theories exist which postulate that the speed of light is not a constant. The interpretation of this statement is as follows. The speed of light is a dimensional quantity and so, as has been emphasized in this context by João Magueijo, it cannot be measured. Measurable quantities in physics are, without exception, dimensionless, although they are often constructed as ratios of dimensional quantities. For example, when the height of a mountain is measured, what is really measured is the ratio of its height to the length of a meter stick. The conventional SI system of units is based on seven basic dimensional quantities, namely distance, mass, time, electric current, thermodynamic temperature, amount of substance, and luminous intensity. These units are defined to be independent and so cannot be described in terms of each other. As an alternative to using a particular system of units, one can reduce all measurements to dimensionless quantities expressed in terms of ratios between the quantities being measured and various fundamental constants such as Newton's constant, the speed of light and Planck's constant; physicists can define at least 26 dimensionless constants which can be expressed in terms of these sorts of ratios and which are currently thought to be independent of one another. By manipulating the basic dimensional constants one can also construct the Planck time, Planck length and Planck energy which make a good system of units for expressing dimensional measurements, known as Planck units. Magueijo's proposal used a different set of units, a choice which he justifies with the claim that some equations will be simpler in these new units. In the new units he fixes the fine structure constant, a quantity which some people, using units in which the speed of light is fixed, have claimed is time-dependent. Thus in the system of units in which the fine structure constant is fixed, the observational claim is that the speed of light is time-dependent. While it may be mathematically possible to construct such a system, it is not clear what additional explanatory power or physical insight such a system would provide, assuming that it does indeed accord with existing empirical data [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@61f25b53 [docID]11439 [hitPos]2 [correct]false [extraScores][F@2a75cf1b , [answer]The electromagnetic wave equation is a second-order partial differential equation that describes the propagation of electromagnetic waves through a medium or in a vacuum. It is a three-dimensional form of the wave equation. The homogeneous form of the equation, written in terms of either the electric field E or the magnetic field B, takes the form: where is the speed of light in a medium with permeability (formula_4), and permittivity (formula_5), and ∇2 is the Laplace operator. In a vacuum, "c" = "c"0 = 299,792,458 meters per second, which is the speed of light in free space. The electromagnetic wave equation derives from Maxwell's equations. It should also be noted that in most older literature, B is called the "magnetic flux density" or "magnetic induction". The origin of the electromagnetic wave equation. In his 1864 paper titled A Dynamical Theory of the Electromagnetic Field, Maxwell utilized the correction to Ampère's circuital law that he had made in part III of his 1861 paper . In "Part VI" of his 1864 paper titled "Electromagnetic Theory of Light", Maxwell combined displacement current with some of the other equations of electromagnetism and he obtained a wave equation with a speed equal to the speed of light. He commented: Maxwell's derivation of the electromagnetic wave equation has been replaced in modern physics education by a much less cumbersome method involving combining the corrected version of Ampère's circuital law with Faraday's law of induction. To obtain the electromagnetic wave equation in a vacuum using the modern method, we begin with the modern 'Heaviside' form of Maxwell's equations. In a vacuum- and charge-free space, these equations are: where ρ = 0 because there's no charge density in free space. Taking the curl of the curl equations gives: We can use the vector identity where V is any vector function of space. And where ∇V is a dyadic which when operated on by the divergence operator formula_10 yields a vector. Since then the first term on the right in the identity vanishes and we obtain the wave equations: where is the speed of light in free space. Covariant form of the homogeneous wave equation. These relativistic equations can be written in contravariant form as where the electromagnetic four-potential is with the Lorenz gauge condition: where is the d'Alembertian operator. (The square box is not a typographical error; it is the correct symbol for this operator.) Homogeneous wave equation in curved spacetime. The electromagnetic wave equation is modified in two ways, the derivative is replaced with the covariant derivative and a new term that depends on the curvature appears. where formula_19 is the Ricci curvature tensor and the semicolon indicates covariant differentiation. The generalization of the Lorenz gauge condition in curved spacetime is assumed: Inhomogeneous electromagnetic wave equation. Localized time-varying charge and current densities can act as sources of electromagnetic waves in a vacuum. Maxwell's equations can be written in the form of a wave equation with sources. The addition of sources to the wave equations makes the partial differential equations inhomogeneous. Solutions to the homogeneous electromagnetic wave equation. 0 sin(-ω"t" + k ⋅ r) and B= "B"0 sin(-ω"t" + k ⋅ r) --> The general solution to the electromagnetic wave equation is a linear superposition of waves of the form for virtually "any" well-behaved function "g" of dimensionless argument φ, where ω is the angular frequency (in radians per second), and k = ("kx", "ky", "kz") is the wave vector (in radians per meter). Although the function "g" can be and often is a monochromatic sine wave, it does not have to be sinusoidal, or even periodic. In practice, "g" cannot have infinite periodicity because any real electromagnetic wave must always have a finite extent in time and space. As a result, and based on the theory of Fourier decomposition, a real wave must consist of the superposition of an infinite set of sinusoidal frequencies. In addition, for a valid solution, the wave vector and the angular frequency are not independent; they must adhere to the dispersion relation: where "k" is the wavenumber and λ is the wavelength. The variable c can only be used in this equation when the electromagnetic wave is in a vacuum. Monochromatic, sinusoidal steady-state. The simplest set of solutions to the wave equation result from assuming sinusoidal waveforms of a single frequency in separable form: where Plane wave solutions. Consider a plane defined by a unit normal vector Then planar traveling wave solutions of the wave equations are and where r = "(x, y, z)" is the position vector (in meters). These solutions represent planar waves traveling in the direction of the normal vector n. If we define the z direction as the direction of n. and the x direction as the direction of E., then by Faraday's Law the magnetic field lies in the y direction and is related to the electric field by the relation formula_32. Because the divergence of the electric and magnetic fields are zero, there are no fields in the direction of propagation. This solution is the linearly polarized solution of the wave equations. There are also circularly polarized solutions in which the fields rotate about the normal vector. Spectral decomposition. Because of the linearity of Maxwell's equations in a vacuum, solutions can be decomposed into a superposition of sinusoids. This is the basis for the Fourier transform method for the solution of differential equations. The sinusoidal solution to the electromagnetic wave equation takes the form and where The wave vector is related to the angular frequency by where "k" is the wavenumber and λ is the wavelength. The electromagnetic spectrum is a plot of the field magnitudes (or energies) as a function of wavelength. Multipole expansion. Assuming monochromatic fields varying in time as formula_40, if one uses Maxwell's Equations to eliminate B, the electromagnetic wave equation reduces to the Helmholtz Equation for E: with "k = ω/c" as given above. Alternatively, one can eliminate E in favor of B to obtain: A generic electromagnetic field with frequency ω can be written as a sum of solutions to these two equations. The three-dimensional solutions of the Helmholtz Equation can be expressed as expansions in spherical harmonics with coefficients proportional to the spherical Bessel functions. However, applying this expansion to each vector component of E or B will give solutions that are not generically divergence-free (∇ · E = ∇ · B = 0), and therefore require additional restrictions on the coefficients. The multipole expansion circumvents this difficulty by expanding not E or B, but r · E or r · B into spherical harmonics. These expansions still solve the original Helmholtz equations for E and B because for a divergence-free field F, ∇2 (r · F) = r · (∇2 F). The resulting expressions for a generic electromagnetic field are: where formula_45 and formula_46 are the "electric multipole fields of order (l, m)", and formula_47 and formula_48 are the corresponding "magnetic multipole fields", and "aE(l,m)" and "aM(l,m)" are the coefficients of the expansion. The multipole fields are given by where "hl(1,2)(x)" are the spherical Hankel functions, "El(1,2)" and "Bl(1,2)" are determined by boundary conditions, and formula_53 are vector spherical harmonics normalized so that The multipole expansion of the electromagnetic field finds application in a number of problems involving spherical symmetry, for example antennae radiation patterns, or nuclear gamma decay. In these applications, one is often interested in the power radiated in the far-field. In this regions, the E and B fields asymptote to The angular distribution of the time-averaged radiated power is then given by Other solutions. Other spherically and cylindrically symmetric analytic solutions to the electromagnetic wave equations are also possible. In spherical coordinates the solutions to the wave equation can be written as follows: and These can be rewritten in terms of the spherical Bessel function. In cylindrical coordinates, the solutions to the wave equation are the ordinary Bessel function of integer order [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@15ab6789 [docID]2924436 [hitPos]2 [correct]false [extraScores][F@2935a446 , [answer]In the fundamental branches of modern physics, namely general relativity and its widely applicable subset special relativity, as well as relativistic quantum mechanics and relativistic quantum field theory, the Lorentz transformation is the transformation rule under which all four vectors and tensors containing physical quantities transform according to in spacetime. The prime examples of such four vectors are the four position and four momentum of a particle, and for fields the electromagnetic tensor and stress–energy tensor. The fact that these objects transform according to the Lorentz transformation is what mathematically "defines" them as vectors and tensors, see tensor. Given the components of the four vectors or tensors in some frame, the "transformation rule" allows one to determine the altered components of the same four vectors or tensors in another frame, which could be boosted or accelerated, relative to the original frame. A "boost" should not be conflated with spatial translation, rather it's characterized by the relative velocity between frames. The transformation rule itself depends on the relative motion of the frames. In the simplest case of two inertial frames the relative velocity between enters the transformation rule. For rotating reference frames or general non-inertial reference frames, more parameters are needed, including the relative velocity (magnitude and direction), the rotation axis and angle turned through. There are many ways to derive the Lorentz transformations utilizing a variety of mathematical tools, spanning from elementary algebra and hyperbolic functions, to linear algebra and group theory. This article provides a few of the easier ones to follow in the context of special relativity, for the simplest case of a Lorentz boost in standard configuration, i.e. two inertial frames moving relative to each other at constant (uniform) relative velocity less than the speed of light, and using Cartesian coordinates so that the "x" and "x"′ axes are collinear. Historical background. The usual treatment (e.g., Einstein's original work) is based on the invariance of the speed of light. However, this is not necessarily the starting point: indeed (as is exposed, for example, in the second volume of the "Course of Theoretical Physics" by Landau and Lifshitz), what is really at stake is the "locality" of interactions: one supposes that the influence that one particle, say, exerts on another can not be transmitted instantaneously. Hence, there exists a theoretical maximal speed of information transmission which must be invariant, and it turns out that this speed coincides with the speed of light in vacuum. The need for locality in physical theories was already noted by Newton (see Koestler's The Sleepwalkers), who considered the notion of an action at a distance "philosophically absurd" and believed that gravity must be transmitted by an agent (such as an interstellar aether) which obeys certain physical laws. Michelson and Morley in 1887 designed an experiment, employing an interferometer and a half-silvered mirror, that was accurate enough to detect aether flow. The mirror system reflected the light back into the interferometer. If there were an aether drift, it would produce a phase shift and a change in the interference that would be detected. However, no phase shift was ever found. The negative outcome of the Michelson–Morley experiment left the concept of aether (or its drift) undermined. There was consequent perplexity as to why light evidently behaves like a wave, without any detectable medium through which wave activity might propagate. In a 1964 paper, Erik Christopher Zeeman showed that the causality preserving property, a condition that is weaker in a mathematical sense than the invariance of the speed of light, is enough to assure that the coordinate transformations are the Lorentz transformations. From physical principles. The problem is usually restricted to two dimensions by using a velocity along the "x" axis such that the "y" and "z" coordinates do not intervene. The following is similar to that of Einstein. As in the Galilean transformation, the Lorentz transformation is linear since the relative velocity of the reference frames is constant as a vector; otherwise, inertial forces would appear. They are called inertial or Galilean reference frames. According to relativity no Galilean reference frame is privileged. Another condition is that the speed of light must be independent of the reference frame, in practice of the velocity of the light source. Spherical wavefronts of light. Consider two inertial frames of reference "O" and "O"′, assuming "O" to be at rest while "O"′ is moving with a velocity "v" with respect "O" along the positive direction of "x"-axis. The origins of "O" and "O"′ initially coincide each other. Let a light signal is emitted from the common origin and travels as a spherical wave fronts. Consider a point "P" on a spherical wavefront is at a distance "r" and "r"′ from the origin of "O" and "O"′ respectively. According to second postulate of special theory of relativity as the speed of light is same in both the frames so "r" and "r"′ will be different only if "t" and "t"′ are different, The equation of the spherical wavefront in frame "O" will be, or similarly, the equation of the spherical wavefront in frame "O"′ will be, or since "O"′ is moving along "x"-axis, therefore, The relation between "x" and "x"′ should be in linear form and in such a way that it should reduce to Galilean transformation at "v" ≪ "c". Therefore, such a relation can be written of the form: where γ, not necessarily a constant, is to be determined. The inverse is: The above two equations gives the relation between "t" and "t"′ as: or substituting the expressions of "x"′, "y"′, "z"′ and "t"′ in terms of "x", "y", "z" and "t" in spherical wavefront equation of "O"′ frame we get, or and therefore, which implies, comparing the coefficients of "t"2 from above equation with the spherical wavefront equation of "O" frame we yield, or which is called the Lorentz factor. Thus we yield the Lorentz transformation from the above expression and is given by, Galilean and Einstein's relativity. In classical kinematics, the total displacement "x" in the R frame is the sum of the relative displacement "x"′ in frame R′ and of the distance between the two origins "x" − "x"′. If "v" is the relative velocity of R′ relative to R, the transformation is: "x" = "x"′ + "vt", or "x"′ = "x" − "vt". This relationship is linear for a constant "v", that is when "R" and "R"′ are Galilean frames of reference. In Einstein's relativity, the main difference from Galilean relativity is that space and time coordinates are intertwined, and in different inertial frames "t" ≠ "t"′. Since space is assumed to be homogeneous, the transformation must be linear. The most general linear relationship is obtained with four constant coefficients, "A", "B", γ, and "b": The Lorentz transformation becomes the Galilean transformation when γ = "B" = 1, "b" = −"v" and "A" = 0. An object at rest in the R′ frame at position "x"′ = 0 moves with constant velocity "v" in the R frame. Hence the transformation must yield "x"′ = 0 if "x" = "vt". Therefore, "b" = −"γv" and the first equation is written as According to the principle of relativity, there is no privileged Galilean frame of reference: therefore the inverse transformation for the position from frame "R"′ to frame "R" should have the same form as the original. To take advantage of this, we arrange by reversing the axes that "R"′ sees "R" moving in the positive "x"′ direction (i.e. just as "R" sees "R"′ in the positive "x" direction ), so that we can write which, when multiplied through by −1, becomes Since the speed of light is the same in all frames of reference, for the case of a light signal, the transformation must guarantee that "t" = "x"/"c" and "t"′ = "x"′/"c". Substituting for "t" and "t"′ in the preceding equations gives: Multiplying these two equations together gives, At any time after "t" = "t"′ = 0, "xx"′ is not zero, so dividing both sides of the equation by "xx"′ results in which is called the "Lorentz factor". The transformation equation for time can be easily obtained by considering the special case of a light signal, satisfying Substituting term by term into the earlier obtained equation for the spatial coordinate gives so that which determines the transformation coefficients "A" and "B" as So "A" and "B" are the unique coefficients necessary to preserve the constancy of the speed of light in the primed system of coordinates. Einstein's popular derivation. In his popular book Einstein derived the Lorentz transformation by arguing that there must be two non-zero coupling constants λ and μ such that that correspond to light traveling along the positive and negative x-axis, respectively. For light "x" = "ct" if and only if "x"′ = "ct"′. Adding and subtracting the two equations and defining gives Substituting "x"′ = 0 corresponding to "x" = "vt" and noting that the relative velocity is "v" = "bc"/"γ", this gives The constant γ can be evaluated as was previously shown above. The Lorentz transformations can also be derived by simple application of the special relativity postulates and using hyperbolic identities. It is sufficient to derive the result in for a boost in the one direction, since for an arbitrary direction the decomposition of the position vector into parallel and perpendicular components can be done after, and generalizations therefrom follow, as outlined above. Hyperbolic geometry. Start from the equations of the spherical wave front of a light pulse, centred at the origin: which take the same form in both frames because of the special relativity postulates. Next, consider relative motion along the "x"-axes of each frame, in standard configuration above, so that "y" = "y"′, "z" = "z"′, which simplifies to Now assume that the transformations take the linear form: where "A", "B", "C", "D" are to be found. If they were non-linear, they would not take the same form for all observers, since fictitious forces (hence accelerations) would occur in one frame even if the velocity was constant in another, which is inconsistent with inertial frame transformations. Substituting into the previous result: and comparing coefficients of "x"2, "t"2, "xt": The formulae resemble the hyperbolic identity Introducing the rapidity parameter "ϕ" as a parametric hyperbolic angle allows the self-consistent identifications where the signs after the square roots are chosen so that "x" and "t" increase. The hyperbolic transformations have been solved for: If the signs were chosen differently the position and time coordinates would need to be replaced by −"x" and/or −"t" so that "x" and "t" increase not decrease. To find what ϕ actually is, from the standard configuration the origin of the primed frame "x"′ = 0 is measured in the unprimed frame to be "x" = "vt" (or the equivalent and opposite way round; the origin of the unprimed frame is "x" = 0 and in the primed frame it is at "x"′ = −"vt"): and manipulation of hyperbolic identities leads to so the transformations are also: From group postulates. Following is a classical derivation (see, e.g., [http://arxiv.org/abs/gr-qc/0107091] and references therein) based on group postulates and isotropy of the space. The coordinate transformations between inertial frames form a group (called the proper Lorentz group) with the group operation being the composition of transformations (performing one transformation after another). Indeed the four group axioms are satisfied: Let us consider two inertial frames, "K" and "K"′, the latter moving with velocity v with respect to the former. By rotations and shifts we can choose the "x" and "x"′ axes along the relative velocity vector and also that the events ("t", "x") = (0, 0) and ("t"′, "x"′) = (0, 0) coincide. Since the velocity boost is along the "x" (and "x"′) axes nothing happens to the perpendicular coordinates and we can just omit them for brevity. Now since the transformation we are looking after connects two inertial frames, it has to transform a linear motion in ("t", "x") into a linear motion in ("t"′, "x"′) coordinates. Therefore it must be a linear transformation. The general form of a linear transformation is where α, β, γ, and δ are some yet unknown functions of the relative velocity "v". Let us now consider the motion of the origin of the frame "K"′. In the "K"′ frame it has coordinates ("t"′, "x"′ = 0), while in the "K" frame it has coordinates ("t", "x" = "vt"). These two points are connected by the transformation from which we get Analogously, considering the motion of the origin of the frame "K", we get from which we get Combining these two gives α = γ and the transformation matrix has simplified, Now let us consider the group postulate "inverse element". There are two ways we can go from the "K"′ coordinate system to the "K" coordinate system. The first is to apply the inverse of the transform matrix to the "K"′ coordinates: The second is, considering that the "K"′ coordinate system is moving at a velocity "v" relative to the "K" coordinate system, the "K" coordinate system must be moving at a velocity −"v" relative to the "K"′ coordinate system. Replacing "v" with −"v" in the transformation matrix gives: Now the function γ can not depend upon the direction of "v" because it is apparently the factor which defines the relativistic contraction and time dilation. These two (in an isotropic world of ours) cannot depend upon the direction of "v". Thus, γ(−"v") = γ("v") and comparing the two matrices, we get According to the "closure" group postulate a composition of two coordinate transformations is also a coordinate transformation, thus the product of two of our matrices should also be a matrix of the same form. Transforming "K" to "K"′ and from "K"′ to "K"′′ gives the following transformation matrix to go from "K" to "K"′′: In the original transform matrix, the main diagonal elements are both equal to γ, hence, for the combined transform matrix above to be of the same form as the original transform matrix, the main diagonal elements must also be equal. Equating these elements and rearranging gives: The denominator will be nonzero for nonzero "v", because γ("v") is always nonzero; If "v" = 0 we have the identity matrix which coincides with putting "v" = 0 in the matrix we get at the end of this derivation for the other values of "v", making the final matrix valid for all nonnegative "v". For the nonzero "v", this combination of function must be a universal constant, one and the same for all inertial frames. Define this constant as δ("v")/"v"γ("v") = κ where κ has the dimension of 1/"v"2. Solving we finally get and thus the transformation matrix, consistent with the group axioms, is given by If κ > 0, then there would be transformations (with κ"v"2 ≫ 1) which transform time into a spatial coordinate and vice versa. We exclude this on physical grounds, because time can only run in the positive direction. Thus two types of transformation matrices are consistent with group postulates: If κ = 0 then we get the Galilean-Newtonian kinematics with the Galilean transformation, where time is absolute, "t"′ = "t", and the relative velocity "v" of two inertial frames is not limited. If κ < 0, then we set "c" = 1/√(−κ) which becomes the invariant speed, the speed of light in vacuum. This yields κ = −1/"c"2 and thus we get special relativity with Lorentz transformation where the speed of light is a finite universal constant determining the highest possible relative velocity between inertial frames. If "v" ≪ "c" the Galilean transformation is a good approximation to the Lorentz transformation. Only experiment can answer the question which of the two possibilities, κ = 0 or κ < 0, is realised in our world. The experiments measuring the speed of light, first performed by a Danish physicist Ole Rømer, show that it is finite, and the Michelson–Morley experiment showed that it is an absolute speed, and thus that κ < 0. [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@58777339 [docID]39499535 [hitPos]2 [correct]false [extraScores][F@5e675bb1 , [answer]Measurements of neutrino speed have been conducted as tests of special relativity and for the determination of the mass of neutrinos. Astronomical searches investigate whether light and neutrinos emitted simultaneously from a distant source are arriving simultaneously on Earth. Terrestrial searches include time of flight measurements using synchronized clocks, and direct comparison of neutrino speed with the speed of other particles. Since it is established that neutrinos possess mass, the speed of neutrinos should be slightly smaller than the speed of light in accordance with special relativity. Existing measurements provided upper limits for deviations of approximately 10−9, or a few parts per billion. Within the margin of error this is consistent with no deviation at all. Overview. [[File:RelNeutrinoSpeed.svg|thumb|350px|Neutrino speed as a function of relativistic kinetic energy, with neutrino mass < 0.2 eV/c². It was assumed for a long time in the framework of the standard model of particle physics, that neutrinos are massless. Thus they should travel at exactly the speed of light according to special relativity. However, since the discovery of neutrino oscillations it is assumed that they are massive. Thus they should travel slightly slower than the speed of light, otherwise their relativistic energy would become infinitely large. This energy is given by "v" being the neutrino speed and "c" the speed of light. The neutrino mass "m" is currently estimated as being 2 eV/c², and is possibly even lower than 0.2 eV/c². According to the latter mass value and the formula for relativistic energy, relative speed differences between light and neutrinos are smaller at high energies, and should arise as indicated in the figure on the right. Time-of-flight measurements conducted so far investigated neutrinos of energy above 10 MeV. However, velocity differences predicted by relativity at such high energies cannot be determined with the current precision of time measurement. The reason why such measurements are still conducted, is connected with the theoretical possibility that significantly larger deviations from light speed might arise under certain circumstances. For instance, it was postulated that neutrinos might be some sort of superluminal particles called tachyons, even though others criticized this proposal. While hypothetical tachyons are thought to be compatible with Lorentz invariance, superluminal neutrinos have also been studied in Lorentz invariance violating frameworks as motivated by speculative variants of quantum gravity, such as the Standard-Model Extension according to which Lorentz-violating neutrino oscillations can arise. Besides time-of-flight measurements, those models also allow for indirect determinations of neutrino speed and other modern searches for Lorentz violation. All of those experiments confirmed Lorentz invariance and special relativity. Fermilab (1970s). Fermilab conducted in the 1970s a series of terrestrial measurements, in which the speed of muons was compared with that of neutrinos and antineutrinos of energies between 30 and 200 GeV. The Fermilab narrow band neutrino beam was generated as follows: 400-GeV protons are hitting the target and causing the production of secondary beams consisting of pions and kaons. Then they are decaying in an evacuated decay tube of 235 meter length. The remaining hadrons were stopped by a secondary dump, so that only neutrinos and some energetic muons can penetrate the earth- and steel shield of 500 meter length, in order to reach the particle detector. Since the protons are transferred in bunches of one nanosecond duration at an interval of 18.73 ns, the speed of muons and neutrinos could be determined. A speed difference would lead to an elongation of the neutrino bunches and to a displacement of the whole neutrino time spectrum. At first, the speeds of muons and neutrinos were compared. Later, also antineutrinos were observed. The upper limit for deviations from light speed was: This was in agreement with the speed of light within the measurement accuracy (95% confidence level), and also no energy dependence of neutrino speeds could be found at this accuracy. Supernova 1987A. The most precise agreement with the speed of light () was determined in 1987 by the observation of electron antineutrinos of energies between 7.5 and 35 MeV originated at the Supernova 1987A at a distance of 157000 ± 16000 light years. The upper limit for deviations from light speed was: thus 1.000000002 times the speed of light. This value was obtained by comparing the arrival times of light and neutrinos. The difference of approximately three hours was explained by the circumstance, that the almost noninteracting neutrinos could pass the supernova unhindered while light required a longer time. MINOS (2007). The first terrestrial measurement of the absolute transit time was conducted by MINOS (2007) at Fermilab. In order to generate neutrinos (the so-called NuMI beam) they used the Fermilab Main Injector, by which 120-GeV-protons were directed to a graphite target in 5 to 6 batches per spill. The emerging mesons decayed in a 675 meter long decay tunnel into muon neutrinos (93%) and muon antineutrinos (6%). The travel time was determined by comparing the arrival times at the MINOS near- and far detector, apart from each other by 734 km. The clocks of both stations were synchronized by GPS, and long optical fibers were used for signal transmission. They measured an early neutrino arrival of approximately 126 ns. Thus the relative speed difference was formula_4 (68% confidence limit). This corresponds to 1.000051±29 times the speed of light, thus apparently faster than light. The major source of error were uncertainties in the fiber optic delays. The statistical significance of this result was less than 1.8σ, thus it was not significant since 5σ is required to be accepted as a scientific discovery. At 99% confidence level it was given a neutrino speed larger than 0.999976c and lower than 1.000126c. Thus the result is also compatible with subluminal speeds. OPERA (2011, 2012). Anomaly. In the OPERA experiment, 17-GeV neutrinos have been used, split in proton extractions of 10,5 µs length generated at CERN, which hit a target at a distance of 743 km. Then pions and kaons are produced which partially decayed into muons and muon neutrinos (CERN Neutrinos to Gran Sasso, CNGS). The neutrinos traveled further to the Laboratori Nazionali del Gran Sasso (LNGS) 730 km away, where the OPERA detector is located. GPS was used to synchronize the clocks and to determine the exact distance. In addition, optical fibers were used for signal transmission at LNGS. The temporal distribution of the proton extractions was statistically compared with approximately 16000 neutrino events. OPERA measured an early neutrinos arrival of approximately 60 nanoseconds, as compared to the expected arrival at the speed of light, thus indicating a neutrino speed faster than that of light. Contrary to the MINOS result, the deviation was 6σ and thus apparently significant. To exclude possible statistical errors, CERN produced bunched proton beams between October and November 2011. The proton extractions were split into short bunches of 3 ns at intervals of 524 ns, so that every neutrino event could be directly connected to a proton bunch. The measurement of twenty neutrino events again gave an early arrival of about 62 ns, in agreement with the previous result. They updated their analysis and increased the significance up to 6,2σ. In February and March 2012, it was shown that there were two mistakes in the experimental equipment: An erroneous cable connection at a computer card, making the neutrinos appearing faster than expected. The other one was an oscillator out of its specification, making the neutrinos appearing slower than expected. Then the time of arrival of cosmic high-energy muons at OPERA and the co-located LVD detector between 2007–2008, 2008–2011, and 2011–2012 were compared. It was found out that between 2008–2011, the cable connector error caused a deviation of approximately 73 ns, and the oscillator error caused ca. 15 ns in the opposite direction. This and the measurement of neutrino velocities consistent with the speed of light by the ICARUS collaboration (see ICARUS (2012)), indicated that the neutrinos were actually not faster than light. End result. Finally, in July 2012 the OPERA collaboration published a new analysis of their data from 2009–2011, which included the instrumental effects stated above, and obtained bounds for arrival time differences (compared to the speed of light): and bounds for speed differences: Also the corresponding new analysis for the bunched beam of October and November 2011 agreed with this result: All of those results are consistent with the speed of light, and the formula_9 bound for the speed difference is more precise by one order of magnitude than previous terrestrial time-of-flight measurements. ICARUS (2012). Already before the OPERA collaboration updated their result, the ICARUS collaboration published measurements of neutrino velocity in March 2012. The ICARUS detector is also located at LNGS, and is receiving CNGS neutrinos from CERN. Simultaneously with OPERA (and using some of the LNGS equipment that was also used by OPERA), they tried to capture neutrino events produced at the bunched beam rerun from October to November 2011. They observed seven neutrino events and found the following arrival time compared to the expected arrival time at the speed of light: This means that no difference exists between the speed of neutrinos and the speed of light within the margin of error. LNGS (2012). Continuing the OPERA and ICARUS measurements, the LNGS experiments Borexino, LVD, OPERA and ICARUS conducted new tests between 10 and 24 May 2012, after CERN provided another bunched beam rerun. All measurements were consistent with the speed of light. The 17-GeV muon neutrino beam consisted of 4 batches per extraction separated by ~300ns, and the batches consisted of 16 bunches separated by ~100ns, with a bunch width of ~2ns. Borexino. The Borexino collaboration analyzed both the bunched beam rerun of Oct.–Nov. 2011 and the second rerun of May 2012. For the 2011 data, they evaluated 36 neutrino events and obtained an upper limit for time of flight differences: For the May 2012 measurements, they improved their equipment by installing a new analogue small–jitter triggering system and a geodetic GPS receiver coupled to a Rb clock. They also conducted an independent high precision geodesy measurement together with LVD and ICARUS. 62 neutrino events could be used for the final analysis, giving a more precise upper limit for time of flight differences corresponding to LVD. The LVD collaboration first analyzed the beam rerun of Oct.–Nov. 2011. They evaluated 32 neutrino events and obtained an upper limit for time of flight differences: In the May 2012 measurements, they used the new LNGS timing facility by the Borexino collaboration, and the geodetic data obtained by LVD, Borexino, and ICARUS (see above). They also updated their Scintillation counters and the trigger. 48 neutrino events (at energies above 50 MeV, average neutrino energy was 17 GeV) have been used for the May analysis, improving the upper limit for time of flight differences corresponding to ICARUS. After publishing the analysis of the beam rerun of Oct.–Nov. 2011 (see above), the ICARUS collaboration also provided an analysis of the May rerun. They substantially improved their own internal timing system and between CERN-LNGS, used the geodetic LNGS measurement together with Borexino and LVD, and employed Borexino's timing facility. 25 neutrino events have been evaluated for the final analysis, yielding an upper limit for time of flight differences: corresponding to Neutrino velocities exceeding the speed of light by more than formula_19 (95% C.L.) are excluded. OPERA. After the correction of the initial results, OPERA published their May 2012 measurements as well. An additional, independent timing system and four different methods of analysis were used for the evaluation of the neutrino events. They provided an upper limit for time of flight differences between light and muon neutrinos (48 to 59 neutrino events depending on the method of analysis): and between light and anti-muon neutrinos (3 neutrino events): consistent with the speed of light in the range of MINOS (2012). Old timing system. The MINOS collaboration further elaborated on their speed measurements of 2007. They examined the data collected over seven years, improved the GPS timing system and the understanding of the delays of electronic components, and also used upgraded timing equipment. The neutrinos span a 10 μs spill containing 5-6 batches. The analyses have been conducted in two ways. First, as in the 2007 measurement, the data at the far detector was statistically determined by the data of the near detector ("Full Spill Approach"): Second, the data connected with the batches themselves have been used ("Wrapped Spill Approach"): This is consistent with neutrinos traveling at the speed of light, and substantially improves their preliminary 2007 results. New timing system. In order to further improve the precision, a new timing system was developed. In particular, a "Resistive Wall Current Monitor" (RWCM) measuring the time distribution of the proton beam, CS atomic clocks, dual frequency GPS receivers, and auxiliary detectors to measure detector latencies have been installed. For the analysis, the neutrino events could be connected with a specific 10μs proton spill, from which a likelihood analysis was generated, and then the likelihoods of different events have been combined. The result: and Additional precision measurements are planned with the improved MINOS+ detector. Indirect determinations of neutrino speed. Lorentz violating frameworks such as the Standard-Model Extension including Lorentz-violating neutrino oscillations also allow for indirect determinations of deviations between light speed and neutrino speed by measuring their energy and the decay rates of other particles over large distances. By this method, much more stringent bounds can be obtained, such as by Borriello "et al.": For more such indirect bounds on superluminal neutrinos, see Modern searches for Lorentz violation#Neutrino speed [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@12925ecb [docID]35182609 [hitPos]3 [correct]false [extraScores][F@4bb5efaf , [answer]The velocity factor (VF), also called wave propagation speed or velocity of propagation (VoP or formula_1), of a transmission medium is the speed at which a wavefront (of an acoustic signal, for example, or an electromagnetic signal, a radio signal, a light pulse in a fibre channel or a change of the electrical voltage on a copper wire) passes through the medium, relative to the speed of light. For optical signals, the velocity factor is the reciprocal of the refractive index. The speed of radio signals in a vacuum, for example, is the speed of light, and so the velocity factor of a radio wave in a vacuum is unity, or 100%. In electrical cables, the velocity factor mainly depends on the insulating material (see table below). The use of the terms "velocity of propagation" and "wave propagation speed" to mean a ratio of speeds is confined to the computer networking and cable industries. In a general science and engineering context, these terms would be understood to mean a true speed or velocity in units of distance per time, while "velocity factor" is used for the ratio. Typical velocity factors. Velocity factor is an important characteristic of communication media such as Category 5 cables and radio transmission lines. Plenum data cable typically has a VF between 0.42 and 0.72 (42% to 72% of the speed of light) and riser cable around 0.70. A VF of 0.70 corresponds to a speed of approximately 210,000,000 m/s or 4.76 ns to travel one meter. Some typical velocity factors for radio communications cables are provided in the ARRL Handbook: Calculating velocity factor. VF equals the reciprocal of the square root of the dielectric constant (relative permittivity), formula_2, of the material through which the signal passes: The VF of a lossless transmission line is given by: where "c" is the speed of light, "L" is the distributed inductance (in henries per unit length) and "C" is the capacitance between the two conductors (in farads per unit length [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@15ab6789 [docID]1072520 [hitPos]3 [correct]false [extraScores][F@77a70f5 , [answer]Emission theory (also called emitter theory or ballistic theory of light) was a competing theory for the special theory of relativity, explaining the results of the Michelson-Morley experiment. Emission theories obey the principle of relativity by having no preferred frame for light transmission, but say that light is emitted at speed "c" relative to its source instead of applying the invariance postulate. Thus, emitter theory combines electrodynamics and mechanics with a simple Newtonian theory. Although there are still proponents of this theory outside the scientific mainstream, this theory is considered to be conclusively discredited by most scientists. History. The name most often associated with emission theory is Isaac Newton. In his "Corpuscular theory" Newton visualized light "corpuscles" being thrown off from hot bodies at a nominal speed of c with respect to the emitting object, and obeying the usual laws of Newtonian mechanics, and we then expect light to be moving towards us with a speed that is offset by the speed of the distant emitter (c ± v). In the 20th century, special relativity was created by Albert Einstein to solve the apparent conflict between electrodynamics and the principle of relativity. The theory's geometrical simplicity was persuasive, and the majority of scientists accepted relativity by 1911. However, a few scientists rejected the second basic postulate of relativity: the constancy of the speed of light in all inertial frames. So different types of emission theories were proposed where the speed of light depends on the velocity of the source, and the Galilean transformation is used instead of the Lorentz transformation. All of them can explain the negative outcome of the Michelson-Morley experiment, since the speed of light is constant with respect to the interferometer in all frames of reference. Some of those theories were: Albert Einstein is supposed to have worked on his own emission theory before abandoning it in favor of his special theory of relativity. Many years later R.S. Shankland reports Einstein as saying that Ritz' theory had been "very bad" in places and that he himself had eventually discarded emission theory because he could think of no form of differential equations that described it, since it leads to the waves of light becoming "all mixed up". Refutations of emission theory. The following scheme was introduced by de Sitter to test emission theories: where "c" is the speed of light, "v" that of the source, "c' " the resultant speed of light, and "k" a constant denoting the extent of source dependence which can attain values between 0 and 1. According to special relativity and the stationary aether, "k"=0, while emission theories allow values up to 1. Numerous terrestrial experiments have been performed, over very short distances, where no "light dragging" or extinction effects could come into play, and again the results confirm that light speed is independent of the speed of the source, conclusively ruling out emission theories. Astronomical sources. In 1910 Daniel Frost Comstock and in 1913 Willem de Sitter wrote that for the case of a double-star system seen edge-on, light from the approaching star might be expected to travel faster than light from its receding companion, and overtake it. If the distance was great enough for an approaching star's "fast" signal to catch up with and overtake the "slow" light that it had emitted earlier when it was receding, then the image of the star system should appear completely scrambled. De Sitter argued that none of the star systems he had studied showed the extreme optical effect behavior, and this was considered the death knell for Ritzian theory and emission theory in general, with formula_2. The idea that perhaps the speed of light only has an effective value of cEMITTER while it is "local" to the emitter, as a "light-dragging" or "proximity" effect has been considered in detail by Fox. This can be expressed in terms of the "extinction effect", and it arguably undermines the cogency of de Sitter type evidence based on optical stars. However, similar observations have been made more recently in the x-ray spectrum by Brecher (1977), which have a long enough extinction distance that it should not affect the results. The observations confirm that the speed of light is independent of the speed of the source, with formula_3. Hans Thirring argued in 1926, that an atom which is accelerated during the emission process by thermal collisions in the sun, is emitting light rays having different velocities at their start- and endpoints. So one end of the light ray would overtake the preceding parts, and consequently the distance between the ends would be elongated up to 500 km until they reach Earth, so that the mere existence of sharp spectral lines in the sun's radiation, disproves the ballistic model. Terrestrial sources. Such experiments include that of Sadeh (1963) who used a time-of-flight technique to measure velocity differences of photons traveling in opposite direction, which were produced by positron annihilation. Another experiment was conducted by Alväger et al. (1963), who compared the time of flight of gamma rays from moving and resting sources. Both experiments found no difference, in accordance with relativity. Filippas and Fox (1964) did not consider Sadeh (1963) and Alväger (1963) to have sufficiently controlled for the effects of extinction. So they conducted an experiment using a setup specifically designed to account for extinction. Data collected from various detector-target distances were consistent with there being no dependence of the speed of light on the velocity of the source, and were inconsistent with modeled behavior assuming c ± v both with and without extinction. Continuing their previous investigations, Alväger et al. (1964) observed π0-mesons which decay into photons at 99.9% light speed. The experiment showed that the photons didn't attain the velocity of their sources and still traveled at the speed of light, with formula_4. The investigation of the media which were crossed by the photons showed that the extinction shift was not sufficient to distort the result significantly. Also measurements of neutrino speed have been conducted. Mesons travelling nearly at light speed were used as sources. Since neutrinos only participate in the electroweak interaction, extinction plays no role. Terrestrial measurements provided upper limits of formula_5. Interferometry. The Sagnac effect demonstrates that one beam on a rotating platform covers less distance than the other beam, which creates the shift in the interference pattern. As Georges Sagnac stated, his experiment directly shows that the speed of light is independent of the velocity of the source. And since the Sagnac effect also occurs in vacuum, extinction effects play no role. The predictions of Ritz's version of emission theory were consistent with almost all terrestrial interferometric tests save those involving the propagation of light in moving media, and Ritz did not consider the difficulties presented by tests such as the Fizeau experiment to be insurmountable. Tolman, however, noted that a Michelson-Morley experiment using an extraterrestrial light source could provide a decisive test of the Ritz hypothesis. In 1924, Rudolf Tomaschek performed a modified Michelson-Morley experiment using starlight, while Dayton Miller used sunlight. Both experiments were inconsistent with the Ritz hypothesis. Babcock and Bergman (1964) placed rotating glass plates between the mirrors of a common path interferometer set up in a static Sagnac configuration. If the glass plates behave as new sources of light so that the total speed of light emerging from their surfaces is "c + v", a shift in the interference pattern would be expected. However, there was no such effect which again confirms special relativity, and which again demonstrates the source independence of light speed. This experiment was executed in vacuum, thus extinction effects should play no role. Albert Abraham Michelson (1913) and Quirino Majorana (1918/9) conducted interferometer experiments with resting sources and moving mirrors (and vice versa), and showed that there is no source dependence of light speed in air. Michelson's arrangement was designed to distinguish between three possible interactions of moving mirrors with light: (1) "the light corpuscles are reflected as projectiles from an elastic wall", (2) "the mirror surface acts as a new source", (3) "the velocity of light is independent of the velocity of the source". His results were consistent with source independence of light speed. Majorana analyzed the light from moving sources and mirrors using an unequal arm Michelson interferometer that was extremely sensitive to wavelength changes. Emission theory asserts that Doppler shifting of light from a moving source represents a frequency shift with no shift in wavelength. Instead, Majorana detected wavelength changes inconsistent with emission theory. Beckmann and Mandics (1965) repeated the Michelson (1913) and Majorana (1918) moving mirror experiments in high vacuum, finding "k" to be less than 0.09. Although the vacuum employed was insufficient to definitively rule out extinction as the reason for their negative results, it was sufficient to make extinction highly unlikely. Light from the moving mirror passed through a Lloyd interferometer, part of the beam traveling a direct path to the photographic film, part reflecting off the Lloyd mirror. The experiment compared the speed of light hypothetically traveling at "c + v" from the moving mirrors, versus reflected light hypothetically traveling at "c" from the Lloyd mirror. Other refutations. Emission theories use the Galilean transformation, according to which time coordinates are invariant when changing frames ("absolute time"). Thus the Ives-Stilwell experiment, which confirms relativistic time dilation, also refutes the emission theory of light. As shown by Howard Percy Robertson, the complete Lorentz transformation can be derived, when the Ives-Stillwell experiment is considered together with the Michelson–Morley experiment and the Kennedy–Thorndike experiment. Furthermore, quantum electrodynamics places the propagation of light in an entirely different, but still relativistic, context, which is completely incompatible with any theory that postulates a speed of light that is affected by the speed of the source [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@58777339 [docID]894774 [hitPos]3 [correct]false [extraScores][F@1f200767 , [answer]The theoretical and experimental justification for the Schrödinger equation motivates the discovery of the Schrödinger equation, the equation that describes the dynamics of nonrelativistic particles. The motivation uses photons, which are relativistic particles with dynamics determined by Maxwell's equations, as an analogue for all types of particles. Classical electromagnetic waves. Nature of light. The quantum particle of light is called a photon. Light has both a wave-like and a particle-like nature. In other words, light can appear to be made of photons (particles) in some experiments and light can act like waves in other experiments. The dynamics of classical electromagnetic waves are completely determined by Maxwell's equations, the classical description of electrodynamics. In the absence of sources, Maxwell's equations can be written as wave equations in the electric and magnetic field vectors. Maxwell's equations thus describe, among other things, the wave-like properties of light. When "classical" (coherent or thermal) light is incident on a photographic plate or CCD, the average number of "hits", "dots", or "clicks" per unit time that result is approximately proportional to the square of the electromagnetic fields of the light. By "formal analogy", the wavefunction of a material particle can be used to find the probability density by taking its absolute-value squared. Unlike electromagnetic fields, quantum-mechanical wavefunctions are complex. (Often in the case of EM fields complex notation is used for convenience, but it is understood that in fact the fields are real. On the contrary, wavefunctions are genuinely complex.) Maxwell's equations were completely known by the latter part of the nineteenth century. The dynamical equations for light were, therefore, well-known long before the discovery of the photon. This is not true for other particles such as the electron. It was surmised from the interaction of light with atoms that electrons also had both a particle-like and a wave-like nature. Newtonian mechanics, a description of the particle-like behavior of macroscopic objects, failed to describe very small objects such as electrons. Abductive reasoning was performed to obtain the dynamics of massive (particles with mass) objects such as electrons. The electromagnetic wave equation, the equation that described the dynamics of light, was used as a prototype for discovering the Schrödinger equation, the equation that describes the wave-like and particle-like dynamics of nonrelativistic massive particles. Plane sinusoidal waves. Electromagnetic wave equation. The electromagnetic wave equation describes the propagation of electromagnetic waves through a medium or in a vacuum. The homogeneous form of the equation, written in terms of either the electric field E or the magnetic field B, takes the form: where "c" is the speed of light in the medium. In a vacuum, c = 2.998 × 108 meters per second, which is the speed of light in free space. The magnetic field is related to the electric field through Faraday's law (cgs units) Plane wave solution of the electromagnetic wave equation. The plane sinusoidal solution for an electromagnetic wave traveling in the z direction is (cgs units and SI units) formula_4 for the electric field and for the magnetic field, where k is the wavenumber, is the angular frequency of the wave, and formula_7 is the speed of light. The hats on the vectors indicate unit vectors in the x, y, and z directions. In complex notation, the quantity formula_8 is the amplitude of the wave. Here is the Jones vector in the x-y plane. The notation for this vector is the bra-ket notation of Dirac, which is normally used in a quantum context. The quantum notation is used here in anticipation of the interpretation of the Jones vector as a quantum state vector. The angles formula_10 are the angle the electric field makes with the x axis and the two initial phases of the wave, respectively. The quantity is the state vector of the wave. It describes the polarization of the wave and the spatial and temporal functionality of the wave. For a coherent state light beam so dim that its average photon number is much less than 1, this is approximately equivalent to the quantum state of a single photon. Energy density of classical electromagnetic waves. Energy in a plane wave. The energy per unit volume in classical electromagnetic fields is (cgs units) For a plane wave, converting to complex notation (and hence dividing by a factor of 2), this becomes where the energy has been averaged over a wavelength of the wave. Fraction of energy in each component. The fraction of energy in the x component of the plane wave (assuming linear polarization) is with a similar expression for the y component. The fraction in both components is Momentum density of classical electromagnetic waves. The momentum density is given by the Poynting vector For a sinusoidal plane wave traveling in the z direction, the momentum is in the z direction and is related to the energy density: The momentum density has been averaged over a wavelength. Angular momentum density of classical electromagnetic waves. The angular momentum density is For a sinusoidal plane wave the angular momentum is in the z direction and is given by (going over to complex notation) where again the density is averaged over a wavelength. Here right and left circularly polarized unit vectors are defined as and Unitary operators and energy conservation. A wave can be transformed by, for example, passing through a birefringent crystal or through slits in a diffraction grating. We can define the transformation of the state from the state at time t to the state at time formula_22 as To conserve energy in the wave we require where formula_25 is the adjoint of U, the complex conjugate transpose of the matrix. This implies that a transformation that conserves energy must obey where I is the identity operator and U is called a unitary operator. The unitary property is necessary to ensure energy conservation in state transformations. Hermitian operators and energy conservation. If formula_27 is an infinitesimal real quantity formula_28, then the unitary transformation is very close to the identity matrix (the final state is very close to the initial state) and can be written and the adjoint by The factor of i is introduced for convenience. With this convention, it will be shown that energy conservation requires H to be a Hermitian operator and that H is related to the energy of a particle. Energy conservation requires Since formula_32 is infinitesimal, which means that formula_33 may be neglected with respect to formula_32, the last term can be omitted. Further, if "H" is equal to its adjoint: it follows that (for infinitesimal translations in time formula_36) so that, indeed, energy is conserved. Operators that are equal to their adjoints are called Hermitian or self-adjoint. The infinitesimal translation of the polarization state is Thus, energy conservation requires that infinitesimal transformations of a polarization state occur through the action of a Hermitian operator. While this derivation is classical, the concept of a Hermitian operator generating energy-conserving infinitesimal transformations forms an important basis for quantum mechanics. The derivation of the Schrödinger equation follows directly from this concept. Quantum analogy of classical electrodynamics. The treatment to this point has been classical. However, the quantum mechanical treatment of particles follows along lines "formally analogous" however, to Maxwell's equations for electrodynamics. The analog of the classical "state vectors" in the classical description is quantum state vectors in the description of photons. Energy, momentum, and angular momentum of photons. Energy. The early interpretation is based on the experiments of Max Planck and the interpretation of those experiments by Albert Einstein, which was that electromagnetic radiation is composed of irreducible packets of energy, known as photons. The energy of each packet is related to the angular frequency of the wave by the relation where formula_41 is an experimentally determined quantity known as the reduced Planck's constant. If there are formula_42 photons in a box of volume formula_43, the energy (neglecting zero point energy) in the electromagnetic field is and the energy density is The energy of a photon can be related to classical fields through the correspondence principle which states that for a large number of photons, the quantum and classical treatments must agree. Thus, for very large formula_42, the quantum energy density must be the same as the classical energy density The average number of photons in the box in a coherent state is then Momentum. The correspondence principle also determines the momentum and angular momentum of the photon. For momentum which implies that the momentum of a photon is Angular momentum and spin. Similarly for the angular momentum which implies that the angular momentum of the photon is the quantum interpretation of this expression is that the photon has a probability of formula_54 of having an angular momentum of formula_55 and a probability of formula_56 of having an angular momentum of formula_57. We can therefore think of the angular momentum of the photon being quantized as well as the energy. This has indeed been experimentally verified. Photons have only been observed to have angular momenta of formula_58. Spin operator. The spin of the photon is defined as the coefficient of formula_55 in the angular momentum calculation. A photon has spin 1 if it is in the formula_60 state and -1 if it is in the formula_61 state. The spin operator is defined as the outer product The eigenvectors of the spin operator are formula_63 and formula_64 with eigenvalues 1 and -1, respectively. The expected value of a spin measurement on a photon is then An operator S has been associated with an observable quantity, the angular momentum. The eigenvalues of the operator are the allowed observable values. This has been demonstrated for angular momentum, but it is in general true for any observable quantity. Probability for a single photon. There are two ways in which probability can be applied to the behavior of photons; probability can be used to calculate the probable number of photons in a particular state, or probability can be used to calculate the likelihood of a single photon to be in a particular state. The former interpretation is applicable to thermal or to coherent light (see Quantum optics). The latter interpretation is the option for a single-photon Fock state. Dirac explains this in the context of the double-slit experiment: Probability amplitudes. The probability for a photon to be in a particular polarization state depends on the probability distribution over the fields as calculated by the classical Maxwell's equations (in the Glauber-Sudarshan P-representation of a one-photon Fock state.) The expectation value of the photon number in a coherent state in a limited region of space is quadratic in the fields. In quantum mechanics, by analogy, the state or probability amplitude of a single particle contains the basic probability information. In general, the rules for combining probability amplitudes look very much like the classical rules for composition of probabilities: (The following quote is from Baym, Chapter 1) de Broglie waves. In 1923 Louis de Broglie addressed the question of whether all particles can have both a wave and a particle nature similar to the photon. Photons differ from many other particles in that they are massless and travel at the speed of light. Specifically de Broglie asked the question of whether a particle that has both a wave and a particle associated with it is consistent with Einstein's two great 1905 contributions, the special theory of relativity and the quantization of energy and momentum. The answer turned out to be positive. The wave and particle nature of electrons was experimentally observed in 1927, two years after the discovery of the Schrödinger equation. de Broglie hypothesis. De Broglie supposed that every particle was associated with both a particle and a wave. The angular frequency formula_66 and wavenumber formula_67 of the wave was related to the energy E and momentum p of the particle by and The question reduces to whether every observer in every inertial reference frame can agree on the phase of the wave. If so, then a wave-like description of particles may be consistent with special relativity. Rest frame. First consider the rest frame of the particle. In that case the frequency and wavenumber of the wave are related to the energy and momentum of the particles properties by and where m is the rest mass of the particle. This describes a wave of infinite wavelength and infinite phase velocity The wave may be written as proportional to This, however, is also the solution for a simple harmonic oscillator, which can be thought of as a clock in the rest frame of the particle. We can imagine a clock ticking at the same frequency as the wave is oscillating. The phases of the wave and the clock can be synchronized. Frame of the observer. It is shown that the phase of the wave in an observer frame is the same as the phase of the wave in a particle frame, and also the same as clocks in the two frames. There is, therefore, consistency of both a wave-like and a particle-like picture in special relativity. Phase of the observer clock. In the frame of an observer moving at relative speed v with respect to the particle, the particle clock is observed to tick at a frequency where is a Lorentz factor that describes time dilation of the particle clock as observed by the observer. The phase of the observer clock is where formula_77 is time measured in the particle frame. Both the observer clock and the particle clock agree on the phase. Phase of the observer wave. The frequency and wavenumber of the wave in the observer frame is given by and with a phase velocity The phase of the wave in the observer frame is The phase of the wave in the observer frame is the same as the phase in the particle frame, as the clock in the particle frame, and the clock in the observer frame. A wave-like picture of particles is consistent with special relativity. Bohr atom. Inconsistency of observation with classical physics. The de Broglie hypothesis helped resolve outstanding issues in atomic physics. Classical physics was unable to explain the observed behaviour of electrons in atoms. Specifically, accelerating electrons emit electromagnetic radiation according to the Larmor formula. Electrons orbiting a nucleus should lose energy to radiation and eventually spiral into the nucleus. This is not observed. Atoms are stable on timescales much longer than predicted by the classical Larmor formula. Also, it was noted that excited atoms emit radiation with discrete frequencies. Einstein used this fact to interpret discrete energy packets of light as, in fact, real particles. If these real particles are emitted from atoms in discrete energy packets, however, must the emitters, the electrons, also change energy in discrete energy packets? There is nothing in Newtonian mechanics that explains this. The de Broglie hypothesis helped explain these phenomena by noting that the only allowed states for an electron orbiting an atom are those that allow for standing waves associated with each electron. Balmer series. The Balmer series identifies those frequencies of light that can be emitted from an excited hydrogen atom: where R is known at the Rydberg constant and is equal to 13.6 electron volts. Assumptions of the Bohr model. The Bohr model, introduced in 1913, was an attempt to provide a theoretical basis for the Balmer series. The assumptions of the model are: Implications of the Bohr model. In a circular orbit the centrifugal force balances the attractive force of the electron where e is the charge on the electron or proton. The energy of the orbiting electron is [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@15ab6789 [docID]6936414 [hitPos]4 [correct]false [extraScores][F@8412ccd , [answer]In Conway's Game of Life (and related cellular automata), the speed of light is a propagation rate across the grid of exactly one step (either horizontally, vertically or diagonally) per generation. In a single generation, a cell can only influence its nearest neighbours, and so the speed of light (by analogy with the speed of light in physics) is the maximum rate at which information can propagate. It is therefore an upper bound to the speed at which any pattern can move. Notation. As in physics, the speed of light is represented with the letter formula_1. This in turn is used as a reference for describing the average propagation speed of spaceships. For example, a glider is said to have a speed of formula_2, as it takes four generations for a given state to be translated by one cell. Similarly, the "lightweight spaceship" is said to have a speed of formula_3, as it takes four generations for a given state to be translated by two cells. Lightspeed propagation. While formula_1 is an absolute upper bound to propagation speed, the maximum speed of a spaceship in Conway's Game of Life is in fact formula_3. This is because it is impossible to build a spaceship that can move every generation. (This is not true for Life, though, for cellular automata in general; for instance many light-speed spaceships exist in Seeds.) It is, however, possible for objects to travel at the speed of light if they move through a medium other than empty space. Such media include trails of hives, and alternating stripes of live and dead cells. Faster than light propagation. Certain patterns can appear to move at a speed greater than one cell per generation, but like faster than light phenomena in physics this is illusory. An example is the "Star Gate", an arrangement of three converging gliders that will mutually annihilate on collision. If a lightweight spaceship (LWSS) hits the colliding gliders, it will appear to move forwards by 11 cells in only 6 generations, and thus travel faster than light. This illusion happens because the glider annihilation reaction proceeds by the creation and soon-after destruction of another LWSS. When the incoming LWSS hits the colliding gliders, it isn't transported, but instead modifies the reaction so that the newly created LWSS can survive. The only signal being transmitted is that determining whether the outgoing LWSS should survive or not. This does not need to reach its destination until after the LWSS has been "transported", and so no information needs to travel faster than light [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@58777339 [docID]20041810 [hitPos]4 [correct]false [extraScores][F@ee99929 , [answer]Just as the railways of Australia has suffered issues from incompatible rail gauges, the different states have tended to go their own way regarding railway signalling practice. New South Wales. Railway signals in New South Wales broadly follow British route signalling practice, with certain American influences and local innovations. The following systems are currently in use, listed in chronological order of introduction: two-position lower quadrant semaphore, three-position upper quadrant semaphore, Double Light Colour Light and Single Light Colour Light. Double Light signals are capable of showing more indications than Single Light signals, therefore Double Light signals are more appropriate for use in dense traffic areas, such as the Sydney metropolitan area, and Single Light signals for the less intense areas. However combinations of the different types may be found at the same interlocking locations. Systems of train signalling. New South Wales signalling systems generally follow British precepts, however American influence has increased somewhat since the 1990s. Originally trains ran on the time-interval system and authority of the timetable. From the early 1870s, several near-misses and minor derailments led to serious discussion of improved safe-working arrangements and the implementation of interlocking. Despite this, very little action was taken until 1877, when a new rule book including the Staff & Ticket and Block Telegraph systems was finally printed. The ink was barely dry on 30 January 1878 when two trains collided head-on at Emu Plains, killing three. This put an end to timetable working on single lines. Absolute Block Telegraph for double lines came in 1879, Saxby & Farmer's mechanical interlocking in 1881, Tyer's Electric Tablet in 1888 (lasting until 1959) and the Electric Staff system in 1891. Tyer's One-Wire Block began to replace the original Preece's Patent instruments from 1891. Sykes' Lock & Block was used on a few suburban sections from 1900. Tyer's Three-Wire Block was used for permissive block working on the Up Relief line from Concord West to North Strathfield Junction from 1911 until 1983. In 1913, the two-wire New South Wales Standard Block, which was similar in principle to Lock & Block, became the standard for new installations. It was unique in having a fourth position, known as Train Arrived, and a different sequence of operation to standard British arrangements. The last of these instruments were removed from Exeter, Bundanoon and Wingello on the Main South line in 2007. By 1907, the majority of signalling equipment, including signals and mechanical lever frames, were being manufactured in-house under the direction of the English-born signal engineer, Mr C.L.N.F. Wilkin. The NSWR Signal Engineer's Branch supplanted McKenzie & Holland as the principal supplier in most installations. Power signalling. Power signalling arrived in 1910 in Sydney Yard, with the commissioning of (Sydney) Station Box. This installation was electro-pneumatic and controlled from a miniature lever frame supplied by the McKenzie, Holland & Westinghouse Power Signal Co. of Worcester, England. Although the lines were not continuously track-circuited and absolute block telegraph remained between signalboxes, there was some control of signals by track circuit and treadle. Upon its replacement by a 432 lever pistol grip power frame in the new Sydney Station West signalbox in the late 1920s, the original miniature lever frame was divided into smaller sections for reuse at other locations. The disused 'Station Box' remained in position for decades after its retirement, but has now been demolished, along with its successor. Track Circuit Block. On 22 June 1913 the first automatic signals in Australia were brought into use between Eveleigh Loco Junction and Sydenham, replacing Tyer's One-Wire Block. These signals were of the two-arm home and distant type. From that time on, a form of Track Circuit Block (TCB) worked from mechanical and power signalboxes, with the gradual spread of automatic signalling between stations, became the standard for new installations, initially with semaphore but later colour light signals. In practice, the term Track Circuit Block was generally shortened to 'Track Block', of which there were a number of (mainly administrative) variations in the rules, such as Double Line Track Block and Single Line Track Block. Track Circuit Block, now known officially as the Rail Vehicle Detection system, remains the standard system of train signalling on all main lines in the RailCorp network. Absolute-Permissive Block. The American Absolute-Permissive Block system, known locally as Single Line Automatic, was installed on the Molong to Dubbo line by Mr C.B. Byles in 1925. Train movements on the line were regulated by a Train Controller at Yeoval, who directed the issue of crossing orders for trains on the line. While a technical success, the traffic density did not warrant the cost of maintaining the signalling system. It was replaced by the first installation of the Divisible Miniature Electric Staff system in 1933. Signalboxes in 2012. Signalboxes remain scattered throughout the RailCorp network, with thirty-six still in regular use in 2012, including nine controlled by local station staff, twenty-six by dedicated Signallers and one shared by both. The majority of these were commissioned before 1970, a few of which have been in continuous use since the late nineteenth century, albeit surviving in a modified state. Eleven are still fitted with Byles-type mechanical lever frames and five have early relay interlocking with Individual Function Switch (IFS) or similar electric switch panels. Newcastle Signalbox, commissioned in 1936 by Mr W.F. Barton, has the last Westinghouse miniature lever power frame in Australia, and one of few remaining in the world. Sixteen signalboxes are fitted with route control relay/solid state interlocking systems and hardwired panels, and three are equipped solely with computerised, VDU-based route control. A few other signalling installations exist outside the RailCorp network, under the control of other rail operators. Junee and Broadmeadow are home to integrated electronic control centres, both of which are run by the Australian Rail Track Corporation, custodian of the busiest lines in the country network of the State. Three-position lower quadrant semaphore. Standard British three position lower quadrant semaphore signals, with an arm for each direction and spectacles mounted below them, were used from the introduction of time-interval working in 1855. The last signal of this type was removed from Girilambone in 1952. The Danger signal was given by a horizontal arm and red light during darkness. The arm was lowered 45 degrees, with a green light for Caution, to "slacken speed", and to an almost vertical position in a slotted post with a white light for All Right. Two-position lower quadrant semaphore. Lower Quadrant Semaphore signals use an arm that works in a horizontal position and may be lowered to (about) a 45-degree angle, they can only give two indications. In the horizontal position a red light is displayed, in the lowered position a green light is displayed. There are two types of arms. A Distant signal uses a fishtail arm, Home and Starting signals use a square-tail arm. Semaphore distant signals in New South Wales are fitted with a fixed green light, positioned above the arm and spectacle, so that they may be easily distinguished from stop signals at night. Although yellow lights were trialled, neither they, nor yellow and black arms were adopted, meaning that distant signal arms are still painted red and white. If the Home or Starting signal is at Danger, the Distant signal will be at Caution, its arm in the horizontal position with a green light over a red light exhibited at night. If the Home and Starting signals are Clear, the Distant will be Clear, its arm lowered, with two green lights exhibited at night. The term "Stop" gradually replaced "Danger" for the purpose of identifying the normal position of stop signals in official publications from circa 1927, with both terms mentioned in the rule book issued in that year. Combined home and distant signals. Where interlockings are closely placed, a Starting signal arm may be fitted above a Distant signal arm. In this case the Starting arm can be placed at Stop and display a red light. As the lower Distant signal arm cannot be cleared while the upper arm is at Stop the signal will show two red lights. The basis for Double Light signalling was thus established by a sequence of Distant and two-arm signals, so that it was then possible to encounter a signal showing two green lights (Clear), followed by a green light over a red light (Caution) then one or two red lights (Stop). Upper quadrant semaphore signals. The power-operated three-position upper quadrant semaphore signal, American in origin, was introduced to New South Wales by the English signal engineer, Mr C.B. Byles, in 1913. Mr Byles (1871–1948) led New South Wales Railways through its introduction of power signalling, from 1911 until 1929. At night, two lights, one above the other, are exhibited. The spectacle attached to the signal arm has three lenses, that is a green for the vertical clear position and red for the other two lenses. In some cases the semaphore arm moves to the horizontal danger position when the line between the distant signal and the stop signal to which it applies is occupied. The Distant signals capable of exhibiting Danger are fitted with an upper red/green spectacle; the two-position versions have a fixed upper green. The Home/Starting signals give three indications. The arm moves from horizontal displaying a red light, upwards to 45 degrees displaying a green or yellow light, and fully vertical also displaying a green light. In double light areas, the lower light of these signals stays red until a full Clear indication is shown. Therefore these signals will show the same lights indicated by the Lower Quadrant Semaphore Distant and two-arm signals, that is, two greens for Clear, a green over a red for Caution and two reds for Stop. Double light colour light. Double Light Colour Light signalling is essentially a two-light multiple-aspect route signalling system, with aspects derived from the night indications of two-arm Home and Distant semaphore signals. This system was introduced in 1924 by Mr C.B. Byles. Most of the Sydney and Newcastle metropolitan areas are still equipped with Double Light Colour Light signals, in accordance with principles established during the tenure of Mr Byles, although some has been replaced with Single Light Colour Light. A basic double light colour light signal consists of two multiple lamp colour light signal heads, one above the other. In case of automatic signals, the upper and lower lights are vertically offset from each other or "staggered". Alternatively, the lower light may be directly below the upper light, like a controlled or semi-automatic signal. An 'A' plate is then fixed to the signal post or the tunnel wall adjacent to the signal to identify it as automatic. Interestingly, the term "semi-automatic" is no longer mentioned in official publications, although the signals still exist. They are classified according to their instantaneous mode of operation rather than capability, i.e. "controlled" or "automatic". Automatic signals in tunnels were originally identified by a white marker light, since the lights could not be staggered, but 'A' plates are now used. The simplest and original form of double light colour light signal provides three indications, that is Clear (green over green), Caution (green over red) and Stop (red over red). A fourth indication, known as Medium, is indicated by green over yellow. This equates to the "double yellow" in British multiple aspect signalling. The Medium indication was introduced in June 1926, three months after the first double yellow indication was used in the UK. Facing junction signals may exhibit an upper yellow light where a diverging route is set. The yellow light was originally said to mean "attention - proceed at medium speed" in accordance with a 1924 study by the Institute of Railway Signal Engineers in which Mr Byles participated. However, "medium speed" was never properly defined and the term has now been removed from the official rule book. The exhibition of an upper yellow light does not impose a specific speed restriction for approaching trains; it is still the responsibility of the driver to be familiar with the route and observe lineside speed boards relating to points and crossings. The turnout indications are as follows: Caution Turnout (yellow over red), meaning proceed on diverging route, prepared to stop at the next signal (originally known as Medium Caution), and Medium Turnout (yellow over yellow), meaning proceed on diverging route, the next signal is exhibiting a proceed indication. Oddly, these indications can also be seen at some trailing junctions, at which only one converging route option is available. Whilst the upper yellow light implies that a reduction in speed may be necessary, this is not consistent with conventional route signalling practice. Turnout and junction signals are the most inconsistent and diverse in the NSW signalling system. Reduced-overlap working and speed control. Projected traffic density warranted the introduction of additional signal indications. One of these was a small green light under the two main red lights of a Stop signal, indicating Low Speed. The Low Speed indication was provided in the Underground City Railway from its opening on 28 February 1932. The standard speed restriction imposed by train stops with the Low Speed indication was originally 17 mph. However, at some locations, the maximum permissible speed is as low as 5 mph, subject to local conditions. In the Underground City and Eastern Suburbs Railway lines, a speed restriction of 30 mph also applies to the Caution indication. The train stops can be seen dropping as the signals step-up to Medium. Closely spaced 'multi home signals', similar to those used on the London Underground, are also a feature of the Underground City and Eastern Suburbs Railway lines. Speed control by intermediate train stops, based on Mr Byles' system, was introduced on the London Underground following the accident at Moorgate on 28 February 1975, in which 43 people were killed. The sequence of aspects on the approach to a preceding train in areas where the Low Speed indication is in use is as follows: Clear, Medium, Caution, Low Speed, Stop. In some cases, the Medium indication will be exhibited at two consecutive signals. At some locations, the Low Speed indication will only be exhibited under reduced overlap conditions (e.g. owing to the presence of a train ahead). Under such a configuration, the sequence of aspects will be: Clear, Medium, Caution, Stop, provided the full overlap is available. An additional subsidiary indication below a stop signal is the Close Up. This appears similar to the Low Speed indication, except that the subsidiary green light is provided in a separate lamp case below a plate labelled "CLOSE UP". Speed control does not apply to the Close Up indication, which is manually selected by the Signaller in most cases and indicates that the section is clear but the station or junction ahead is blocked. Close Up signals are equivalent to the British warning ("W") signal for restricted acceptance. They now being superseded by approach-controlled Caution indications for reduced-overlap working. Preliminary medium indications and turnout repeaters. A fifth main aspect, Preliminary Medium, is now available, with a pulsating yellow light beneath an upper green. It is used before a Medium indication to provide additional notice of the need to reduce speed for a facing junction. The typical sequence of aspects on the approach to a junction where Preliminary Medium is in use is as follows: Clear, Preliminary Medium (with or without directional indicator), Medium, Medium Turnout or Caution Turnout at the junction signal. At some locations, Preliminary Medium may be exhibited at two consecutive signals. Directional indicators have been introduced to resolve the ambiguity that may arise when a medium aspect (green over yellow) can precede either a caution aspect (green over red) or one of the turnout aspects (yellow over red/yellow). The indicator is provided at the signal in rear of a junction signal. When illuminated it displays a white bar, inclined at 45 degrees to the left or right, placed above the main signal heads. The bar is not lit when the next signal applies to the straight route, but is illuminated when the next signal applies to the turnout. At high speed junctions, turnout indications are no longer provided. Instead, the signal in rear of the junction signal is provided with a directional indicator. The junction signal is provided with a route indicator, and exhibits the least-restrictive straight route indication permitted by track circuit occupancy. Double light colour light repeaters. Repeaters in the form of Double Light Colour Light signals are provided at some locations in the metropolitan area. These "repeaters" are unusual in that they do not replicate the indication of the stop signals to which they apply. Instead, they are wired like separate block signals or Distant signals. For instance, the repeater will be at green over red when the signal to which it applies is at Stop. The red over red indication will only be exhibited when the line between the repeater and the stop signal is occupied, and is treated as a Permissive Stop. Single light colour light. With the replacement of older signals in areas with less traffic, Single Light Colour Light signals were introduced in the 1950s by Mr D.J. Vernon, Signal Engineer. This system is derived directly from British multiple-aspect signalling, with American influence in the form of a marker light. Using a single green light for Clear and a single yellow light for Caution, these signals exhibit a single red light with a smaller lower red "marker" light for Stop. Turnout indications can be provided with three yellow lights at an angle of 45 degrees under a red light in the main signal head. The fourth indication, Medium, which equates to the British Preliminary Caution, is a flashing or pulsating yellow light. Originally, a permanently illuminated white marker light was positioned beneath the main colour light head in lieu of the red marker light. The caution turnout indication was a single yellow in the main head over three white lights at an angle of 45 degrees. The red marker light was introduced from 1965. The majority of early single light colour light signals had either been replaced or retrofitted with red marker lights, by 2000. Some Upper Quadrant Semaphore signals were adapted as Single Light Colour Light signals giving the same colour indications while retaining the arm. The sequence of indications of Single Light Colour Light signals is one green light for Clear, one yellow light for Caution, one red light with a lower smaller red light for Stop. The last Upper Quadrant Semaphore Single Light signal was on the Up South Main Line at Moss Vale, which was replaced in 2007 when Moss Vale and Moss Vale Junction were resignalled, resulting in the closure of local signal boxes at those locations. Contradictory meanings. Signalling systems vary between the states of Australia as each railway was established under the different colonial governments with separate legislation. As with the notorious situation of having different gauges, there are differing signal systems. The Victorian railways use Speed Signalling. This has led to similar signal indications giving very different meanings in these two states. For example, in New South Wales Green-over-Red means 'Caution', indicating the next signal is at 'Stop'. In Victoria that same aspect, Green-over-Red, means 'Clear Normal Speed', indicating the next signal is anything but at 'Stop'. There is obvious potential for confusion. On the main south line from Sydney, Single Light Colour Light signals are now exclusively used from Spring Creek bridge (south of Galong) to Albury on the Victorian border. This forms a buffer zone between the areas giving conflicting signal indications. Gradually the remaining Upper Quadrant signals (and Double Light Colour Light signals at Binalong) are being replaced by Single Light Colour Light signals. Victoria. The railways of Victoria use a mix of railway signalling practices: British route signalling with home and distant signals ("2 position signalling") and American speed signalling ("3 position signalling"). Semaphore signals were used on the very first railway lines, but only a bare minimum were provided as the time interval system being relied upon instead. The first interlocking of signals to protect trains was provided in 1874, as before this time conflicting moves could be made. The design of the signals also progressed, with the "disc" type siding signals first introduced in 1885, and the lower quadrant "somersault" type main line signals adopted in 1887, both of which are still in use today. Green was not adopted as the "All Right" colour until 1898, with white being used before this time. Red was the usually colour of all signal arms, until yellow was chosen as the colour for distant signals in 1926, with full adoption made in 1930. Colour light signals first appeared in 1918, and by 1924 they were the standard for new installations. The safeworking of trains between stations on the early lines was time interval working, where a train would be allowed to leave a given time after the train before it. With heavier traffic this method became unsafe, with Staff and Ticket working on single lines adopted from 1873, and telegraph block working from 1878 on double lines. Both of these systems ensured that only one train would be in a section of track at one time. Telegraphic block working was then replaced with "Winters Block" working between 1883 and 1888, a system that is a predecessor of the "Double Line Block" system which is still used today. Later years saw variations made to the "Staff and Ticket" system, with busier lines provided with Electric Staff working which provided greater safely when more trains ran. Heavier suburban traffic on the Melbourne network saw a greater strain on the block working then used, which required a large number of manned signalboxes to enable trains to run close together. As a result it was decided to adopt power signalling under the Automatic Block System (ABS) of safeworking, where the presence of trains automatically control the signals after them, providing a safe distance between trains. Introduced from 1915, the system was based on American speed signalling practice with GRS2A upper quadrant mechanical signals with two arms able to indicate up to 5 different speed aspects to train drivers. These signals were later replaced by colour light signals which are the standard today, but the old mechanical style remained until 2001. A variant of the "Automatic Block System", "Automatic and Track Control" (ATC) has since been introduced, which provides the same benefits as ABS on single lines of track, while still ensuring only one train in a section at a time. Centralised Traffic Control was also introduced in the 1960s on the new standard gauge line to Albury, and then on the main interstate line to Adelaide, allowing trains to be directed from a distance. Today little mechanical signalling remains, with local signal boxes controlling signals abolished from many areas as part of the Regional Fast Rail project. Today the suburban network and busier regional lines use variants of Automatic Block Signalling, while quieter lines use the Train Staff and Ticket or Train Order systems of safeworking. Train protection has also progressed, with the Train Protection & Warning System also introduced on major passenger lines. South Australia. South Australia uses two primary forms of signalling. Nearly all signal boxes in South Australia have now been closed, with most rail traffic is coordinated through centralised traffic control systems, either under Australian Rail Track Corporation control from Mile End or TransAdelaide control from Adelaide. Where these two networks interface, such as at the Goodwood level crossing or at Torrens Junction, control is usually from ARTC after release from TransAdelaide. Despite the almost uniform CTC control some signal boxes still exist, such as Dry Creek South although they are not normally switched in. Speed signalling. Before 1988, signalling in the metropolitan area was three-position speed signalling, similar to the Victorian system. All mainline signals have two signal 'heads' (originally upper quadrant semaphore arms but now colour light, LED or searchlight), both are always lit. The aspects shown depend on the allowable speed for the route set. If the route was for Normal speed, the 'proceed' component of the indication is conveyed by the top head, if the move is for Medium speed (40 km/h, such as when entering a loop or siding), the 'proceed' component of the indication is conveyed by the lower head. The colours displayed by either depended on how many blocks ahead were clear; green if two or more blocks were clear and yellow if only one was clear (i.e. the next signal showed Stop). Hence the aspects (and indications) are: In addition to these aspects there is Yellow-over-Green, which carries the meaning 'Reduce to Medium speed' (40 km/h) and is used when the next signal displays one of the Medium speed indications. If either of the two main lights never shows any colour other than red, it is replaced with a permanently lit red marker lamp. Dwarf signals. There is also a Low speed aspect for calling-on, shunting or entering a low speed area such as a goods siding. This is indicated by a small yellow light beneath the two main lights, i.e. Red-over-Red-over-Yellow. Dwarf signals are also used to show Low speed aspects. These were originally upper quadrant disc signals but are now all of the colour light or searchlight variety. The aspects (and indications) are: Permissive vs Absolute. Signals are divided into Permissive and Absolute signals. Absolute signals cannot be passed at Stop without permission from the signaller or controller, whereas Permissive signals at Stop can be passed after having stopped for a one-minute waiting period. Absolute signals can be identified by the fact that the two lights are vertically aligned, whereas with Permissive signals they are vertically off-set (staggered on different sides of the post). Dwarf signals are always Absolute. Suburban network. In 1988 the TransAdelaide lines were resignalled with the opening of the Adelaide control centre. The system is quite similar to modern UK colour light signalling, being a route signalling system, though it lacks the double yellow aspect and makes use of 'permissive' signals. There are three basic aspects: red for stop, yellow for caution and green for clear. A reduce to medium speed aspect is also used to give early warning of a divergence, and is given by a flashing yellow light. All aspects are indicated with colour light signals. Despite this re-signalling, some parts of the TransAdelaide network still use the original 3-position speed signalling (such as the Dry Creek to Port Adelaide line). There is also a low speed aspect indicated by a lunar-white position-light signal mounted below the main head. It shows two lunar-white lights at 45 degrees (to mimic an upper-quadrant semaphore) to indicate proceed at low speed. These position light signals are also used for dwarf signals and in this case they can also display 'stop' as a red light and lunar-white light in a horizontal row. Junctions on running signals are indicated by a row of five lunar-white lights angled in the direction of divergence. If multiple routes are to be signalled, several rows may be used. Permissive signals are indicated by a circular 'P' plate offset below the signal head and Absolute by a square 'A' plate directly below the signal head. As with three-position signalling, dwarf (low speed) signals are always absolute. An 'A' plate in New South Wales or Victoria means the signal is automatic and therefore a Permissive signal, which is again a source of conflicting meaning between the differing signalling systems. In the Adelaide station yard Theatre-style route indicators are used on both running signals and shunt signals; platform numbers for 'up' trains and route through the yard for 'down' trains. The down indicators can show these indications: Queensland. Queensland's QR Network (and Aurizon, which makes it confusing, see bottom of article * for more information) uses three forms of safeworking systems. 1. Remote Control Signalling (RCS). 2. Direct Traffic Control (DCS). 3. Staff and Ticket. Remote Control Signalling is otherwise known as Centralised Train Control (CTC) and Direct Train Control is more commonly referred to as Train Orders. QR Network follows UK practice, except that flashing yellow allows simultaneous entry into a crossing loop with no overlaps. Also allows trains in the suburban area to follow trains without an overlap. Unlike Western Australia, there are speed signals and distance signals combined. Some are flashing and three types of indicators below the signal use numbers to give the speed the train needs to reduce to. The flashing yellow signal is used for quicker crossings but unfortunately defeats the purpose of preventing a conflicting movement. Where a train may be fouling a set of points (for example proceeding into the loop) only metres from a signal where an opposing train can collide with the train fouling the points if the opposing train has a SPAD (Signal Passed at Danger). There is extra distance of about 20 metres beyond the points to the entrance signal where a train in Western Australia would wait and a crew on a stationary train would have time to disembark in case of a train running past the red signal on the loop or main. The name for these types are Dynamic Speed Indicators. DSIs have numerals on an indicator and are not signals as such, but work in conjunction with the signal. The Queensland system abounds with bells and whistles (hot wheel detectors, axle counters and other fancy things) and has too much complexity derived from the system it was sold. Rumours are that the Goonyella Rail System is the Gold Crown in Queensland Railways where no expense was spared. The unnecessary use of leading position light signals for points is a glaring example. Interlocking should prevent a signal from indicating a proceed aspect if the points are not set of which the leading position lights indicate whether the points are set or not. These are used on running lines and trains will not be able to stop if indicated wrongly. So the purpose defeats itself and leads the author to speculate that this was a bell and whistle which was sold merely for Government bureaucrats who hold no accountability to taxpayers. Queensland makes use of the flashing yellow on signals (Approach and Home) to also indicate the next signal is at red and proceed at a speed not exceeding 40 km/h. The only similarity between Queensland and Western Australia is the junction and route indicators. They are simple and easy to understand. Starting Signals are also a similarity between Queendland and Western Australia but that is where it ends. Staff and ticket was disposed of in Western Australia years ago for Train Orders and one wonders why Queensland has not followed suit. Staff and Ticket need maintenance of that system mechanically and it is amazing that Queensland sees fit to keep these systems running on lines that have light traffic. Unfortunately there are no access privileges to QR Network Standards on the internet (it is copyright and confidential intellectual property) so reference must be given to QR Network's STD/0037/SWK: Observance of Signals Manual. These same references are made in the above reference of Institution of Railway Signal Engineers Australasia. These documents can only be obtained by contacting QR Network, so source material must be given from articles such as the Institution of Signalling Engineers Australasia at the top. A Form SW10 is required to pass any signal at danger in Queensland unlike Western Australia a Proceed Order is issued ONLY to pass a Departure Signal at STOP. In Western Australia all other signals (on double line for example) can be given verbal authority to pass at stop. In Queensland this is not the case. Also Queensland has bidirectional dual trackage where WA has unidirectional track on its dual track freight system (Avon Yard to Kwinana). The Form SW10 in its Standards is an encyclopedia and the amount of detail is quite large and largely unnecessary. On a daily basis it is not used and an example is cane crossings. This is not used by normal trains usually so I would say a large amount of information in there which could be reduced by 75% at least. Let me give an example of how much bloating is contained (unnecessarily) within the QR Network Standards. In Western Australia Proceed Orders to Pass Departure Signals at STOP has 3 pages. In the QR STandard Remote Controlled Signalling Manual (SAF/STD/0040/NET) it has 24 pages devoted to Form SW10. And then QR Standards are not laid out well. It gives facts like Purpose: Responsibility: Description and is relatively disordered and not uniform in the presentation of the information. It needs to be specific and outline the procedure to be followed and not give university definitions which confuse everyone and leave people to weed out the information they don't need. At least the Western Australian Rules are relatively recent and online for source material in reference to this article. There are several revisions I believe but I doubt the substance has changed very much because it is such an excellent system which was refined to be the only most necessary rules for working without having to go looking in five different standards. It is concise and allows the reader to clearly understand their role and can be understood in practically every situation that has and will exist. Queensland has a far more complex system where the budget for over-investing in infrastructure and not reforming their system to a more simplified and robust cost effective operation was not given accountability. If it had then it may have generated reforms which would have prepared it for more slimline competitive operations where private companies would not be forced to conduct their own operations as BHP is doing now. Western Australia. Western Australia's Pilbara mining railroads will not be dealt with here as there is a lack of information due to the private nature of the railroads. The rail network of WAGR (1890-1976), Westrail (1975-2000), Westnet (2000-2011) and Brookfield Rail (2011-current day) who now operate it, consists today of Train Orders and Centralized Train Control. In the southern half of Western Australia, the railways were all 1067mm gauge and there was no standard gauge rail at all, apart from the trans line running from the East to Kalgoorlie. Only in the 1960's did the Commonwealth and Western Australian Governments decide to build a standard gauge railway from Kalgoorlie to Perth. Leonora and Esperance were also "widened" to standard gauge at this time because they were connected through Coolgardie and that railway was decommissioned once the Standard Gauge project was completed. Staff and Ticket were the primary form of signalling safe working system used in Western Australia apart from major yards and main lines such as the Eastern Goldfields Railway. This has been replaced with Train Orders and no staff and ticket exists in Western Australia and hasn't for more than 20 years. In 1968 the standard gauge was completed between Kalgoorlie and Kwinana, with a dual gauge section built in the Avon Valley between Avon Yard and Kwinana. From Avon Yard to Kalgoorlie this is known as the Single Line Automatic Signalling System using absolute block. Absolute block is used all throughout Western Australia with exception to the Pilbara Railways as no information has been attained to cover that area. A three aspect colour light signal is used in the Western Australian standard gauge railway and the beauty of this system is it is almost completely uniform through the network in automatic signalling territory (narrow gauge as well). When a crossing loop is reached a Home Signal will have a signal for each with the crossing segment signal being 45 degrees beneath the main line signal in the direction of the turnout. For example if the turnout was a right hand the crossing signal would be on the right side below the main signal. Either signal is taken as the Home signal even if taking the loop. Each has an individual number and letter. All branch lines in Western Australia use Train Orders, formerly staff and ticket. Train Orders does utilize a signal at the entrance and the exit of the territory. The Western Australian system has to be the best in the world as it is not only very simple to understand but the complexities of leading position lights and other odd attachments to signalling apparatus have been either eradicated or were never part of it. Switchlocks are used to exit mainline CTC territory for sidings, such as CBH yard terminals on the Standard Gauge. The Train Controller must release the switchlock thus interlocking the signals to stop and this is similar to Queensland as well. Unlike Queensland, points on mainlines in Western Australia are not equipped with leading position light signalling. In fact there is no need for this fancy type of signals as it is just an added cost sold by a signalling company to Government bureaucrats who don't know what they are getting anyway. This saves a lot of money on equipment and also on drivers workloads having to pay attention to non-critical information during critical phases of approaching signals. The driver's attention should be on critical factors. There are simple yellow light shunt signals on signal posts which authorize movements to the next obstruction (vehicle) or signal. It is one of the best signalling systems in the world providing a lower cost due to less bells and whistles but a much higher quality of being simple. An accident occurred at Hines Hill in 1996 where a train entered the mainline from the East at high speed on a slow speed shunt signal indication and due to the driver not maintaining control of the train it collided with an Eastbound Westrail freight train after it had passed Departure Signal at Danger. This also shows the folly of Queensland's system allowing two trains to enter loops at the same time. The driver of the National Rail train had not reduced the speed of his train and the train controller unusually called the slow speed shunt signal, not meant for mainline operations. There are no speed signals in Western Australia's freight system operated by Brookfield Rail (unsure about Pilbara) and they are either Approach, Outer Home, Home, Departure and Starting signals. Perth's urban passenger network is operated by the Public Transport Authority and the Rules are identical to that of the freight, with a few minor differences. One of them is the use of Station Limits boards similar to that used in Train Order territory. Tasmania. Follows UK practice [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@244efc35 [docID]3318850 [hitPos]5 [correct]false [extraScores][F@1328e82b , [answer]Variable speed of light (VSL) is a hypothesis that states that the speed of light, usually denoted by "c", may be a function of space and time. Variable speed of light occurs in some situations of classical physics, as equivalent formulation of accepted theories, but also in various alternative theories of gravitation and cosmology, many of them non-mainstream. In classical physics, the refractive index describes how light slows down when traveling through a medium. The speed of light in vacuum instead is considered a constant, and defined by the SI as 299792458 m/s. Alternative theories therefore usually modify the definitions of meter and seconds. VSL should not be confused with faster than light theories. Notable VSL attempts have been done by Einstein in 1911, by Robert Dicke in 1957, and by several researchers starting from the late 1980s. Since some of them contradict established concepts, VSL theories are a matter of debate. Einstein's VSL attempt in 1911. While Einstein's first mentioned a variable speed of light in 1907, he reconsidered the idea more thoroughly in 1911. In analogy to the situation in media, where a shorter wavelength formula_1, by means of formula_2, leads to a lower speed of light, Einstein assumed that clocks in a gravitational field run slower, whereby the corresponding frequencies formula_3 are influenced by the gravitational potential (eq.2, p. 903): Einstein commented: In a subsequent paper in 1912 he concluded that However, Einstein deduced a light deflection at the sun of “almost one arcsecond” which is just one-half of the correct value later derived by his theory of general relativity. While the correct value was later measured by Eddington in 1919, Einstein gave up his VSL theory for other reasons. Notably, in 1911 he had considered variable time only, while in general relativity, albeit in another theoretical context, both space and time measurements are influenced by nearby masses. Dicke's 1957 attempt and Mach's principle. Robert Dicke, in 1957, developed a related VSL theory of gravity. In contrast to Einstein, Dicke assumed not only the frequencies to vary, but also the wavelengths. Since formula_2, this resulted in a relative change of "c" twice as much as considered by Einstein. Dicke assumed a refractive index formula_6 (eqn.5) and proved it to be consistent with the observed value for light deflection. In a comment related to Mach's principle, Dicke suggested that, while the right part of the term in eq. 5 is small, the left part, 1, could have “its origin in the remainder of the matter in the universe”. Given that in a universe with an increasing horizon more and more masses contribute to the above refractive index, Dicke considered a cosmology where "c" decreased in time, providing an alternative explanation to the cosmological redshift (p. 374). Dicke's theory does not contradict the SI definition of c= 299792458 m/s, since the time and length units second and meter can vary accordingly (p. 366). Other VSL attempts related to Einstein and Dicke. Though Dicke's attempt presented an alternative to general relativity, the notion of a spatial variation of the speed of light as such does not contradict general relativity. Rather it is implicitly present in general relativity, occurring in the coordinate space description, as it is mentioned in several textbooks, e.g. Will, eqs. 6.14, 6.15, or Weinberg, eq. 9.2.5 (formula_7 denoting the gravitational potential −"GM"/"r"): Based on this, variable speed of light models have been developed which agree with all known tests of general relativity, but some distinguish for higher-order tests. Other models claim to shed light on the equivalence principle or make a link to Dirac's Large Numbers Hypothesis. Modern VSL theories as an alternative to cosmic inflation. The varying speed of light cosmology has been proposed independently by Jean-Pierre Petit in 1988, John Moffat in 1992, and the two-man team of Andreas Albrecht and João Magueijo in 1998 to explain the horizon problem of cosmology and propose an alternative to cosmic inflation. An alternative VSL model has also been proposed. In Petit's VSL model, the variation of "c" accompanies the joint variations of all physical constants combined to space and time scale factors changes, so that all equations and measurements of these constants remain unchanged through the evolution of the universe. The Einstein field equations remain invariant through convenient joint variations of "c" and "G" in Einstein's constant. According to this model, the cosmological horizon grows like R, the space scale, which ensures the homogeneity of the primeval universe, which fits the observational data. Late-model restricts the variation of constants to the higher energy density of the early universe, at the very beginning of the radiation-dominated era where spacetime is identified to space-entropy with a metric conformally flat. The idea from Moffat and the team Albrecht–Magueijo is that light propagated as much as 60 orders of magnitude faster in the early universe, thus distant regions of the expanding universe have had time to interact at the beginning of the universe. There is no known way to solve the horizon problem with variation of the fine-structure constant, because its variation does not change the causal structure of spacetime. To do so would require modifying gravity by varying Newton's constant or redefining special relativity . Classically, varying speed of light cosmologies propose to circumvent this by varying the dimensionful quantity "c" by breaking the Lorentz invariance of Einstein's theories of general and special relativity in a particular way. More modern formulations preserve local Lorentz invariance. Various other VSL occurrences. Virtual photons. Virtual photons in some calculations in quantum field theory may also travel at a different speed for short distances; however, this doesn't imply that anything can travel faster than light. While it has been claimed (see VSL criticism below) that no meaning can be ascribed to a dimensional quantity such as the speed of light varying in time (as opposed to a dimensionless number such as the fine structure constant), in some controversial theories in cosmology, the speed of light also varies by changing the postulates of special relativity. Varying photon speed. The photon, the particle of light which mediates the electromagnetic force is believed to be massless. The so-called Proca action describes a theory of a massive photon. Classically, it is possible to have a photon which is extremely light but nonetheless has a tiny mass, like the neutrino. These photons would propagate at less than the speed of light defined by special relativity and have three directions of polarization. However, in quantum field theory, the photon mass is not consistent with gauge invariance or renormalizability and so is usually ignored. However, a quantum theory of the massive photon can be considered in the Wilsonian effective field theory approach to quantum field theory, where, depending on whether the photon mass is generated by a Higgs mechanism or is inserted in an ad hoc way in the Proca Lagrangian, the limits implied by various observations/experiments may be different. So therefore, the speed of light is not constant. Varying "c" in quantum theory. In quantum field theory the Heisenberg uncertainty relations indicate that photons can travel at any speed for short periods. In the Feynman diagram interpretation of the theory, these are known as "virtual photons", and are distinguished by propagating off the mass shell. These photons may have any velocity, including velocities greater than the speed of light. To quote Richard Feynman "...there is also an amplitude for light to go faster (or slower) than the conventional speed of light. You found out in the last lecture that light doesn't go only in straight lines; now, you find out that it doesn't go only at the speed of light! It may surprise you that there is an amplitude for a photon to go at speeds faster or slower than the conventional speed, "c"." These virtual photons, however, do not violate causality or special relativity, as they are not directly observable and information cannot be transmitted acausally in the theory. Feynman diagrams and virtual photons are usually interpreted not as a physical picture of what is actually taking place, but rather as a convenient calculation tool (which, in some cases, happen to involve faster-than-light velocity vectors). Relation to other constants and their variation. Gravitational constant "G". In 1937, Paul Dirac and others began investigating the consequences of natural constants changing with time. For example, Dirac proposed a change of only 5 parts in 1011 per year of Newton's constant "G" to explain the relative weakness of the gravitational force compared to other fundamental forces. This has become known as the Dirac large numbers hypothesis. However, Richard Feynman showed in his famous lectures that the gravitational constant most likely could not have changed this much in the past 4 billion years based on geological and solar system observations (although this may depend on assumptions about the constant not changing other constants). (See also strong equivalence principle.) Fine structure constant "α". One group, studying distant quasars, has claimed to detect a variation of the fine structure constant at the level in one part in 105. Other authors dispute these results. Other groups studying quasars claim no detectable variation at much higher sensitivities. For over three decades since the discovery of the Oklo natural nuclear fission reactor in 1972, even more stringent constraints, placed by the study of certain isotopic abundances determined to be the products of a (estimated) 2 billion year-old fission reaction, seemed to indicate no variation was present. However, Lamoreaux and Torgerson of the Los Alamos National Laboratory conducted a new analysis of the data from Oklo in 2004, and concluded that "α" has changed in the past 2 billion years by 4.5 parts in . They claimed that this finding was "probably accurate to within 20%." Accuracy is dependent on estimates of impurities and temperature in the natural reactor. These conclusions have yet to be verified by other researchers. Paul Davies and collaborators have suggested that it is in principle possible to disentangle which of the dimensionful constants (the elementary charge, Planck's constant, and the speed of light) of which the fine-structure constant is composed is responsible for the variation. However, this has been disputed by others and is not generally accepted. Criticisms of the VSL concept. Dimensionless and dimensionful quantities. It has to be clarified what a variation in a dimensionful quantity actually means, since any such quantity can be changed merely by changing one's choice of units. John Barrow wrote: Any equation of physical law can be expressed in such a manner to have all dimensional quantities normalized against like dimensioned quantities (called "nondimensionalization") resulting in only dimensionless quantities remaining. In fact, physicists can "choose" their units so that the physical constants "c", "G", "ħ" = "h"/(2π), 4π"ε"0, and "k"B take the value one, resulting in every physical quantity being normalized against its corresponding Planck unit. For that, it has been claimed that specifying the evolution of a dimensional quantity is meaningless and does not make sense. When Planck units are used and such equations of physical law are expressed in this nondimensionalized form, no dimensional physical constants such as "c", "G", "ħ", "ε"0, nor "k"B remain, only dimensionless quantities. Shorn of their anthropometric unit dependence, there simply is no speed of light, gravitational constant, nor Planck's constant, remaining in mathematical expressions of physical reality to be subject to such hypothetical variation. For example, in the case of a hypothetically varying gravitational constant, "G", the relevant dimensionless quantities that potentially vary ultimately become the ratios of the Planck mass to the masses of the fundamental particles. Some key dimensionless quantities (thought to be constant) that are related to the speed of light (among other dimensional quantities such as "ħ", "e", "ε"0), notably the fine-structure constant or the proton-to-electron mass ratio, does have meaningful variance and their possible variation continues to be studied. Relation to relativity and definition of "c". In relativity, space-time is 4 dimensions of the same physical property of either space or time, depending on which perspective is chosen. The conversion factor of length=i*c*time is described in Appendix 2 of Einstein's "Relativity". A changing "c" in relativity would mean the imaginary dimension of time is changing compared to the other three real-valued spacial dimensions of space-time. Specifically regarding VSL, if the SI meter definition was reverted to its pre-1960 definition as a length on a prototype bar (making it possible for the measure of "c" to change), then a conceivable change in "c" (the reciprocal of the amount of time taken for light to travel this prototype length) could be more fundamentally interpreted as a change in the dimensionless ratio of the meter prototype to the Planck length or as the dimensionless ratio of the SI second to the Planck time or a change in both. If the number of atoms making up the meter prototype remains unchanged (as it should for a stable prototype), then a perceived change in the value of "c" would be the consequence of the more fundamental change in the dimensionless ratio of the Planck length to the sizes of atoms or to the Bohr radius or, alternatively, as the dimensionless ratio of the Planck time to the period of a particular caesium-133 radiation or both. General critique of varying "c" cosmologies. From a very general point of view, G. Ellis expressed concerns that a varying "c" would require a rewrite of much of modern physics to replace the current system which depends on a constant c. Ellis claimed that any varying "c" theory (1) must redefine distance measurements (2) must provide an alternative expression for the metric tensor in general relativity (3) might contradict Lorentz invariance (4) must modify Maxwell's equations (5) must be done consistently with respect to all other physical theories. Whether these concerns apply to the proposals of Einstein (1911) and Dicke (1957) is a matter of debate, though VSL cosmologies remain out of mainstream physics [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@12925ecb [docID]1267762 [hitPos]5 [correct]false [extraScores][F@2cb1f3cb , [answer]The centimetre–gram–second system (abbreviated CGS or cgs) is a variant of the metric system of physical units based on centimeter as the unit of length, gram as a unit of mass, and second as a unit of time. All CGS mechanical units are unambiguously derived from these three base units, but there are several different ways of extending the CGS system to cover electromagnetism. The CGS system has been largely supplanted by the MKS system, based on meter, kilogram, and second. MKS was in turn extended and replaced by the International System of Units (SI). The latter adopts the three base units of MKS, plus the ampere, mole, candela and kelvin. In many fields of science and engineering, SI is the only system of units in use. However, there remain certain subfields where CGS is prevalent. In the United States, the FPS system is still widely used. In measurements of purely mechanical systems (involving units of length, mass, force, energy, pressure, and so on), the differences between CGS and SI are straightforward and rather trivial; the unit-conversion factors are all powers of 10 arising from the relations and . For example, the CGS-derived unit of force is the dyne, equal to , while the SI-derived unit of force is the newton, . Thus it is straightforward to show that . On the other hand, in measurements of electromagnetic phenomena (involving units of charge, electric and magnetic fields, voltage, and so on), converting between CGS and SI is much more subtle and involved. In fact, formulas for physical laws of electromagnetism (such as Maxwell's equations) need to be adjusted depending on which system of units one uses. This is because there is no one-to-one correspondence between electromagnetic units in SI and those in CGS, as is the case for mechanical units. Furthermore, within CGS, there are several plausible choices of electromagnetic units, leading to different unit "sub-systems", including Gaussian, "ESU", "EMU", and Heaviside–Lorentz. Among these choices, Gaussian units are the most common today, and in fact the phrase "CGS units" is often used to refer specifically to CGS-Gaussian units. History. The CGS system goes back to a proposal in 1832 by the German mathematician Carl Friedrich Gauss. In 1874, it was extended by the British physicists James Clerk Maxwell and William Thomson with a set of electromagnetic units. The sizes of many CGS units turned out to be inconvenient for practical purposes. For example, many everyday objects are hundreds or thousands of centimeters long, such as humans, rooms and buildings. Thus the CGS system never gained wide general use outside the field of science. Starting in the 1880s, and more significantly by the mid-20th century, CGS was gradually superseded internationally for scientific purposes by the MKS (meter–kilogram–second) system, which in turn developed into the modern SI standard. Since the international adoption of the MKS standard in the 1940s and the SI standard in the 1960s, the technical use of CGS units has gradually declined worldwide, in the United States more slowly than elsewhere. CGS units are today no longer accepted by the house styles of most scientific journals, textbook publishers, or standards bodies, although they are commonly used in astronomical journals such as the "Astrophysical Journal". CGS units are still occasionally encountered in technical literature, especially in the United States in the fields of material science, electrodynamics and astronomy. The continued usage of CGS units is most prevalent in magnetism and related fields, as the primary MKS unit, the tesla, is inconvenienently large, leading to the continued common use of the gauss, the CGS equivalent. The units gram and centimeter remain useful "as prefixed units" within the SI system, especially for instructional physics and chemistry experiments, where they match the small scale of table-top setups. However, where derived units are needed, the SI ones are generally used and taught instead of the CGS ones today. For example, a physics lab course might ask students to record lengths in centimeters, and masses in grams, but force (a derived unit) in newtons, a usage consistent with the SI system. Definition of CGS units in mechanics. In mechanics, the CGS and SI systems of units are built in an identical way. The two systems differ only in the scale of two out of the three base units (centimeter versus meter and gram versus kilogram, respectively), while the third unit (second as the unit of time) is the same in both systems. There is a one-to-one correspondence between the base units of mechanics in CGS and SI, and the laws of mechanics are not affected by the choice of units. The definitions of all derived units in terms of the three base units are therefore the same in both systems, and there is an unambiguous one-to-one correspondence of derived units: Thus, for example, the CGS unit of pressure, barye, is related to the CGS base units of length, mass, and time in the same way as the SI unit of pressure, pascal, is related to the SI base units of length, mass, and time: Expressing a CGS derived unit in terms of the SI base units, or vice versa, requires combining the scale factors that relate the two systems: Derivation of CGS units in electromagnetism. CGS approach to electromagnetic units. The conversion factors relating electromagnetic units in the CGS and SI systems are much more complex – so much so that formulae expressing physical laws of electromagnetism are different depending on what system of units one uses. This illustrates the fundamental difference in the ways the two systems are built: Alternate derivations of CGS units in electromagnetism. Electromagnetic relationships to length, time and mass may be derived by several equally appealing methods. Two of them rely on the forces observed on charges. Two fundamental laws relate (independently of each other) the electric charge or its rate of change (electric current) to a mechanical quantity such as force. They can be written in system-independent form as follows: Maxwell's theory of electromagnetism relates these two laws to each other. It states that the ratio of proportionality constants formula_10 and formula_14 must obey formula_17, where "c" is the speed of light in vacuum. Therefore, if one derives the unit of charge from the Coulomb's law by setting formula_18, it is obvious that the Ampère's force law will contain a prefactor formula_19. Alternatively, deriving the unit of current, and therefore the unit of charge, from the Ampère's force law by setting formula_20 or formula_21, will lead to a constant prefactor in the Coulomb's law. Indeed, both of these mutually exclusive approaches have been practiced by the users of CGS system, leading to the two independent and mutually exclusive branches of CGS, described in the subsections below. However, the freedom of choice in deriving electromagnetic units from the units of length, mass, and time is not limited to the definition of charge. While the electric field can be related to the work performed by it on a moving electric charge, the magnetic force is always perpendicular to the velocity of the moving charge, and thus the work performed by the magnetic field on any charge is always zero. This leads to a choice between two laws of magnetism, each relating magnetic field to mechanical quantities and electric charge: These two laws can be used to derive Ampère's force law above, resulting in the relationship: formula_25. Therefore, if the unit of charge is based on the Ampère's force law such that formula_26, it is natural to derive the unit of magnetic field by setting formula_27. However, if it is not the case, a choice has to be made as to which of the two laws above is a more convenient basis for deriving the unit of magnetic field. Furthermore, if we wish to describe the electric displacement field D and the magnetic field H in a medium other than vacuum, we need to also define the constants ε0 and μ0, which are the vacuum permittivity and permeability, respectively. Then we have (generally) formula_28 and formula_29, where P and M are polarization density and magnetization vectors. The factors λ and λ′ are rationalization constants, which are usually chosen to be formula_30, a dimensionless quantity. If λ = λ′ = 1, the system is said to be "rationalized": the laws for systems of spherical geometry contain factors of 4π (for example, point charges), those of cylindrical geometry – factors of 2π (for example, wires), and those of planar geometry contain no factors of π (for example, parallel-plate capacitors). However, the original CGS system used λ = λ′ = 4π, or, equivalently, formula_31. Therefore, Gaussian, ESU, and EMU subsystems of CGS (described below) are not rationalized. Various extensions of the CGS system to electromagnetism. The table below shows the values of the above constants used in some common CGS subsystems: The constant "b" in SI system is a unit-based scaling factor defined as: formula_32. Also, note the following correspondence of the above constants to those in Jackson and Leung: In system-independent form, Maxwell's equations can be written as: formula_37 Note that of all these variants, only in Gaussian and Heaviside–Lorentz systems formula_38 equals formula_39 rather than 1. As a result, vectors formula_40 and formula_41 of an electromagnetic wave propagating in vacuum have the same units and are equal in magnitude in these two variants of CGS. Electrostatic units (ESU). In one variant of the CGS system, Electrostatic units (ESU), charge is defined via the force it exerts on other charges, and current is then defined as charge per time. It is done by setting the Coulomb force constant formula_42, so that Coulomb's law does not contain an explicit prefactor. The ESU unit of charge, franklin (Fr), also known as statcoulomb or esu charge, is therefore defined as follows: Therefore, in electrostatic CGS units, a franklin is equal to a centimeter times square root of dyne: The unit of current is defined as: Dimensionally in the ESU CGS system, charge "q" is therefore equivalent to m1/2L3/2t−1. Hence, neither charge nor current is an independent physical quantity in ESU CGS. This reduction of units is an application of the Buckingham π theorem. ESU notation. All electromagnetic units in ESU CGS system that do not have proper names are denoted by a corresponding SI name with an attached prefix "stat" or with a separate abbreviation "esu". Electromagnetic units (EMU). In another variant of the CGS system, Electromagnetic units (EMU), current is defined via the force existing between two thin, parallel, infinitely long wires carrying it, and charge is then defined as current multiplied by time. (This approach was eventually used to define the SI unit of ampere as well). In the EMU CGS subsystem, this is done by setting the Ampere force constant formula_45, so that Ampère's force law simply contains 2 as an explicit prefactor (this prefactor 2 is itself a result of integrating a more general formulation of Ampère's law over the length of the infinite wire). The EMU unit of current, biot (Bi), also known as abampere or emu current, is therefore defined as follows: The unit of charge in CGS EMU is: Dimensionally in the EMU CGS system, charge "q" is therefore equivalent to m1/2L1/2. Hence, neither charge nor current is an independent physical quantity in EMU CGS. EMU notation. All electromagnetic units in EMU CGS system that do not have proper names are denoted by a corresponding SI name with an attached prefix "ab" or with a separate abbreviation "emu". Relations between ESU and EMU units. The ESU and EMU subsystems of CGS are connected by the fundamental relationship formula_17 (see above), where "c" = 29,979,245,800 ≈ 3·1010 is the speed of light in vacuum in centimeters per second. Therefore, the ratio of the corresponding "primary" electrical and magnetic units (e.g. current, charge, voltage, etc. – quantities proportional to those that enter directly into Coulomb's law or Ampère's force law) is equal either to "c"−1 or "c": and Units derived from these may have ratios equal to higher powers of "c", for example: Other variants. There were at various points in time about half a dozen systems of electromagnetic units in use, most based on the CGS system. These also include the Gaussian units and the Heaviside–Lorentz units. Further complicating matters is the fact that some physicists and electrical engineers in North America use hybrid units, such as volts per "centimeter" for electric fields and amperes per "centimeter" for magnetic fields. However, these are essentially the same as the SI units, by the simple conversion of all lengths used from meters into centimeters. Electromagnetic units in various CGS systems. In this table, "c" = 29,979,245,800 ≈ 3·1010 is the speed of light in vacuum in the CGS units of centimeters per second. The symbol "↔" is used instead of "=" as a reminder that the SI and CGS units are "corresponding" but not "equal" because they have incompatible dimensions. For example, according to the next-to-last row of the table, if a capacitor has a capacitance of 1 F in SI, then it has a capacitance of (10−9 "c"2) cm in ESU; "but" it is usually incorrect to replace "1 F" with "(10−9 "c"2) cm" within an equation or formula. (This warning is a special aspect of electromagnetism units in CGS. By contrast, for example, it is "always" correct to replace "1 m" with "100 cm" within an equation or formula.) One can think of the SI value of the Coulomb constant "k"C as: This explains why SI to ESU conversions involving factors of "c"2 lead to significant simplifications of the ESU units, such as 1 statF = 1 cm and 1 statΩ = 1 s/cm: this is the consequence of the fact that in ESU system "k"C = 1. For example, a centimeter of capacitance is the capacitance between a sphere of radius 1 cm in vacuum and infinity. The capacitance "C" between two concentric spheres of radii "R" and "r" in ESU CGS system is: By taking the limit as "R" goes to infinity we see "C" equals "r". Pro and contra. While the absence of explicit prefactors in some CGS subsystems simplifies some theoretical calculations, it has the disadvantage that sometimes the units in CGS are hard to define through experiment. Also, lack of unique unit names leads to a great confusion: thus "15 emu" may mean either 15 abvolts, or 15 emu units of electric dipole moment, or 15 emu units of magnetic susceptibility, sometimes (but not always) per gram, or per mole. On the other hand, SI starts with a unit of current, the ampere, that is easier to determine through experiment, but which requires extra multiplicative factors in the electromagnetic equations. With its system of uniquely named units, the SI also removes any confusion in usage: 1.0 ampere is a fixed value of a specified quantity, and so are 1.0 henry, 1.0 ohm, and 1.0 volt . A key virtue of the Gaussian CGS system is that electric and magnetic fields have the same units, formula_54 is replaced by formula_55, and the only dimensional constant appearing in the Maxwell equations is formula_56, the speed of light. The Heaviside–Lorentz system has these desirable properties as well (with formula_57 equaling 1), but it is a "rationalized" system (as is SI) in which the charges and fields are defined in such a way that there are many fewer factors of formula_58 appearing in the formulas, and it is in Heaviside–Lorentz units that the Maxwell equations take their simplest form. In SI, and other rationalized systems (for example, Heaviside–Lorentz), the unit of current was chosen such that electromagnetic equations concerning charged spheres contain 4π, those concerning coils of current and straight wires contain 2π and those dealing with charged surfaces lack π entirely, which was the most convenient choice for applications in electrical engineering. However, modern hand calculators and personal computers have reduced this "advantage" to nothing. In some fields where formulas concerning spheres are common (for example, in astrophysics), it has been argued that the nonrationalized CGS system can be somewhat more convenient notationally. In fact, in certain fields, specialized unit systems are used to simplify formulas even further than "either" SI "or" CGS, by using some system of natural units. For example, those in particle physics use a system where every quantity is expressed by only one unit, the electron-volt, with lengths, times, and so on all converted into electron-volts by inserting factors of c and the Planck constant formula_59. This unit system is very convenient for calculations in particle physics, but it would be impractical in all other contexts [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@15ab6789 [docID]7346 [hitPos]5 [correct]false [extraScores][F@22320955 , [answer]As formulated by Albert Einstein in 1905, the theory of special relativity was based on two main postulates: There have been various alternative formulations of special relativity over the years. Some of these formulations are equivalent to the original formulation whereas others result in modifications. "Single postulate" approaches. "Equivalent to the original ? Yes." According to some references, the theory of special relativity can be derived from a single postulate: the principle of relativity. This claim can be misleading because actually these formulations rely on various unsaid assumptions such as isotropy and homogeneity of space. The question here is not about the exact number of postulates. The phrase "single postulate" is just used in comparison with the original "two postulate" formulation. The real question here is whether universal lightspeed can be deduced rather than assumed. The Lorentz transformations, up to a nonnegative free parameter, can be derived without first postulating the universal lightspeed. Experiment rules out the validity of the Galilean transformations and this means the parameter in the Lorentz transformations is nonzero hence there is a finite maximum speed before anything has been said about light. Combining this with Maxwell's equations shows that light travels at this maximum speed. The numerical value of the parameter in these transformations is determined by experiment, just as the numerical values of the parameter pair c and the permittivity of free space are left to be determined by experiment even when using Einstein's original postulates. When the numerical values in both Einstein's and these other approaches have been found then these different approaches result in the same theory. So the end result of the interlocking trio of theory+Maxwell+experiment is the same either way. This is the sense in which universal lightspeed can be deduced rather than postulated. For some historical information, see: History of special relativity#Spacetime physics and the section "Lorentz transformation without second postulate" for the approaches of Ignatowski and Frank/Rothe. However, according to Pauli (1921), Resnick (1967), and Miller (1981), those models were insufficient. But the constancy of the speed of light is contained in Maxwell's equations. That section includes the phrase "Ignatowski was forced to recourse to electrodynamics to include the speed of light.". So, the trio of "principle of relativity+Maxwell+numerical values from experiment" gives special relativity and this should be compared with "principle of relativity+second postulate+Maxwell+numerical values from experiment". Since Einstein's 1905 paper is all about electrodynamics he is assuming Maxwell's equations, and the theory isn't practically applicable without numerical values. When compared like with like, from the point of view of asking what is knowable, the second postulate can be deduced. If you restrict your attention to just the standalone theory of relativity then yes you need the postulate. But given all the available knowledge we don't need to postulate it. In other words different domains of knowledge are overlapping and thus taken together have more information than necessary. This can be summarized as follows: There are references which discuss in more detail the principle of relativity Lorentz ether theory. "Equivalent to the original ? Yes." Hendrik Lorentz and Henri Poincaré developed their version of special relativity in a series of papers from about 1900 to 1905. They used Maxwell's equations and the principle of relativity to deduce a theory that is mathematically equivalent to the theory later developed by Einstein. Minkowski spacetime. "Equivalent to the original ? Yes." Minkowski space (or Minkowski spacetime) is a mathematical setting in which special relativity is conveniently formulated. Minkowski space is named for the German mathematician Hermann Minkowski, who around 1907 realized that the theory of special relativity (previously developed by Poincaré and Einstein) could be elegantly described using a four-dimensional spacetime, which combines the dimension of time with the three dimensions of space. Mathematically there are a number of ways in which the four-dimensions of Minkowski spacetime are commonly represented: as a four-vector with 4 real coordinates, as a four-vector with 3 real and one complex coordinate, or using tensors. Test theories of special relativity. "Equivalent to the original ? No." Test theories of special relativity are flat space-time theories which differ from special relativity by having a different postulate about light concerning one-way speed of light vs two-way speed of light. Different postulates on light result in different notions of time simultaneity. There is Robertson's test theory (1949) which predicts different experimental results from Einstein's special relativity, and then there is Edward's theory (1963) which cannot be called a test theory because it is physically equivalent to special relativity, and then there is the Mansouri-Sexl theory (1977) which is equivalent to Robertson's theory. Curvilinear coordinates and non-inertial frames. "Equivalent to the original ? Curvilinear is a generalization, but the original SR can be applied locally." There can be misunderstandings over the sense in which SR can be applied to accelerating frames. Special relativity cannot be used to describe a global frame for non-inertial i.e. accelerating frames. However general relativity implies that special relativity "can" be applied locally where the observer is confined to making local measurements. For example an analysis of Bremsstrahlung does not require general relativity, SR is sufficient. For examples see Can Special Relativity handle accelerations?, Differential aging from acceleration, an explicit formula and SR treatment of arbitrarily accelerated motion. The key point is that you can use special relativity to describe all kinds of accelerated phenomena, and also to predict the measurements made by an accelerated observer who's confined to making measurements at one specific location only. If you try to build a complete frame for such an observer, one that is meant to cover all of spacetime, you'll run into difficulties (there'll be a horizon, for one). The problem is that you cannot derive from the postulates of special relativity that an acceleration will not have a non-trivial effect. E.g. in case of the twin paradox, we know that you can compute the correct answer of the age difference of the twins simply by integrating the formula for time dilation along the trajectory of the travelling twin. This means that one assumes that at any instant, the twin on its trajectory can be replaced by an inertial observer that is moving at the same velocity of the twin. This gives the correct answer, as long as we are computing effects that are local to the travelling twin. The fact that the acceleration that distinguishes the local inertial rest frame of the twin and the true frame of the twin does not have any additional effect follows from general relativity (it has, of course, been verified experimentally). In 1943, Moller obtained a transform between an inertial frame and a frame moving with constant acceleration, based on Einstein's vacuum eq and a certain postulated time-independent metric tensor, although this transform is of limited applicability as it does not reduce to the Lorentz transform when a=0. Throughout the 20th century efforts were made in order to generalize the Lorentz transformations to a set of transformations linking inertial frames to non-inertial frames with uniform acceleration. So far, these efforts failed to produce satisfactory results that are both consistent with 4-dimensional symmetry and to reduce in the limit a=0 to the Lorentz transformations. Hsu and Hsu claim that they have finally come up with suitable transformations for constant linear acceleration (uniform acceleration). They call these transformations: Generalized Moller-Wu-Lee Transformations. They also say: "But such a generalization turns out not to be unique from a theoretical viewpoint and there are infinitely many generalizations. So far, no established theoretical principle leads to a simple and unique generalization." de Sitter relativity. "Equivalent to the original ? No." According to and the references therein, if you take Minkowski's ideas to their logical conclusion then not only are boosts non-commutative but translations are also non-commutative. This means that the symmetry group of space time is a de Sitter group rather than the Poincaré group. This results in spacetime being slightly curved even in the absence of matter or energy. This residual curvature is caused by a cosmological constant to be determined by observation. Due to the small magnitude of the constant, then special relativity with the Poincaré group is more than accurate enough for all practical purposes, although near the big bang and inflation de Sitter relativity may be more useful due to the cosmological constant being larger back then. Note this is not the same thing as solving Einstein's field equations for general relativity to get a de Sitter Universe, rather de Sitter relativity is about getting a de Sitter Group for special relativity which neglects gravity. Taiji relativity. "Equivalent to the original ? Yes." This section is based on the work of Jong-Ping Hsu and Leonardo Hsu. They decided to use the word "Taiji" which is a Chinese word meaning the ultimate principles that existed before the creation of the world. In SI units, time is measured in seconds, but taiji time is measured in units of metres — the same units used to measure space. Their arguments about choosing what units to measure time in, lead them to say that they can develop a theory of relativity which is experimentally indistinguishable from special relativity, but without using the second postulate in their derivation. Their claims have been disputed. There is a discussion of taiji relativity in the book. The transformations that they derive involve the factor formula_2 where β is the velocity measured in metres per metre (a dimensionless quantity). This looks the same as (but should NOT be conceptually confused with) the velocity as a fraction of light v/c that appears in some expressions for the Lorentz transformations. Expressing time in metres has previously been done by other authors: Taylor and Wheeler in "Spacetime Physics" and Moore in "Six Ideas that Shaped Physics". The transformations are derived using just the principle of relativity and have a maximal speed of 1, which is quite unlike "single postulate" derivations of the Lorentz transformations in which you end up with a parameter that may be zero. So this is not the same as other "single postulate" derivations. However the relationship of taiji time "w" to standard time "t" must still be found, otherwise it would not be clear how an observer would measure taiji time. The taiji transformations are then combined with Maxwell's equations to show that the speed of light is independent of the observer and has the value 1 in taiji speed (i.e. it has the maximal speed). This can be thought of as saying: a time of 1 metre is the time it takes for light to travel 1 metre. Since we can measure the speed of light by experiment in m/s to get the value c, we can use this as a conversion factor. i.e. we have now found an operational definition of taiji time: w=ct. So we have: w metres = (c m/s) * t seconds But it is not just due to the choice of units that there is a maximum speed. It is the principle of relativity, that Hsu&Hsu say, when applied to 4d spacetime, implies the invariance of the 4d-spacetime interval formula_3 and this leads to the coordinate transformations involving the factor formula_4 where beta is the magnitude of the velocity between two inertial frames. The difference between this and the spacetime interval formula_5 in Minkowski space is that formula_3 is invariant purely by the principle of relativity whereas formula_5 requires both postulates. The "principle of relativity" in spacetime is taken to mean invariance of laws under 4-dimensional transformations. Hsu&Hsu then explore other relationships between w and t such as w=bt where b is a function. They show that there are versions of relativity which are consistent with experiment but have a definition of time where the "speed" of light is not constant. They develop one such version called "common relativity" which is more convenient for performing calculations for "relativistic many body problems" than using special relativity. Euclidean relativity. "Equivalent to the original ? No, the velocity addition formula is different." Euclidean relativity uses a Euclidean (++++) metric as opposed to the traditional Minkowski (+---) or (-+++) metric which is derived from the Minkowski metric by rewriting formula_8 into the equivalent formula_9. The roles of time t and proper time formula_10 have switched so that proper time formula_10 takes the role of the coordinate for the 4th spatial dimension. A universal velocity formula_12 for all objects in 4D space-time appears from the regular time derivative formula_13. The approach differs from the so-called Wick rotation or "complex" Euclidean relativity. In Wick rotation, time formula_14 is replaced by formula_15, which also leads to a positive definite metric but it maintains proper time formula_10 as the Lorentz invariant value whereas in Euclidean relativity formula_10 becomes a coordinate. Because formula_13 implies that photons travel at the speed of light in the subspace {x, y, z} and baryonic matter that is at rest in {x, y, z} travels normal to photons along formula_19, a paradox arises on how photons can be propagated in a space-time. The possible existence of parallel space-times or parallel worlds shifted and co-moving along formula_10 is the approach of Giorgio Fontana. The Euclidean geometry is consistent with classical, Minkowski based relativity in two reference frames. The hyperbolic Minkowski geometry turns into a rotation in 4D circular geometry where length contraction and time dilation result from the geometric projection of 4D properties to 3D space. In three reference frames an inconsistency appears in the velocity addition formula, also affecting other formulas that depend on the velocity addition formula. The inconsistency does so far not imply known contradictions with experimental data but compared to the classical formula it predicts small deviations (<formula_21 m/s) in the addition result when both input speeds are very high (>10 km/s) and have similar magnitude. Very special relativity. "Equivalent to the original ? No" Ignoring gravity, experimental bounds seem to suggest that special relativity with its Lorentz symmetry and Poincaré symmetry describes spacetime. Surprisingly, Cohen and Glashow have demonstrated that a small subgroup of the Lorentz group is sufficient to explain all the current bounds. The minimal subgroup in question can be described as follows: The stabilizer of a null vector is the special Euclidean group SE(2), which contains T(2) as the subgroup of parabolic transformations. This T(2), when extended to include either parity or time reversal (i.e. subgroups of the orthochronous and time-reversal respectively), is sufficient to give us all the standard predictions. Their new symmetry is called Very Special Relativity (VSR). Doubly special relativity. "Equivalent to the original ? No." Doubly special relativity (DSR) is a modified theory of special relativity in which there is not only an observer-independent maximum velocity (the speed of light), but an observer-independent minimum length (the Planck length). The motivation to these proposals is mainly theoretical, based on the following observation: The Planck length is expected to play a fundamental role in a theory of Quantum Gravity, setting the scale at which Quantum Gravity effects cannot be neglected and "new" phenomena are observed. If Special Relativity is to hold up exactly to this scale, different observers would observe Quantum Gravity effects at different scales, due to the Lorentz-FitzGerald contraction, in contradiction to the principle that all inertial observers should be able to describe phenomena by the same physical laws. A drawback of the usual doubly special relativity models is that they are valid only at the energy scales where ordinary special relativity is supposed to break down, giving rise to a patchwork relativity. On the other hand, de Sitter relativity is found to be invariant under a simultaneous re-scaling of mass, energy and momentum, and is consequently valid at all energy scales [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@58777339 [docID]20069953 [hitPos]5 [correct]false [extraScores][F@5924bb9b , [answer]Test theories of special relativity give a mathematical framework for analyzing results of experiments to verify special relativity. An experiment to test the theory of relativity cannot assume the theory is true, and therefore needs some other framework of assumptions that are wider than those of relativity. For example, a test theory may have a different postulate about light concerning one-way speed of light vs. two-way speed of light, it may have a preferred frame of reference, and may violate Lorentz invariance in many different ways. Test theories predicting different experimental results from Einstein's special relativity, are "Robertson's test theory (1949)", and the "Mansouri-Sexl theory (1977)" which is equivalent to Robertson's theory. Another, more extensive model is the Standard-Model Extension, which also includes the standard model and general relativity. Robertson-Mansouri-Sexl framework. Basic principles. Howard Percy Robertson (1949) extended the Lorentz transformation by adding additional parameters. He assumed a preferred frame of reference, in which the two-way speed of light, "i.e." the average speed from source to observer and back, is isotropic, while it is anisotropic in relatively moving frames due to the parameters employed. In addition, Robertson used the Poincaré-Einstein synchronization in all frames, making the one-way speed of light isotropic in all of them. A very similar model was introduced by Reza Mansouri and Roman Ulrich Sexl (1977). Contrary to Robertson, Mansouri-Sexl not only added additional parameters to the Lorentz transformation, but also discussed different synchronization schemes. The Poincaré-Einstein synchronization is only used in the preferred frame, while in relatively moving frames they used "external synchronization", "i.e.", the clock indications of the preferred frame are employed in those frames. Therefore, not only the two-way speed of light but also the one-way speed is anisotropic in moving frames. Since the two-way speed of light in moving frames is anisotropic in both models, and only this speed is measurable without synchronization scheme in experimental tests, both models are experimentally equivalent and summarized as the "Robertson-Mansouri-Sexl test theory" (RMS). On the other hand, in special relativity the two-way speed of light is isotropic, therefore RMS gives different experimental predictions as special relativity. By evaluating the RMS parameters, this theory serves as a framework for assessing possible violations of Lorentz invariance. Theory. In the following, the notation of Mansouri-Sexl is used. They chose the coefficients "a", "b", "d", "e" of the following transformation between reference frames: where "T", "X", "Y", "Z" are the Cartesian coordinates measured in a postulated preferred frame (in which the speed of light "c" is isotropic), and "t", "x", "y", "z" are the coordinates measured in a frame moving in the +"X" direction (with the same origin and parallel axes) at speed "v" relative to the preferred frame. And therefore formula_5 is the factor by which the interval between ticks of a clock increases when it moves (time dilation) and formula_6 is factor by which the length of a measuring rod is shortened when it moves (length contraction). If formula_7 and formula_8 and formula_9 then the Lorentz transformation follows. The purpose of the test theory is to allow "a"("v") and "b"("v") to be measured by experiment, and to see how close the experimental values come to the values predicted by special relativity. (Notice that Newtonian physics, which has been conclusively excluded by experiment, results from formula_10) The value of "e"("v") depends only on the choice of clock synchronization and cannot be determined by experiment. Mansouri-Sexl discussed the following synchronization schemes: By giving the effects of time dilation and length contraction the exact relativistic value, this test theory is experimentally equivalent to special relativity, independent of the chosen synchronization. So Mansouri and Sexl spoke about the "remarkable result that a theory maintaining absolute simultaneity is equivalent to special relativity." They also noticed the similarity between this test theory and Lorentz ether theory of Hendrik Lorentz, Joseph Larmor and Henri Poincaré. Though Mansouri, Sexl, and the overwhelming majority of physicists prefer special relativity over such an aether theory, because the latter "destroys the internal symmetry of a physical theory". Experiments with RMS. RMS is currently used in the evaluation process of many modern tests of Lorentz invariance. To second order in "v/c", the parameters of the RMS framework have the following form: Deviations from the two-way (round-trip) speed of light are given by: where formula_15 is the speed of light in the preferred frame, and formula_16 is the speed of light measured in the moving frame at an angle formula_17 from the direction in which the frame is moving. To verify that special relativity is correct, the expected values of the parameters are formula_18, and thus formula_19. The fundamental experiments to test those parameters, still repeated with increased accuracy, are: The combination of those three experiments, together with the Poincaré-Einstein convention to synchronize the clocks in all inertial frames, is necessary to obtain the complete Lorentz transformation. Michelson–Morley only tested the combination between β and δ, while Kennedy–Thorndike tested the combination between α and β. To obtain the individual values, it's necessary to measure one of these quantities directly. This was achieved by Ives–Stilwell who measured α. So β can be determined using Kennedy-Thorndike, and subsequently δ using Michelson–Morley. In addition to those second order tests, Mansouri and Sexl described some experiments measuring "first" order effects in "v/c" (such as Rømer's determination of the speed of light) as being "measurements of the one-way speed of light". These are interpreted by them as tests of the equivalence of internal synchronizations, "i.e." between synchronization by slow clock transport and by light. They emphasize that the negative results of those tests are also consistent with aether theories in which moving bodies are subject to time dilation. However, even though many recent authors agree that measurements of the equivalence of those two clock-synchronization schemes are important tests of relativity, they don't speak of "one-way speed of light" in connection with such measurements anymore, because of their consistency with non-standard synchronizations. Those experiments are consistent with all synchronizations using anisotropic one-way speeds on the basis of isotropic "two-way" speed of light and "two-way" time dilation of moving bodies. Standard Model Extension. Another, more extensive, model is the Standard Model Extension (SME) by Alan Kostelecký and others. Contrary to the Roberson-Mansouri-Sexl (RMS) framework, which is kinematic in nature and restricted to special relativity, SME not only accounts for special relativity, but for dynamical effects of the standard model and general relativity as well. It investigates possible spontaneous breaking of both Lorentz invariance and CPT symmetry. RMS is fully included in SME, though the latter has a much larger group of parameters that can indicate any Lorentz or CPT violation. For instance, a couple of SME parameters was tested in a 2007 study sensitive to 10-16. It employed two simultaneous interferometers over a year's observation: Optical in Berlin at 52°31'N 13°20'E and microwave in Perth at 31°53'S 115°53E. A preferred background (leading to Lorentz Violation) could never be at rest relative to both of them. A large number of other tests has been carried out in recent years, such as the Hughes–Drever experiments. A list of derived and already measured SME-values was given by Kostelecký and Russell [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@12925ecb [docID]17503609 [hitPos]6 [correct]false [extraScores][F@1f031520 , [answer]The year 2005 has been named the World Year of Physics in recognition of the 100th anniversary of Albert Einstein's "Miracle Year," in which he published four landmark papers, and the subsequent advances in the field of physics. History. Physics has been the basis for understanding the physical world and nature as a whole. The applications of physics are the basis for much of today's technology. In order to both raise the worldwide awareness of physics and celebrate the major advances made in the field, the International Union of Pure and Applied Physics resolved that 2005 should be commemorated as the World Year of Physics. This has subsequently been endorsed by both the United Nations and the United States Congress [http://www.theithacajournal.com/news/stories/20050514/localnews/2136011.html]. Annus Mirabilis. The year 2005 is significant primarily because of the changes that have occurred in the philosophy of physics over the past 100 years. These changes began in 1905 with the publication of four papers by Einstein that: explained Brownian motion, introduced the special theory of relativity, and described how the photoelectric effect could be explained by the quantization of light, which helped launch quantum mechanics, and developed "E = mc"2. These papers are commonly called his Annus Mirabilis Papers because they later defined 1905 as a "miracle year" for physics. Most physicists agree that the first three of those papers deserved Nobel Prizes, but only the paper on the photoelectric effect would win one. What makes these papers remarkable is that, in each case, Einstein boldly took an idea from theoretical physics to its logical consequences and managed to explain experimental results that had baffled scientists for decades. Photoelectric effect. The first paper proposed the idea of "energy quanta" and showed how it could be used to explain such phenomena as the photoelectric effect. The idea of energy quanta was motivated by Max Planck's earlier derivation of the law of black-body radiation by assuming that luminous energy could only be absorbed or emitted in discrete amounts, called "quanta". Einstein showed that, by assuming that light actually "consisted" of discrete packets, he could explain the mysterious photoelectric effect. The idea of light quanta contradicted the wave theory of light that followed naturally from James Clerk Maxwell's equations for electromagnetic behavior and, more generally, the assumption of infinite divisibility of energy in physical systems. Even after experiments showed that Einstein's equations for the photoelectric effect were accurate, his explanation was not universally accepted. However, by 1921, when he was awarded the Nobel Prize and his work on photoelectricity was mentioned by name in the award citation, most physicists thought that light quanta were possible. A complete picture of the photoelectric effect was only obtained after the maturity of quantum mechanics. Brownian motion. His second article that year delineated a stochastic model of Brownian motion. Brownian motion generates expressions for the root mean square displacement of particles. Using the then-controversial kinetic theory of fluids, it established that the phenomenon, which still lacked a satisfactory explanation decades after it was first observed, provided empirical evidence for the reality of atoms. It also lent credence to statistical mechanics, which was also controversial at the time. Before this paper, atoms were recognized as a useful concept, but physicists and chemists hotly debated whether atoms were real entities. Einstein's statistical discussion of atomic behavior gave experimentalists a way to count atoms by looking through an ordinary microscope. Wilhelm Ostwald, one of the leaders of the anti-atom school, later told Arnold Sommerfeld that he had been converted to a belief in atoms by Einstein's complete explanation of Brownian motion. Special relativity. Einstein's third paper that year was a highly self-contained work, hardly making reference to other works which may have led to its development. This paper introduced a theory of time, distance, mass and energy which was consistent with electromagnetism, but omitted the force of gravity. Special relativity avoids the problem in science that was present after the Michelson-Morley experiment failed to measure a speed difference between perpendicular light beams, by postulating that the speed of light is "not" relative to some medium and is the same for all observers irrespective of their relative velocities. This is unlike all other known waves, which require a medium (such as water or air) to propagate. Einstein's explanation arises from two postulates: The first is Galileo's idea that the physical laws are the same for all observers that move with constant velocity relative to each other. The second was that the speed of light is the same for every observer. Special relativity has several striking consequences, because the concepts of absolute time and space are incompatible with an absolute speed of light. The theory abounds with paradoxes and appeared to make little sense, landing Einstein substantial ridicule, but he eventually managed to work out the apparent contradictions and solve the problems. Consequences. Einstein's special theory of relativity heralded a new kind of physics, one that digressed from the classical mechanics that had been derived from Newton's calculus. Although his 1905 paper on the photoelectric effect helped spur the development of quantum mechanics, Einstein himself considered quantum theory, which introduced the concept of uncertainty into the laws of the physical world, incomplete. His deterministic view is illustrated in the famous quote "I am convinced that He does not play dice." Einstein viewed quantum mechanics as a means simply to the end of a unified field theory, which would unite the disparate theories of quantum field theory, general relativity, and electromagnetism. However, he never denied that quantum mechanics was very successful in explaining and predicting physical phenomena. The quest for a unified field theory is continuing with work into quantum mechanics, string theory, and superconductivity. The year recognizes the fundamental shift in natural philosophy from a theory of the absolute to that of the uncertainty and relativity spurred by Einstein's 1905 work [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@58777339 [docID]580858 [hitPos]6 [correct]false [extraScores][F@3614331c , [answer]A light-year (symbol: ly), sometimes written light year, lightyear, or Light-year is an astronomical unit of length (not time) equal to just under 9.5 trillion kilometres (or about 6 trillion miles). As defined by the International Astronomical Union (IAU), a light-year is the distance that light travels in vacuum in one Julian year. The light-year is most often used when expressing distances to stars and other distances on a galactic scale, especially in non-specialist and popular science publications. The unit usually used in professional astrometry is the parsec (symbol: pc, approximately 3.26 light-years; the distance at which one Astronomical Unit subtends an angle of one second of arc). Numerical value. 1 light-year = metres (exactly) The figures above are based on a Julian year (not Gregorian year) of exactly (based on a day of exactly SI seconds) and a defined speed of light of  m/s, both included in the IAU (1976) System of Astronomical Constants, used since 1984. Other values. Before 1984, the tropical year (not the Julian year) and a measured (not defined) speed of light were included in the IAU (1964) System of Astronomical Constants, used from 1968 to 1983. The product of Simon Newcomb's J1900.0 mean tropical year of ephemeris seconds and a speed of light of  km/s produced a light-year of (rounded to the seven significant digits in the speed of light) found in several modern sources was probably derived from an old source such as C. W. Allen's 1973 "Astrophysical Quantities" reference work, which was updated in 2000. Other high-precision values are not derived from a coherent IAU system. A value of found in some modern sources is the product of a mean Gregorian year of and the defined speed of light ( m/s). Another value, , is the product of the J1900.0 mean tropical year and the defined speed of light. History. The first successful measurement of the distance to a star other than our Sun was made by Friedrich Bessel in 1838. The star was 61 Cygni, and he used a heliometer designed by Joseph von Fraunhofer. The largest unit for expressing distances across space at that time was the astronomical unit (AU), equal to the radius of the Earth's orbit In those terms, trigonometric calculations based on 61 Cygni's parallax of 0.314 arcseconds, showed the distance to the star to be . Bessel realised that a much larger unit of length was needed to make the vast interstellar distances comprehensible. James Bradley stated in 1729 that light travelled times faster than the Earth in its orbit. In 1769, a transit of Venus revealed the distance of the Earth from the Sun, and this, together with Bradley's figure, allowed the speed of light to be calculated as , very close to the modern value. Bessel used this speed to work out how far light would travel in a year, and announced that the distance to 61 Cygni was 10.3 light-years. This was the first appearance of the light-year as a unit of distance, and, although modern astronomers prefer the parsec, it is popularly used to gauge the expanses of interstellar and intergalactic space. Distances in light-years. Distances measured in fractions of a light-year (or in light-months, etc.) usually involve objects within a star system. Distances measured in light-years include distances between nearby stars, such as those in the same spiral arm or globular cluster. One kilolight-year, abbreviated "kly", is one thousand light-years (about 307 parsecs). Kilolight-years are typically used to measure distances between parts of a galaxy. One megalight-year, abbreviated "Mly", is one million light-years (about 307 kiloparsecs). Megalight-years are typically used to measure distances between neighbouring galaxies and galaxy clusters. One gigalight-year, abbreviation "Gly", is one billion light-years (about 307 megaparsecs)—one of the largest distance measures used. Gigalight-years are typically used to measure distances to supergalactic structures, including quasars and the Sloan Great Wall. Related units. Other units of length can similarly be formed by multiplying units of time by the speed of light. For example, the light-second, useful in astronomy, telecommunications and relativistic physics, is exactly metres or of a light-year. Units such as the light-minute, light-hour and light-day are sometimes used in popular science publications. The light-month, roughly one-twelfth of a light-year, is also used occasionally for approximate measures. The Hayden Planetarium specifies the light month more precisely as 30 days of light travel time. Light travels approximately one foot in a nanosecond; the term "light-foot" is sometimes used as an informal measure of time, and is about a nanosecond [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@244efc35 [docID]23473595 [hitPos]7 [correct]false [extraScores][F@52d8a319 , [answer]In the context of classical theories of gravitation, the speed of gravity is the speed at which changes in a gravitational field propagate. This is the speed at which a change in the distribution of energy and momentum of matter results in subsequent alteration, at a distance, of the gravitational field which it produces. In a more physically correct sense, the "speed of gravity" refers to the speed of a gravitational wave. Background. The speed of gravitational waves in the general theory of relativity is equal to the "speed of light" in vacuum, "c". Within the theory of special relativity, the constant "c" is not exclusively about light; instead it is the highest possible speed for any physical interaction in nature. Formally, "c" is a conversion factor for changing the unit of time to the unit of space. This makes it the only speed which does not depend either on the motion of an observer or a source of light and/or gravity. Thus, the speed of "light" is also the speed of gravitational waves and any other massless particle. Such particles include the gluon (carrier of the strong force), the photons that light waves consist of, and the theoretical gravitons which make up the associated field particles of gravity (a theory of the graviton requires a theory of quantum gravity, however). Static Fields. The speed of physical changes in a gravitational or electromagnetic field should not be confused with "changes" in the behavior of static fields that are due to pure observer-effects. These changes in direction of a static field, because of relativistic considerations, are the same for an observer when a distant charge is moving, as when an observer (instead) decides to move with respect to a distant charge. Thus, constant motion of an observer with regard to a static charge and its extended static field (either a gravitational or electric field) does not change the field. For static fields, such as the electrostatic field connected with electric charge, or the gravitational field connected to a massive object, the field extends to infinity, and does not propagate. Motion of an observer does not cause the direction of such a field to change, and by symmetrical considerations, changing the observer frame so that the charge appears to be moving at a constant rate, "also" does not cause the direction of its field to change, but requires that it continue to "point" in the direction of the charge, at all distances from the charge. The consequence of this is that static fields (either electric or gravitational) always point directly to the actual position of the bodies that they are connected to, without any delay that is due to any "signal" traveling (or propagating) from the charge, over a distance to an observer. This remains true if the charged bodies and their observers are made to "move" (or not), by simply changing reference frames. This fact sometimes causes confusion about the "speed" of such static fields, which sometimes appear to change infinitely quickly when the changes in the field are mere artifacts of the motion of the observer, or of observation. In such cases, nothing actually changes infinitely quickly, save the point of view of an observer of the field. For example, when an observer begins to move with respect to a static field that already extends over light years, it appears as though "immediately" the entire field, along with its source, has begun moving at the speed of the observer. This, of course, includes the extended parts of the field. However, this "change" in the apparent behavior of the field source, along with its distant field, does not represent any sort of propagation that is faster than light. Newtonian gravitation. Isaac Newton's formulation of a gravitational force law requires that each particle with mass respond instantaneously to every other particle with mass irrespective of the distance between them. In modern terms, Newtonian gravitation is described by the Poisson equation, according to which, when the mass distribution of a system changes, its gravitational field instantaneously adjusts. Therefore the theory assumes the speed of gravity to be infinite. This assumption was adequate to account for all phenomena with the observational accuracy of that time. It was not until the 19th century that an anomaly in astronomical observations which could not be reconciled with the Newtonian gravitational model of instantaneous action was noted: the French astronomer Urbain Le Verrier determined in 1859 that the elliptical orbit of Mercury precesses at a significantly different rate than is predicted by Newtonian theory. Laplace. The first attempt to combine a finite gravitational speed with Newton's theory was made by Laplace in 1805. Based on Newton's force law he considered a model in which the gravitational field is defined as a radiation field or fluid. Changes in the motion of the attracting body are transmitted by some sort of waves. Therefore, the movements of the celestial bodies should be modified in the order "v/c", where "v" is the relative speed between the bodies and "c" is the speed of gravity. The effect of a finite speed of gravity goes to zero as "c" goes to infinity, but not as 1/"c"2 as it does in modern theories. This led Laplace to conclude that the speed of gravitational interactions is at least 7×106 times the speed of light. This velocity was used by many in the 19th century to criticize any model based on a finite speed of gravity, like electrical or mechanical explanations of gravitation. From a modern point of view, Laplace's analysis is incorrect. Not knowing about Lorentz' invariance of static fields, Laplace assumed that when an object like the Earth is moving around the Sun, the attraction of the Earth would not be toward the instantaneous position of the Sun, but toward where the Sun "had been" if its position was retarded using the relative velocity (this retardation actually "does" happen with the "optical" position of the Sun, and is called annual solar aberration). Putting the Sun immobile at the origin, when the Earth is moving in an orbit of radius "R" with velocity "v" presuming that the gravitational influence moves with velocity "c", moves the Sun's true position ahead of its optical position, by an amount equal to "vR/c", which is the travel time of gravity from the sun to the Earth times the relative velocity of the sun and the Earth. The pull of gravity (if it behaved like a wave, such as light) would then be always displaced in the direction of the Earth's velocity, so that the Earth would always be pulled toward the optical position of the Sun, rather than its actual position. This would cause a pull ahead of the Earth, which would cause the orbit of the Earth to spiral outward. Such an outspiral would be suppressed by an amount "v/c" compared to the force which keeps the Earth in orbit; and since the Earth's orbit is observed to be stable, Laplace's "c" must be very large. As is now known, it may be considered to be infinite in the limit of straight-line motion, since as a static influence, it is instantaneous at distance, when seen by observers at constant transverse velocity. For orbits in which velocity (direction of speed) changes slowly, it is almost infinite. The attraction toward an object moving with a steady velocity is towards its instantaneous position with no delay, for both gravity and electric charge. In a field equation consistent with special relativity (i.e., a Lorentz invariant equation), the attraction between static charges moving with constant relative velocity, is always toward the instantaneous position of the charge (in this case, the "gravitational charge" of the Sun), not the time-retarded position of the Sun. When an object is moving in orbit at a steady speed but changing velocity "v", the effect on the orbit is order "v"2/"c"2, and the effect preserves energy and angular momentum, so that orbits do not decay. Electrodynamical analogies. Early theories. At the end of the 19th century, many tried to combine Newton's force law with the established laws of electrodynamics, like those of Wilhelm Eduard Weber, Carl Friedrich Gauss, Bernhard Riemann and James Clerk Maxwell. Those theories are not invalidated by Laplace's critique, because although they are based on finite propagation speeds, they contain additional terms which maintain the stability of the planetary system. Those models were used to explain the perihelion advance of Mercury, but they could not provide exact values. One exception was Maurice Lévy in 1890, who succeeded in doing so by combining the laws of Weber and Riemann, whereby the speed of gravity is equal to the speed of light. So those hypotheses were rejected. However, a more important variation of those attempts was the theory of Paul Gerber, who derived in 1898 the identical formula, which was also derived later by Einstein for the perihelion advance. Based on that formula, Gerber calculated a propagation speed for gravity of 305 000 km/s, i.e. practically the speed of light. But Gerber's derivation of the formula was faulty, i.e., his conclusions did not follow from his premises, and therefore many (including Einstein) did not consider it to be a meaningful theoretical effort. Additionally, the value it predicted for the deflection of light in the gravitational field of the sun was too high by the factor 3/2. Lorentz. In 1900 Hendrik Lorentz tried to explain gravity on the basis of his ether theory and the Maxwell equations. After proposing (and rejecting) a Le Sage type model, he assumed like Ottaviano Fabrizio Mossotti and Johann Karl Friedrich Zöllner that the attraction of opposite charged particles is stronger than the repulsion of equal charged particles. The resulting net force is exactly what is known as universal gravitation, in which the speed of gravity is that of light. This leads to a conflict with the law of gravitation by Isaac Newton, in which it was shown by Pierre Simon Laplace that a finite speed of gravity leads to some sort of aberration and therefore makes the orbits unstable. However, Lorentz showed that the theory is not concerned by Laplace's critique, because due to the structure of the Maxwell equations only effects in the order "v"2/"c"2 arise. But Lorentz calculated that the value for the perihelion advance of Mercury was much too low. He wrote: In 1908 Henri Poincaré examined the gravitational theory of Lorentz and classified it as compatible with the relativity principle, but (like Lorentz) he criticized the inaccurate indication of the perihelion advance of Mercury. Lorentz covariant models. Henri Poincaré argued in 1904 that a propagation speed of gravity which is greater than "c" would contradict the concept of local time (based on synchronization by light signals) and the principle of relativity. He wrote: However, in 1905 Poincaré calculated that changes in the gravitational field can propagate with the speed of light if it is presupposed that such a theory is based on the Lorentz transformation. He wrote: Similar models were also proposed by Hermann Minkowski (1907) and Arnold Sommerfeld (1910). However, those attempts were quickly superseded by Einstein's theory of general relativity. The Whitehead's theory of gravitation (1922) explains gravitational red shift, light bending, perihelion shift and Shapiro delay. General relativity. Background. In general relativity, the gravitational potential is identified with the metric tensor and the gravitational force field with the Christoffel symbols of the spacetime manifold. Tidal gravitational field is associated with the curvature of spacetime. General relativity predicts that gravitational radiation should exist and propagate as a wave at the speed of light. A slowly evolving and weak gravitational field will produce, according to general relativity, similar effects to those we might expect from Newtonian gravitation. In particular, the gravitoelectric (static and continuous) component of a gravitational field should not be confused with a possible additional gravitomagnetic component (gravitational radiation); see Petrov classification. Since the gravitoelectric field is a static field, like the static electric field, it cannot be used for superluminal transmission of quantized (discrete) information, i.e., it could not constitute a well-ordered series of impulses carrying a well-defined meaning (this is the same for gravity and electromagnetism). If one of two gravitoelectrically interacting particles were to suddenly be displaced (accelerated) from its position, the other particle would not feel the change due to the acceleration, until a delay corresponding with the speed of light. Such accelerations resulting from the change in quadrupole moment of star systems, like the Hulse-Taylor binary have been observed to carry off a great deal of energy (almost 2% of the energy of our own Sun's output) as gravitational waves. However, such waves would theoretically travel at the speed of light. However, in the case of two gravitoelectrically interacting particle ensembles, such as two planets or stars moving at constant velocity with respect to each other, each body feels a force which is directed at the instantaneous position of the other body, without a speed-of-light delay. The principle of Lorentz invariance demands symmetry between what is seen by a body moving in a static field, and what is seen by a moving body that is the source of such a static field. Thus, since a moving body does not see aberration in a static field emanating from a "motionless body" (at rest), Lorentz invariance requires that in the reference frame of the previously moving body, the (now moving) body that is the source of the static field must still show no retardation or aberration of its field lines, at distance. Thus, moving charged bodies (including bodies that are sources of static gravitational fields) exhibit static field lines which do not bend with distance, and do not show any speed of light delay effects, as seen from bodies moving with regard to them. In other words, since the gravitoelectric field is, by definition, static and continuous, it does not propagate. If such a source of a static field is accelerated (for example stopped) with regard to its formerly constant velocity frame, its distant field continues to be updated as though the charged body continued with constant velocity. This effect causes the distant fields of unaccelerated moving charges to appear to be "updated" instantly for their constant velocity motion, as seen from distant positions, in the frame where the source-object is moving at constant velocity. However, as discussed, this is an effect which can be removed at any time, by transitioning to a new reference frame in which the distant charged body is now at rest. Aberration of field direction in general relativity, for a weakly accelerated observer. The finite speed of gravitational interaction in general relativity does not lead to the sorts of problems with the aberration of gravity that Newton was originally concerned with, because there is no such aberration in static field effects. Because the acceleration of the Earth with regard to the Sun is small (meaning, to a good approximation, the two bodies can be regarded as traveling in straight lines past each other with unchanging velocity), the orbital results calculated by general relativity are the same as those of Newtonian gravity with instantaneous action at a distance, because they are modelled by the behavior of a static field with constant-velocity relative motion, and no aberration for the forces involved. Although the calculations are considerably more complicated, one can show that a static field in general relativity does not suffer from aberration problems as seen by an unaccelerated observer (or a weakly accelerated observer, such as the Earth). Analogously, the "static term" in the electromagnetic Liénard–Wiechert potential theory of the fields from a moving charge, does not suffer from either aberration or positional-retardation. Only the term corresponding to "acceleration" and "electromagnetic emission" in the Liénard–Wiechert potential shows a direction toward the time-retarded position of the emitter. It is in fact not very easy to construct a self-consistent gravity theory in which gravitational interaction propagates at a speed other than the speed of light, which complicates discussion of this possibility. Possible experimental measurements. The speed of gravity (more correctly, the speed of gravitational waves) can be calculated from observations of the orbital decay rate of binary pulsars PSR 1913+16 (the Hulse-Taylor binary system noted above) and PSR B1534+12. The orbits of these binary pulsars are decaying due to loss of energy in the form of gravitational radiation. The rate of this energy loss ("gravitational damping") can be measured, and since it depends on the speed of gravity, comparing the measured values to theory shows that the speed of gravity is equal to the speed of light to within 1%. However, according to PPN formalism setting, measuring the speed of gravity by comparing theoretical results with experimental results will depend on the theory; use of a theory other than that of general relativity could in principle show a different speed, although the existence of gravitational damping at all implies that the speed cannot be infinite. In September 2002, Sergei Kopeikin and Edward Fomalont announced that they had made an indirect measurement of the speed of gravity, using their data from VLBI measurement of the retarded position of Jupiter on its orbit during Jupiter's transit across the line-of-sight of the bright radio source quasar QSO J0842+1835. Kopeikin and Fomalont concluded that the speed of gravity is between 0.8 and 1.2 times the speed of light, which would be fully consistent with the theoretical prediction of general relativity that the speed of gravity is "exactly" the same as the speed of light. Several physicists, including Clifford M. Will and Steve Carlip, have criticized these claims on the grounds that they have allegedly misinterpreted the results of their measurements. Notably, prior to the actual transit, Hideki Asada in a paper to the Astrophysical Journal Letters theorized that the proposed experiment was essentially a roundabout confirmation of the speed of light instead of the speed of gravity. However, Kopeikin and Fomalont continue to vigorously argue their case and the means of presenting their result at the press-conference of AAS that was offered after the peer review of the results of the Jovian experiment had been done by the experts of the AAS scientific organizing committee. In later publication by Kopeikin and Fomalont, which uses a bi-metric formalism that splits the space-time null cone in two – one for gravity and another one for light, the authors claimed that Asada's claim was theoretically unsound. The two null cones overlap in general relativity, which makes tracking the speed-of-gravity effects difficult and requires a special mathematical technique of gravitational retarded potentials, which was worked out by Kopeikin and co-authors but was never properly employed by Asada and/or the other critics. Stuart Samuel also suggested that the experiment did not actually measure the speed of gravity because the effects were too small to have been measured. A response by Kopeikin and Fomalont challenges this opinion. It is important to understand that none of the participants in this controversy are claiming that general relativity is "wrong". Rather, the debate concerns whether or not Kopeikin and Fomalont have really provided yet another verification of one of its fundamental predictions. A comprehensive review of the definition of the speed of gravity and its measurement with high-precision astrometric and other techniques appears in the textbook "Relativistic Celestial Mechanics in the Solar System". If Coulomb propagation speed exceeded light, it might have implications for the speed of gravity propagation. In late 2012, experimenters the Istituto Nazionale di Fisica Nucleare,Laboratori Nazionali di Frascati in Frascati performed an experiment which indicated that there was no measurable delay in propagation of the force between a beam of electrons and detectors. This was taken as indicating that the field seemed to travel with the beam of electrons as if it were a rigid structure preceding the beam. Though awaiting corroboration, the results indicate that aberration is not present in the Coulomb force. In November 2013, Y. Zhu announced that he observed the speed of gravitational force, calculating the variations of the orbit of the geosynchronous satellites perturbed by the Sun. It is shown that the gravitational force of the Sun acting on the satellite is from the present position of the Sun. It indicates that the speed of gravitational force is much larger than the speed of light in a vacuum. From this observation and the recent experiments, the structure of the fields of a moving source (a body or a charge) is studied. A method to measure the speed of gravitational force in laboratory and a line to indirectly test the wavelengths of gravitational waves are presented [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@61f25b53 [docID]13478488 [hitPos]7 [correct]false [extraScores][F@a91691b , [answer]Modern searches for Lorentz violation are scientific studies that look for deviations from Lorentz invariance or symmetry, a set of fundamental frameworks that underpin modern science and fundamental physics in particular. These studies try to determine whether violations or exceptions might exist for well-known physical laws such as special relativity and CPT symmetry, as predicted by some variations of quantum gravity, string theory, and some alternatives to general relativity. Lorentz violations concern the fundamental predictions of special relativity, such as the principle of relativity, the constancy of the speed of light in all inertial frames of reference, and time dilation, as well as the predictions of the standard model of particle physics. To assess and predict possible violations, test theories of special relativity and effective field theories (EFT) such as the Standard-Model Extension (SME) have been invented. These models introduce Lorentz and CPT violations through spontaneous symmetry breaking caused by hypothetical background fields, resulting in some sort of preferred frame effects. This could lead, for instance, to modifications of the dispersion relation, causing differences between the maximal attainable speed of matter and the speed of light. Both terrestrial and astronomical experiments have been carried out, and new experimental techniques have been introduced. No Lorentz violations could be measured thus far, and exceptions in which positive results were reported have been refuted or lack further confirmations. For discussions of many experiments, see Mattingly (2005). For a detailed list of results of recent experimental searches, see Kostelecký and Russell (2008–2013). For a recent overview and history of Lorentz violating models, see Liberati (2013). See also the main article Tests of special relativity. Assessing Lorentz invariance violations. Early models assessing the possibility of slight deviations from Lorentz invariance have been published between the 1960s and the 1990s. In addition, a series of test theories of special relativity and effective field theories (EFT) for the evaluation and assessment of many experiments have been developed, including: However, the Standard-Model Extension (SME) in which Lorentz violating effects are introduced by spontaneous symmetry breaking, is used for most modern analyses of experimental results. It was introduced by Kostelecký and coworkers in 1997 and the following years, containing all possible Lorentz and CPT violating coefficients not violating gauge symmetry. It includes not only special relativity, but the standard model and general relativity as well. Models whose parameters can be related to SME and thus can be seen as special cases of it, include the older RMS and c2 models, the Coleman-Glashow model confining the SME coefficients to dimension 4 operators and rotation invariance, and the Gambini-Pullin model or the Meyers-Pospelov model corresponding to dimension 5 or higher operators of SME. Speed of light. Terrestrial. Many terrestrial experiments have been conducted, mostly with optical resonators or in particle accelerators, by which deviations from the isotropy of the speed of light are tested. Anisotropy parameters are given, for instance, by the Robertson-Mansouri-Sexl test theory (RMS). This allows to distinguish between the relevant orientation and velocity dependent parameters. In modern variants of the Michelson–Morley experiment, the dependence of light speed on the orientation of the apparatus and the relation of longitudinal and transverse lengths of bodies in motion is analyzed. Also modern variants of the Kennedy–Thorndike experiment, by which the dependence of light speed on the velocity of the apparatus and the relation of time dilation and length contraction is analyzed, have been conducted. The current precision, by which an anisotropy of the speed of light can be excluded, is at the 10−17 level. This is related to the relative velocity between the solar system and the rest frame of the cosmic microwave background radiation of ∼368 km/s (see also Resonator Michelson–Morley experiments). In addition, the Standard-Model Extension (SME) can be used to obtain a larger number of isotropy coefficients in the photon sector. It uses the even- and odd-parity coefficients (3x3 matrices) formula_1, formula_2 and formula_3. They can be interpreted as follows: formula_1 represent anisotropic shifts in the two-way (forward and backwards) speed of light, formula_2 represent anisotropic differences in the one-way speed of counterpropagating beams along an axis, and formula_3 represent isotropic (orientation-independent) shifts in the one-way phase velocity of light. It was shown that such variations in the speed of light can be removed by suitable coordinate transformations and field redefinitions, though the corresponding Lorentz violations cannot be removed, because such redefinitions only transfer those violations from the photon sector to the matter sector of SME. While ordinary symmetric optical resonators are suitable for testing even-parity effects and provide only tiny constraints on odd-parity effects, also asymmetric resonators have been built for the detection of odd-parity effects. For additional coefficients in the photon sector leading to birefringence of light in vacuum, which cannot be redefined as the other photon effects, see #Vacuum birefringence. Another type of test of the formula_2 related one-way light speed isotropy in combination with the electron sector of the SME was conducted by Bocquet "et al." (2010). They searched for fluctuations in the 3-momentum of photons during Earth's rotation, by measuring the Compton scattering of ultrarelativistic electrons on monochromatic laser photons in the frame of the cosmic microwave background radiation, as originally suggested by Vahe Gurzadyan and Amur Margarian (for details on that 'Compton Edge' method and analysis see, discussion e.g.). Solar system. Besides terrestrial tests also astrometric tests using Lunar Laser Ranging (LLR), "i.e." sending laser signals from Earth to Moon and back, have been conducted. They are ordinarily used to test general relativity and are evaluated using the Parameterized post-Newtonian formalism. However, since these measurements are based on the assumption that the speed of light is constant, they can also be used as tests of special relativity by analyzing potential distance and orbit oscillations. For instance, Zoltán Lajos Bay and White (1981) demonstrated the empirical foundations of the Lorentz group and thus special relativity by analyzing the planetary radar and LLR data. In addition to the terrestrial Kennedy–Thorndike experiments mentioned above, Müller & Soffel (1995) and Müller et al. (1999) tested the RMS velocity dependence parameter by searching for anomalous distance oscillations using LLR. Since time dilation is already confirmed to high precision, a positive result would prove that light speed depends on the observer's velocity and length contraction is direction dependent (like in the other Kennedy–Thorndike experiments). However, no anomalous distance oscillations have been observed, with a RMS velocity dependence limit of formula_8, comparable to that of Hils and Hall (1990, see table above on the right). Vacuum dispersion. Another effect often discussed in connection with Quantum gravity (QG) is the possibility of Dispersion of light in vacuum ("i.e." the dependence of light speed on photon energy), due to Lorentz violating Dispersion relations. This effect should be strong at energy levels comparable to, or beyond the Planck energy formula_9 GeV, while being extraordinarily weak at energies accessible in the laboratory or observed in astrophysical objects. In an attempt to observe a weak dependence of speed on energy, light from distant astrophysical sources such as gamma ray bursts and distant galaxies has been examined in many experiments. Especially the Fermi-LAT group was able show that no energy dependence and thus no observable Lorentz violation occurs in the photon sector even beyond the Planck energy, which excludes a large class of Lorentz-violating quantum gravity models. Vacuum birefringence. Lorentz violating dispersion relations due to the presence of an anisotropic space might also lead to vacuum birefringence and parity violations. For instance, the polarization plane of photons might rotate due to velocity differences between left- and right-handed photons. In particular, gamma ray bursts, galactic radiation, and the cosmic microwave background radiation are examined. The SME coefficients formula_10 and formula_11 for Lorentz violation are given, 3 and 5 denote the mass dimensions employed. The latter corresponds to formula_12 in the EFT of Meyers and Pospelov by formula_13, formula_14 being the Planck mass. Maximal attainable speed. Threshold constraints. Lorentz violations could lead to differences between the speed of light and the limiting or maximal attainable speed (MAS) of any particle, whereas in special relativity the speeds should be the same. One possibility is to investigate otherwise forbidden effects at threshold energy in connection with particles having a charge structure (protons, electrons, neutrinos). This is because the dispersion relation is assumed to be modified in Lorentz violating EFT models such as SME. Depending on which of these particles travels faster or slower than the speed of light, effects such as the following can occur: Since astronomic measurements also contain additional assumptions – like the unknown conditions at the emission or along the path traversed by the particles, or the nature of the particles –, terrestrial measurements provide results of greater clarity, even though the bounds are lower (the following bounds describe maximal deviations between the speed of light and the limiting velocity of matter): Clock comparison and spin coupling. By this kind of spectroscopy experiments – sometimes called Hughes–Drever experiments as well – violations of Lorentz invariance in the interactions of protons and neutrons are tested by studying the energy levels of those nucleons in order to find anisotropies in their frequencies ("clocks"). Using spin-polarized torsion balances, also anisotropies with respect to electrons can be examined. Methods used mostly focus on vector spin interactions and tensor interactions, and are often described in CPT odd/even SME terms (in particular parameters of bμ and cμν). Such experiments are currently the most sensitive terrestrial ones, because the precision by which Lorentz violations can be excluded lies at the 10−33 GeV level. These tests can be used to constrain deviations between the maximal attainable speed of matter and the speed of light, in particular with respect to the parameters of cμν that are also used in the evaluations of the threshold effects mentioned above. Time dilation. The classic time dilation experiments such as the Ives–Stilwell experiment, the Moessbauer rotor experiments, and the Time dilation of moving particles, have been enhanced by modernized equipment. For example, the Doppler shift of lithium ions traveling at high speeds is evaluated by using saturated spectroscopy in heavy ion storage rings. For more information, see Modern Ives–Stilwell experiments. The current precision with which time dilation is measured (using the RMS test theory), is at the ~10−8 level. It was shown, that Ives-Stilwell type experiments are also sensitive to the formula_3 isotropic light speed coefficient of the SME, as introduced above. Chou "et al." (2010) even managed to measure a frequency shift of ~10−16 due to time dilation, namely at every day's speeds such as 36 km/h. CPT and antimatter tests. Another fundamental symmetry of nature is CPT symmetry. It was shown that CPT violations lead to Lorentz violations in quantum field theory (even though there are nonlocal exceptions). CPT symmetry requires, for instance, the equality of mass, and equality of decay rates between matter and antimatter. For classic tests of decay rates, see Accelerator tests of time dilation and CPT symmetry. Modern tests by which CPT symmetry has been confirmed are mainly conducted in the neutral meson sector. In large particle accelerators, direct measurements of mass differences between top- and antitop-quarks have been conducted as well. Using SME, also additional consequences of CPT violation in the neutral meson sector can be formulated. Other SME related CPT tests have been performed as well: Other particles and interactions. Third generation particles have been examined for potential Lorentz violations using SME. For instance, Altschul (2007) placed upper limits on Lorentz violation of the tau of 10−8, by searching for anomalous absorption of high energy astrophysical radiation. In the BaBar experiment (2007) it was searched for sidereal variations during Earth's rotation using B mesons (thus bottom quarks) and their antiparticles. No Lorentz and CPT violating signal was found with an upper limit of formula_16. Also top quark pairs have been examined in the D0 experiment (2012). They showed that the cross section production of these pairs doesn't depend on sidereal time during Earth's rotation. Lorentz violation bounds on Bhabha scattering have been given by Charneski "et al". (2012). They showed that differential cross sections for the vector and axial couplings in QED become direction dependent in the presence of Lorentz violation. They found no indication of such an effect, placing upper limits on Lorentz violations of formula_17. Gravitation. The influence of Lorentz violation on gravitational fields and thus general relativity was analyzed as well. The standard framework for such investigations is the Parameterized post-Newtonian formalism (PPN), in which Lorentz violating preferred frame effects are described by the parameters formula_18 (see the PPN article on observational bounds on these parameters). Lorentz violations are also discussed in relation to Alternatives to general relativity such as Loop quantum gravity, Emergent gravity, Einstein aether theory or Hořava–Lifshitz gravity. Also SME is suitable to analyze Lorentz violations in the gravitational sector. Bailey and Kostelecky (2006) constrained Lorentz violations down to formula_19 by analyzing the perihelion shifts of Mercury and Earth, and down to formula_20 in relation to solar spin precession. Battat "et al". (2007) examined Lunar Laser Ranging data and found no oscillatory perturbations in the lunar orbit. Their strongest SME bound excluding Lorentz violation was formula_21. Iorio (2012) obtained bounds at the formula_19 level by examining Keplerian orbital elements of a test particle acted upon by Lorentz-violating gravitomagnetic accelerations. Xie (2012) analyzed the advance of periastron of binary pulsars, setting limits on Lorentz violation at the formula_23 level. Neutrino tests. Neutrino oscillations. Although neutrino oscillations have been experimentally confirmed, the theoretical foundations are still controversial, as it can be seen in the discussion related to sterile neutrinos. This makes predictions of possible Lorentz violations very complicated. It is generally assumed that neutrino oscillations require a certain finite mass. However, oscillations could also occur as a consequence of Lorentz violations, so there are speculations as to how much those violations contribute to the mass of the neutrinos. Additionally, a series of investigations have been published in which a sidereal dependence of the occurrence of neutrino oscillations was tested, which could arise when there were a preferred background field. This, possible CPT violations, and other coefficients of Lorentz violations in the framework of SME, have been tested. Here, some of the achieved GeV bounds for the validity of Lorentz invariance are stated: Neutrino speed. Since the discovery of neutrino oscillations, it is assumed that their speed is slightly below the speed of light. Direct velocity measurements indicated an upper limit for relative speed differences between light and neutrinos of formula_24, see measurements of neutrino speed. Also indirect constraints on neutrino velocity, on the basis of effective field theories such as SME, can be achieved by searching for threshold effects such as Vacuum Cherenkov radiation. For example, neutrinos should exhibit Bremsstrahlung in the form of electron-positron pair production. Another possibility in the same framework is the investigation of the decay of pions into muons and neutrinos. Superluminal neutrinos would considerably delay those decay processes. The absence of those effects indicate tight limits for velocity differences between light and neutrinos. Velocity differences between neutrino flavors can be constrained as well. A comparison between muon- and electron-neutrinos by Coleman & Glashow (1998) gave a negative result, with bounds formula_25. Reports of alleged Lorentz violations. Open reports. In 2001, the LSND experiment observed a 3.8σ excess of antineutrino interactions in neutrino oscillations, which contradicts the standard model. First results of the more recent MiniBooNE experiment appeared to exclude this data above an energy scale of 450 MeV, but they had checked neutrino interactions, not antineutrino ones. In 2008, however, they reported an excess of electron-like neutrino events between 200–475 MeV. And in 2010, when carried out with antineutrinos (as in LSND), the result was in agreement with the LSND result, that is, an excess at the energy scale from 450–1250 MeV was observed. Whether those anomalies can be explained by sterile neutrinos, or whether they indicate Lorentz violations, is still discussed and subject to further theoretical and experimental researches. Solved reports. In 2011 the OPERA Collaboration published (in a non-peer reviewed arXiv preprint) the results of neutrino measurements, according to which neutrinos are slightly traveling faster than light. The neutrinos apparently arrived early by ~60 ns. The standard deviation was 6σ, clearly beyond the 5σ limit necessary for a significant result. However, in 2012 it was found that this result was due to measurement errors. The end result was consistent with the speed of light, see Faster-than-light neutrino anomaly. In 2010, MINOS reported differences between the disappearance (and thus the masses) of neutrinos and antineutrinos at the 2.3 sigma level. This would violate CPT symmetry and Lorentz symmetry. However, in 2011 MINOS updated their antineutrino results, reporting that the difference is not as great as initially expected, after evaluating further data. In 2012, they published a paper in which they reported that the difference is now removed. In 2007, the MAGIC Collaboration published a paper, in which they claimed a possible energy dependence of the speed of photons from the galaxy Markarian 501. They admitted, that also a possible energy-dependent emission effect could have cause this result as well. However, the MAGIC result was superseded by the substantially more precise measurements of the Fermi-LAT group, which couldn't find any effect even beyond the Planck energy. For details, see section Dispersion. In 1997, Nodland & Ralston claimed to have found a rotation of the polarization plane of light coming from distant radio galaxies. This would indicate an anisotropy of space. This attracted some interest in the media. However, some criticisms immediately appeared, which disputed the interpretation of the data, and who alluded to errors in the publication. More recent researches also haven't found any evidence for this effect, see section Birefringence. In popular culture. In the Futurama episode "Law and Oracle" (2011), Erwin Schrödinger is pulled over by cops for violating Lorentz invariance, by going 15 miles per hour over the speed of light [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@12925ecb [docID]33431450 [hitPos]7 [correct]false [extraScores][F@607a9a5a , [answer]In quantum mechanics, the concept of matter waves or de Broglie waves reflects the wave–particle duality of matter. The theory was proposed by Louis de Broglie in 1924 in his PhD thesis. The de Broglie relations show that the wavelength is inversely proportional to the momentum of a particle and is also called de Broglie wavelength. Also the frequency of matter waves, as deduced by de Broglie, is directly proportional to the total energy "E" (sum of its rest energy and the kinetic energy) of a particle. Historical context. At the end of the 19th century, light was thought to consist of waves of electromagnetic fields which propagated according to Maxwell’s equations, while matter was thought to consist of localized particles (See history of wave and particle viewpoints). This division was challenged when, in his 1905 paper on the photoelectric effect, Albert Einstein postulated that light was emitted and absorbed as localized packets, or "quanta" (now called photons). These quanta would have an energy where formula_2 is the frequency of the light and "h" is Planck’s constant. In the modern convention, frequency is symbolized by "f" as is done in the rest of this article. Einstein’s postulate was confirmed experimentally by Robert Millikan and Arthur Compton over the next two decades. Thus it became apparent that light has both wave-like and particle-like properties. De Broglie, in his 1924 PhD thesis, sought to expand this wave-particle duality to all particles: In 1926, Erwin Schrödinger published an equation describing how this matter wave should evolve—the matter wave equivalent of Maxwell’s equations—and used it to derive the energy spectrum of hydrogen. That same year Max Born published his now-standard interpretation that the square of the amplitude of the matter wave gives the probability to find the particle at a given place. This interpretation was in contrast to De Broglie’s own interpretation, in which the wave corresponds to the physical motion of a localized particle. de Broglie relations. Quantum mechanics. The de Broglie equations relate the wavelength "λ" to the momentum "p", and frequency "f" to the total energy "E" of a particle: where "h" is Planck's constant. The equation can be equivalently written as using the definitions In each pair, the second is also referred to as the Planck-Einstein relation, since it was also proposed by Planck and Einstein. Special relativity. Using the relativistic momentum formula from special relativity allows the equations to be written as where "m"0 is the particle's rest mass, "v" is the particle's velocity, γ is the Lorentz factor, and "c" is the speed of light in a vacuum. See below for details of the derivation of the de Broglie relations. Group velocity (equal to the particle's speed) should not be confused with phase velocity (equal to the product of the particle's frequency and its wavelength). In the case of a non-dispersive medium, they happen to be equal, but otherwise they are not. Group velocity. Albert Einstein first explained the wave–particle duality of light in 1905. Louis de Broglie hypothesized that any particle should also exhibit such a duality. The velocity of a particle, he concluded then (but may be questioned today, see above), should always equal the group velocity of the corresponding wave. De Broglie deduced that if the duality equations already known for light were the same for any particle, then his hypothesis would hold. This means that where "E" is the total energy of the particle, "p" is its momentum, "ħ" is the reduced Planck constant. For a free non-relativistic particle it follows that where "formula_10" is the mass of the particle and "v" its velocity. Also in special relativity we find that where "m" is the mass of the particle, "c" is the speed of light in a vacuum, formula_12 is the Lorentz factor, and "v" is the velocity of the particle regardless of wave behavior. Group velocity (equal to an electron's speed) should not be confused with phase velocity (equal to the product of the electron's frequency multiplied by its wavelength). Both in relativistic and non-relativistic quantum physics, we can identify the group velocity of a particle's wave function with the particle velocity. Quantum mechanics has very accurately demonstrated this hypothesis, and the relation has been shown explicitly for particles as large as molecules. Phase velocity. In quantum mechanics, particles also behave as waves with complex phases. By the de Broglie hypothesis, we see that Using relativistic relations for energy and momentum, we have where "E" is the total energy of the particle (i.e. rest energy plus kinetic energy in kinematic sense), "p" the momentum, formula_12 the Lorentz factor, "c" the speed of light, and β the speed as a fraction of "c". The variable "v" can either be taken to be the speed of the particle or the group velocity of the corresponding matter wave. Since the particle speed formula_16 for any particle that has mass (according to special relativity), the phase velocity of matter waves always exceeds "c", i.e. and as we can see, it approaches "c" when the particle speed is in the relativistic range. The superluminal phase velocity does not violate special relativity, as it carries no information. See the article on "signal velocity" for details. Four-vectors. Using the four-momentum P = ("E/c", p) and the four-wavevector K = ("ω/c", k), the De Broglie relations form a single equation: which is frame-independent. Experimental confirmation. Matter waves were first experimentally confirmed to occur in the Davisson-Germer experiment for electrons, and the de Broglie hypothesis has been confirmed for other elementary particles. Furthermore, neutral atoms and even molecules have been shown to be wave-like. Electrons. In 1927 at Bell Labs, Clinton Davisson and Lester Germer fired slow-moving electrons at a crystalline nickel target. The angular dependence of the reflected electron intensity was measured, and was determined to have the same diffraction pattern as those predicted by Bragg for x-rays. Before the acceptance of the de Broglie hypothesis, diffraction was a property that was thought to be only exhibited by waves. Therefore, the presence of any diffraction effects by matter demonstrated the wave-like nature of matter. When the de Broglie wavelength was inserted into the Bragg condition, the observed diffraction pattern was predicted, thereby experimentally confirming the de Broglie hypothesis for electrons. This was a pivotal result in the development of quantum mechanics. Just as the photoelectric effect demonstrated the particle nature of light, the Davisson–Germer experiment showed the wave-nature of matter, and completed the theory of wave-particle duality. For physicists this idea was important because it means that not only can any particle exhibit wave characteristics, but that one can use wave equations to describe phenomena in matter if one uses the de Broglie wavelength. Neutral atoms. Experiments with Fresnel diffraction and specular reflection of neutral atoms confirm the application of the de Broglie hypothesis to atoms, i.e. the existence of atomic waves which undergo diffraction, interference and allow quantum reflection by the tails of the attractive potential. Advances in laser cooling have allowed cooling of neutral atoms down to nanokelvin temperatures. At these temperatures, the thermal de Broglie wavelengths come into the micrometre range. Using Bragg diffraction of atoms and a Ramsey interferometry technique, the de Broglie wavelength of cold sodium atoms was explicitly measured and found to be consistent with the temperature measured by a different method. This effect has been used to demonstrate atomic holography, and it may allow the construction of an atom probe imaging system with nanometer resolution. The description of these phenomena is based on the wave properties of neutral atoms, confirming the de Broglie hypothesis. Molecules. Recent experiments even confirm the relations for molecules and even macromolecules, which are normally considered too large to undergo quantum mechanical effects. In 1999, a research team in Vienna demonstrated diffraction for molecules as large as fullerenes. The researchers calculated a De Broglie wavelength of the most probable C60 velocity as 2.5 pm. More recent experiments prove the quantum nature of molecules with a mass up to 6910 amu. In general, the De Broglie hypothesis is expected to apply to any well isolated object. Spatial Zeno effect. The matter wave leads to the spatial version of the quantum Zeno effect. If an object (particle) is observed with frequency Ω » ω in a half-space (say, "y" < 0), then this observation prevents the particle, which stays in the half-space "y" > 0 from entry into this half-space "y" < 0. Such an "observation" can be realized with a set of rapidly moving absorbing ridges, filling one half-space. In the system of coordinates related to the ridges, this phenomenon appears as a specular reflection of a particle from a ridged mirror, assuming the grazing incidence (small values of the grazing angle). Such a ridged mirror is universal; while we consider the idealised "absorption" of the de Broglie wave at the ridges, the reflectivity is determined by wavenumber "k" and does not depend on other properties of a particle [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@15ab6789 [docID]229104 [hitPos]7 [correct]false [extraScores][F@2dcfc785 , [answer]Slow light is the propagation of an optical pulse or other modulation of an optical carrier at a very low group velocity. Slow light occurs when a propagating pulse is substantially slowed down by the interaction with the medium in which the propagation takes place. In 1999, Danish physicist Lene Vestergaard Hau led a combined team from Harvard University and the Rowland Institute for Science which succeeded in slowing a beam of light to about 17 meters per second, and researchers at UC Berkeley slowed the speed of light traveling through a semiconductor to 9.7 km/s in 2004. Hau later succeeded in stopping light completely, and developed methods by which it can be stopped and later restarted. This was in an effort to develop computers that will use only a fraction of the energy of today's machines. In 2005, IBM created a microchip fashioned of fairly standard materials, potentially paving the way toward commercial adoption. Background. When light propagates through a material, it travels slower than the vacuum speed, c. This is a change in the phase velocity of the light and is manifested in physical effects such as refraction. This reduction in speed is quantified by the ratio between c and the phase velocity. This ratio is called the refractive index of the material. Slow light is a dramatic reduction in the group velocity of light, not the phase velocity. Slow light effects are not due to abnormally large refractive indices, as explained below. The simplest picture of light given by classical physics is of a wave or disturbance in the electromagnetic field. In a vacuum, Maxwell's equations predict that these disturbances will travel at a specific speed, denoted by the symbol c. This well-known physical constant is commonly referred to as the speed of light. The postulate of the constancy of the speed of light in all inertial reference frames lies at the heart of special relativity and has given rise to a popular notion that the "speed of light is always the same". However, in many situations light is more than a disturbance in the electromagnetic field. In addition to propagating through a vacuum, light may also propagate through many types of matter, denoted as the "medium". Light traveling within a medium is no longer a disturbance solely of the electromagnetic field, but rather a disturbance of the field and the positions and velocities of the charged particles (electrons) within the material. The motion of the electrons is determined by the field (due to the Lorentz force) but the field is determined by the positions and velocities of the electrons (due to Gauss' law and Ampere's law). The behavior of a disturbance of this combined electromagnetic-charge density field (i.e. light) is still determined by Maxwell's equations, but the solutions are complicated because of the intimate link between the medium and the field. Understanding the behavior of light in a material is simplified by limiting the types of disturbances studied to sinusoidal functions of time. For these types of disturbances Maxwell's equations transform into algebraic equations and are easily solved. These special disturbances propagate through a material at a speed slower than c called the phase velocity. The ratio between c and the phase velocity is called the refractive index or index of refraction of the material. The index of refraction is not a constant for a given material, but depends on temperature, pressure, and upon the frequency of the (sinusoidal) light wave. This leads to an effect called dispersion. A human perceives the amplitude of the sinusoidal disturbance as the brightness of the light and the frequency as the color. If a light is turned on or off at a specific time or otherwise modulated, then the amplitude of the sinusoidal disturbance is also time-dependent. The time-varying amplitude does not propagate at the phase velocity but rather at the group velocity. The group velocity depends not only on the refractive index of the material, but also the way in which the refractive index changes with frequency (i.e. the derivative of refractive index with respect to frequency). Slow light refers to a large reduction in the group velocity of light. If the dispersion relation of the refractive index is such that the index changes rapidly over a small range of frequencies, then the group velocity might be very low, thousands or millions of times less than "c", even though the index of refraction is still a typical value (between 1.5 and 3.5 for glasses and semiconductors). Different ways to achieve slow light. There are many mechanisms which can generate slow light, all of which create narrow spectral regions with high dispersion, i.e. peaks in the dispersion relation. Schemes are generally grouped into two categories: material dispersion and waveguide dispersion. Material dispersion mechanisms such as Electromagnetically Induced Transparency (EIT), Coherent Population Oscillation (CPO), and various Four Wave Mixing (FWM) schemes produce a rapid change in refractive index as a function of optical frequency, i.e. they modify the temporal component of a propagating wave. This is done by using a nonlinear effect to modify the dipole response of a medium to a signal or "probe" field. Waveguide dispersion mechanisms such as photonic crystals, Coupled Resonator Optical Waveguides (CROW), and other micro-resonator structures modify the spatial component (k-vector) of a propagating wave. Slowlight can also be achieved exploiting the dispersion properties of planar waveguides realized with single negative metamaterials (SNM) or double negative metamaterials (DNM). A predominant figure of merit of slow light schemes is the "Delay-Bandwidth Product" (DBP). Most slow light schemes can actually offer an arbitrarily long delay for a given device length (length/delay = signal velocity) at the expense of bandwidth. The product of the two is roughly constant. A related figure of merit is the "fractional delay", the time a pulse is delayed divided by the total time of the pulse. Plasmon induced transparency – an analog of EIT - provides another approach based on the destructive interference between different resonance modes. Recent work has now demonstrated this effect over a broad transparency window across a frequency range greater than 0.40 THz Potential use. Slow light could be used to greatly reduce noise, which could allow all types of information to be transmitted more efficiently. Also, optical switches controlled by slow light could cut power requirements a million-fold compared to switches now operating everything from telephone equipment to supercomputers. Slowing light could lead to a more orderly traffic flow in networks. Meanwhile, slow light can be used to build interferometers that are far more sensitive to frequency shift as compared to conventional interferometers. This property can be used to build better, smaller frequency sensor and compact high resolution spectrometers. Slow light in fiction. Slow glass is a fictional material in Bob Shaw's short story "Light of Other Days" ("Analog", 1966), and several subsequent stories. The glass, which delays the passage of light by years or decades, is used to construct windows, called "scenedows", that enable city dwellers, submariners and prisoners to watch "live" countryside scenes. "Slow glass" is a material where the delay light takes in passing through the glass is attributed to photons passing "...through a spiral tunnel coiled outside the radius of capture of each atom in the glass." Shaw later reworked the stories into the novel "Other Days, Other Eyes" (1972). The slow light experiments are mentioned in Dave Eggers' novel "You Shall Know Our Velocity". In the novel, the speed of light is described as a "Sunday crawl". On Discworld, where Terry Pratchett's novel series takes place, light travels only a few hundred miles per hour due to Discworld's "embarrassingly strong" magic field. In Maurice Renard's novel "Le maître de la lumière" ("The Master of Light", 1933), the description of "“luminite”" might be one of the earliest mentions of slow glass [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@58777339 [docID]3850488 [hitPos]7 [correct]false [extraScores][F@79ea256 , [answer]A light meter is a device used to measure the amount of light. In photography, a light meter is often used to determine the proper exposure for a photograph. Typically a light meter will include a computer, either digital or analog, which allows the photographer to determine which shutter speed and f-number should be selected for an optimum exposure, given a certain lighting situation and film speed. Light meters are also used in the fields of cinematography and scenic design, in order to determine the optimum light level for a scene. They are used in the general field of lighting, where they can help to reduce the amount of waste light used in the home, light pollution outdoors, and plant growing to ensure proper light levels. Use in photography. The earliest type of light meters were called "extinction meters" and contained a numbered or lettered row of neutral density filters of increasing density. The photographer would position the meter in front of his subject and note the filter with the greatest density that still allowed incident light to pass through. The letter or number corresponding to the filter was used as an index into a chart of appropriate aperture and shutter speed combinations for a given film speed. Extinction meters suffered from the problem that they depended on the light sensitivity of the human eye (which can vary from person to person) and subjective interpretation. Later meters removed the human element and relied on technologies incorporating selenium, CdS, and silicon photodetectors. Selenium and silicon light meters use sensors that are photovoltaic: they "generate" a voltage proportional to light exposure. Selenium sensors generate enough voltage for direct connection to a meter; they need no battery to operate and this made them very convenient in completely mechanical cameras. Selenium sensors however cannot measure low light accurately (ordinary lightbulbs can take them close to their limits) and are altogether unable to measure very low light, such as candlelight, moonlight, starlight etc. Silicon sensors need an amplification circuit and require a power source such as batteries to operate. CdS light meters use a sensor based on photoresistance, i.e. their electrical resistance changes proportionately to light exposure. These also require a battery to operate. Most modern light meters use silicon or CdS sensors. They indicate the exposure either with a needle galvanometer or on an LCD screen. Many modern consumer still and video cameras include a built-in meter that measures a scene-wide light level and are able to make an approximate measure of appropriate exposure based on that. Photographers working with controlled lighting and cinematographers use handheld light meters to precisely measure the light falling on various parts of their subjects and use suitable lighting to produce the desired exposure levels. There are two general types of light meters: reflected-light and incident-light. Reflected-light meters measure the light "reflected by the scene" to be photographed. All in-camera meters are reflected-light meters. Reflected-light meters are calibrated to show the appropriate exposure for “average” scenes. An unusual scene with a preponderance of light colors or specular highlights would have a higher reflectance; a reflected-light meter taking a reading would incorrectly compensate for the difference in reflectance and lead to underexposure. Badly underexposed sunset photos are common exactly because of this effect: the brightness of the setting sun fools the camera's light meter and, unless the in-camera logic or the photographer take care to compensate, the picture will be grossly underexposed and dull. This pitfall is avoided by incident-light meters which measure the amount of light "falling on the subject" using an integrating sphere (usually, a translucent hemispherical plastic dome is used to approximate this) placed on top of the light sensor. Because the incident-light reading is independent of the subject's reflectance, it is less likely to lead to incorrect exposures for subjects with unusual average reflectance. Taking an incident-light reading requires placing the meter at the subject's position and pointing it in the general direction of the camera, something not always achievable in practice, e.g., in landscape photography where the subject distance approaches infinity. Another way to avoid under- or over-exposure for subjects with unusual reflectance is to use a spot meter: a reflected-light meter that measures light in a very tight cone, typically with a one degree circular angle of view. An experienced photographer can take multiple readings over the shadows, midrange and highlights of the scene to determine optimal exposure, using systems like the Zone System. Many modern cameras include sophisticated multi-segment metering systems that measure the luminance of different parts of the scene to determine the optimal exposure. When using a film whose spectral sensitivity is not a good match to that of the light meter, for example orthochromatic black-and-white or infrared film, the meter may require special filters and re-calibration to match the sensitivity of the film. There are other types of specialized photographic light meters. Flash meters are used in flash photography to verify correct exposure. Color meters are used where high fidelity in color reproduction is required. Densitometers are used in photographic reproduction. Exposure meter calibration. In most cases, an incident-light meter will cause a medium tone to be recorded as a medium tone, and a reflected-light meter will cause "whatever is metered" to be recorded as a medium tone. What constitutes a “medium tone” depends on meter calibration and several other factors, including film processing or digital image conversion. Meter calibration establishes the relationship between subject lighting and recommended camera settings. The calibration of photographic light meters is covered by . Exposure equations. For reflected-light meters, camera settings are related to ISO speed and subject luminance by the reflected-light exposure equation: formula_1 where For incident-light meters, camera settings are related to ISO speed and subject illuminance by the incident-light exposure equation: where Calibration constants. Determination of calibration constants has been largely subjective; The constants formula_6 and formula_9 shall be chosen by statistical analysis of the results of a large number of tests carried out to determine the acceptability to a large number of observers, of a number of photographs, for which the exposure was known, obtained under various conditions of subject manner and over a range of luminances. In practice, the variation of the calibration constants among manufacturers is considerably less than this statement might imply, and values have changed little since the early 1970s. of 10.6 to 13.4 with luminance in cd/m². Two values for formula_6 are in common use: 12.5 (Canon, Nikon, and Sekonic) and 14 (Minolta, Kenko, and Pentax); the difference between the two values is approximately 1/6 EV. The earliest calibration standards were developed for use with wide-angle averaging reflected-light meters (Jones and Condit 1941). Although wide-angle average metering has largely given way to other metering sensitivity patterns (e.g., spot, center-weighted, and multi-segment), the values for formula_6 determined for wide-angle averaging meters have remained. The incident-light calibration constant depends on the type of light receptor. Two receptor types are common: flat (cosine-responding) and hemispherical (cardioid-responding). With a flat receptor, formula_9 of 240 to 400 with illuminance in lux; a value of 250 is commonly used. A flat receptor typically is used for measurement of lighting ratios, for measurement of illuminance, and occasionally, for determining exposure for a flat subject. For determining practical photographic exposure, a hemispherical receptor has proven more effective. Don Norwood, inventor of incident-light exposure meter with a hemispherical receptor, thought that a sphere was a reasonable representation of a photographic subject. According to his patent (Norwood 1938), the objective was to provide an exposure meter which is substantially uniformly responsive to light incident upon the photographic subject from practically all directions which would result in the reflection of light to the camera or other photographic register. and the meter provided for "measurement of the effective illumination obtaining at the position of the subject." With a hemispherical receptor, recommends a range for formula_9 of 320 to 540 with illuminance in lux; in practice, values typically are between 320 (Minolta) and 340 (Sekonic). The relative responses of flat and hemispherical receptors depend upon the number and type of light sources; when each receptor is pointed at a small light source, a hemispherical receptor with formula_9 = 330 will indicate an exposure approximately 0.40 step greater than that indicated by a flat receptor with formula_9 = 250. With a slightly revised definition of illuminance, measurements with a hemispherical receptor indicate “effective scene illuminance.” Calibrated reflectance. It is commonly stated that reflected-light meters are calibrated to an 18% reflectance, but the calibration has nothing to do with reflectance, as should be evident from the exposure formulas. However, some notion of reflectance is implied by a comparison of incident- and reflected-light meter calibration. Combining the reflected-light and incident-light exposure equations and rearranging gives Reflectance formula_20 is defined as A uniform perfect diffuser (i.e., one following Lambert's cosine law) of luminance formula_4 emits a flux density of formula_23formula_4; reflectance then is Illuminance is measured with a flat receptor. It is straightforward to compare an incident-light measurement using a flat receptor with a reflected-light measurement of a uniformly illuminated flat surface of constant reflectance. Using values of 12.5 for formula_6 and 250 for formula_9 gives With a formula_6 of 14, the reflectance would be 17.6%, close to that of a standard 18% neutral test card. In theory, an incident-light measurement should agree with a reflected-light measurement of a test card of suitable reflectance that is perpendicular to the direction to the meter. However, a test card seldom is a uniform diffuser, so incident- and reflected-light measurements might differ slightly. In a typical scene, many elements are not flat and are at various orientations to the camera, so that for practical photography, a hemispherical receptor usually has proven more effective for determining exposure. Using values of 12.5 for formula_6 and 330 for formula_9 gives With a slightly revised definition of reflectance, this result can be taken as indicating that the average scene reflectance is approximately 12%. A typical scene includes shaded areas as well as areas that receive direct illumination, and a wide-angle averaging reflected-light meter responds to these differences in illumination as well as differing reflectances of various scene elements. Average scene reflectance then would be where “effective scene illuminance” is that measured by a meter with a hemispherical receptor. to be measured by aiming the receptor at a transilluminated diffuse surface, and for incident-light calibration to be measured by aiming the receptor at a point source in a darkened room. For a perfectly diffusing test card and perfectly diffusing flat receptor, the comparison between a reflected-light measurement and an incident-light measurement is valid for any position of the light source. However, the response of a hemispherical receptor to an off-axis light source is approximately that of a cardioid rather than a cosine, so the 12% “reflectance” determined for an incident-light meter with a hemispherical receptor is valid only when the light source is on the receptor axis. Cameras with internal meters. Calibration of cameras with internal meters is covered by nonetheless, many manufacturers specify (though seldom state) exposure calibration in terms of formula_6, and many calibration instruments (e.g., Kyoritsu-Arrowin multi-function camera testers ) use the specified formula_6 to set the test parameters. Exposure determination with a neutral test card. If a scene differs considerably from a statistically average scene, a wide-angle averaging reflected-light measurement may not indicate the correct exposure. To simulate an average scene, a substitute measurement sometimes is made of a neutral test card, or "gray card". At best, a flat card is an approximation to a three-dimensional scene, and measurement of a test card may lead to underexposure unless adjustment is made. The instructions for a Kodak neutral test card recommend that the indicated exposure be increased by ½ step for a frontlighted scene in sunlight. The instructions also recommend that the test card be held vertically and faced in a direction midway between the Sun and the camera; similar directions are also given in the "Kodak Professional Photoguide". The combination of exposure increase and the card orientation gives recommended exposures that are reasonably close to those given by an incident-light meter with a hemispherical receptor when metering with an off-axis light source. In practice, additional complications may arise. Many neutral test cards are far from perfectly diffuse reflectors, and specular reflections can cause increased reflected-light meter readings that, if followed, would result in underexposure. It is possible that the neutral test card instructions include a correction for specular reflections. Use in illumination. Light meters or light detectors are also used in illumination. Their purpose is to measure the illumination level in the interior and to switch off or reduce the output level of luminaires. This can greatly reduce the energy burden of the building by significantly increasing the efficiency of its lighting system. It is therefore recommended to use light meters in lighting systems, especially in rooms where one cannot expect users to pay attention to manually switching off the lights. Examples include hallways, stairs, and big halls. There are, however, significant obstacles to overcome in order to achieve a successful implementation of light meters in lighting systems, of which user acceptance is by far the most formidable. Unexpected or too frequent switching and too bright or too dark rooms are very annoying and disturbing for users of the rooms. Therefore, different switching algorithms have been developed [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@244efc35 [docID]168435 [hitPos]8 [correct]false [extraScores][F@45ed2ce9 , [answer]Railway signals in Germany are regulated by the "Eisenbahn-Signalordnung" (ESO, railway signalling rules). There are several signalling systems in use: History. Reichsbahn. Originally, the railway company of each German state had its own signalling system. After these companies were merged into the (Deutsche Reichsbahn), a common signalling system, the H/V system, was created based on two key types of signal. However, Bavaria was permitted to use its own designs of signal, with slightly different meanings, as part of the national scheme for many years thereafter. Main signals (German: "Hauptsignale") allowed the train to proceed or not. These signals were semaphores, whose arms had a circle on their right-hand end, and were painted white with red edges (occasionally the colours were reversed). There would be at least one arm, which meant "stop" when aligned horizontally and "proceed" when inclined upwards at 45 degrees. Up to two additional arms could be mounted below for different routes. They were inclined at 45 degrees upwards when a diverging route was to be taken, and aligned vertically when the main route was set. At night, a horizontal arm showed a red light, a diagonal arm showed a green and a vertical arm showed none. Route signalling was changed circa 1930 to speed signalling. On a double armed-signal, one horizontal arm meant "stop" (aspect Hp 0), one inclined arm meant "proceed at line speed" (Hp 1) and two inclined arms meant "proceed at reduced speed" (Hp 2 - usually 40 km/h). Third arms were removed; in 1948, the green light of the lower arm was changed to yellow. This system is still in use today. There were also distant signals (German: "Vorsignale") to advise the driver as to what aspect the next main signal showed, so that he could slow or stop his train in time if necessary. Unlike in British practice, every main signal had its own distant; and each main signal-distant signal pair was worked from a single signal lever. German distant signals took the form of a yellow circular board with a black edge. If the board was visible, it meant that the main signal was at "danger". To indicate that the main signal was showing "clear", the board was flipped horizontally through 90 degrees so as to be practically invisible. If the main signal to which the distant applied was a double-armed one, then a yellow arm, bordered black, was mounted on the post below the disc. If the main signal showed "proceed at reduced speed", then as well as showing the disc, the arm was inclined at 45 degrees. At night, distant signals showed two lights, one diagonally left below the other. Reading downwards, green-green meant "expect clear" (Vr 1), yellow-yellow meant "expect stop" (Vr 0), and green-yellow meant "expect clear at reduced speed" (Vr 2). The first colour-light signals had come into use, but were not widespread when the country was divided in 1949. These signals simply showed the night aspects of the semaphore signals, and as such are not considered as a separate signalling system. Only the S-Bahn systems in Berlin and Hamburg were equipped with newly designed colour-light signals at that time, of the Sv system. Post WWII. Two new companies were founded; the Deutsche Bundesbahn in West Germany and the Deutsche Reichsbahn in East Germany. The Reichsbahn kept the name of the old Deutsche Reichsbahn, to keep infrastructure and the right to operate trains in West Berlin. Both companies had their own regulations from then on and they continued separately until 1990 when the country and the rail companies were reunified. Even today, the signalling systems of the two halves of the country differ, and although a new system has been created which will eventually become universal, the vast size of Germany's railway network means that different systems are likely to remain in use for many years to come. West. The Deutsche Bundesbahn mainly kept the H/V signalling, with colour-lights directly replacing sempahore signals; nevertheless, the signals did change their design over the years. One very early change was the inclusion of shunting signals (Sh, Schutzsignale) into the main signal's head. This affected mainly exit signals. These signals show two red lights to disallow any train or shunting movements and a red-white-white combination to allow shunting movements only. Later the backup signal aspects (Zs1, Zs7, Zs8), which had their own head, were also merged into the main head. The new signals also underwent aesthetic change; all the signals, whether distant, main or shunting, were merged into a common rectangular head. The signalling system didn't change much to allow Gleiswechselbetrieb (regular wrong line working - i.e. running on left-hand track instead of right-hand). Only a new aspect (Zs6) was created to inform the driver if he was to be diverted onto the wrong line. Several minor experiments with new signalling systems were made, but only one survived: the Augsburg - Donauwörth line, equipped with Sk-type signals. The signals still exist, and the new common (West and East) signalling system were developed from these. East. The Deutsche Reichsbahn adopted a new signalling system from the OSShd, an international organization of railways, mainly "Eastern" ones. H/V colour-light signals as built in the West did exist in some places in the GDR where they were installed before the war, but most colour-light signals in the East were of the OSShd-designed Hl system. Reunification. Several signalling systems and two sets of regulations exist within reunified Germany. The regulations are being gradually merged to form a single set, a process still in progress. Most of the network will come under the control of centralized signal boxes; during this process complete lines generally change to the new common Ks signalling system. However, old-style signals are still installed on a few new lines for various reasons, usually if no surrounding lines have Ks-type signals. Even new semaphores are still occasionally installed, though now mainly as spares in semaphore-only areas. Currently no new signalling system is being developed; the current systems are perfectly adeqate for today's operations. The next logical step will be a system without fixed signals, as current developments show: LZB, ETCS Compared to other countries. Systems similar to the H/V system can be found in Austria and Switzerland. Most of the aspects look identical, though the lamps are sometimes aligned differently. Austrian signals have one more and Swiss signals two more possible aspects. They can show a speed limit/announcement of 60 km/h and 90 km/h (Swiss only). Systems similar to the Hl system can be found in many former Eastern bloc countries. The Ks system can be considered similar to signals in many countries, at least in appearance, as the colour codes: red for "stop", yellow for "caution" and green for "proceed" are commonly used in other countries. Compared to UK Signalling. UK Signalling seems to be similar to the Ks system at first sight. The major difference is that British signals do not work on the principle of speed signalling. To prevent trains running over a switch too fast several workarounds are in use. One of them is the "approach release". With this system the train approaches a junction seeing a "warning" aspect, the signal before the junction stays at "stop". It is released to a "clear" aspect once the train has reduced its speed. Normally, however, only the route a train is to take is indicated; with semaphores, this is done using several arms side-by-side, each applying to a separate route; and with colour-lights, a 'feather' - a row of white lights which 'point' in the direction the train is to take. In both cases, speed adjustment depends solely on the driver's judgement and his route knowledge. In Germany route indicators are used in several cases but do not have an influence on the speed; they are used purely to prove to the driver that the route is correctly set. Main signal/distant signal (H/V). The most common system is the H/V (Hauptsignal/Vorsignal) system which consists of main signals and distant signals. Depending on the technology used in the signal box, these signals may be indicated by semaphores or colour-light signals. The colour-lights show the same light patterns the semaphores show at night. The aspects shown here may be accompanied by Zs2, Zs2v, Zs3, Zs3v, Zs6 (see section #Additional signals) or Lf signals (see section #Langsamfahrsignale) in order to show speed limits or the choice of track that follows. A main signal (with lights only) is marked by a "Mastschild" (board with a colour code) to protect in case of a signal lights failure. The location of a distant signal is marked by an Ne2 (unless it is a repetition or it is at the location of a main signal) also for the case where the signal lights fail or where a semaphore distant signal shows Vr1 (see section #Nebensignale). Hp 0: Stop. A main signal ("Hauptsignal") shows Hp 0 by a horizontal semaphore arm, one red light or two horizontal red lights. The signal requires the driver to stop in front of it. (Originally, the aspect with two red lights was called Hp 00 and had a different meaning than a single red light with regards to shunting, but nowadays, the meaning is the same.) Usually, Hp 0 is protected by a 2000 Hz magnet of the PZB train safety system which triggers the emergency brake should the train pass the signal. Vr 0: Expect stop. A distant signal ("Vorsignal") shows Vr 0 by a yellow disc or two yellow lights (the right light is above the left light). This signal indicates that its respective main signal shows Hp 0. As the signal is placed at braking distance from the main signal, the driver needs to apply the brakes immediately to stop the train before reaching the main signal. Vr 0 is usually accompanied by an activated 1000 Hz magnet of the PZB train safety system. Passing it triggers hidden timers in the vehicle's part of the PZB system, which first checks that the driver acknowledges the distant signal, then it supervises the braking. If the acknowledgement does not occur in time or if, at any point, the system detects that the train´s speed exceeds the limits of a certain braking profile, it triggers the emergency brake. Hp 1: Proceed. Aspect Hp 1, indicated by one raised semaphore arm or a single green light, allows the driver to proceed at the speed indicated in his schedule. Vr 1: Expect proceed. Vr 1 indicates that the main signal shows Hp 1. On a semaphore, the yellow disc is flipped horizontally through 90 degrees, while colour-light signals (or the semaphores at night) show two green lights with the right one above the left one. Hp 2: Proceed at reduced speed. If both arms of the semaphore point up, or a green light is show vertically above a yellow light, the train may proceed but at reduced speed. Unless another speed is indicated by additional signals, the train must obey a speed limit of 40 km/h. Hp 2 usually indicates that the next block is free, but that the points/switches are set so as to make running over them at full speed dangerous. Vr 2: Expect proceed at reduced speed. Vr 2 orders the driver to slow the train to 40 km/h prior to reaching the main signal showing Hp 2. Other speed limits may be indicated by additional signals. Hl Signals. Hl-signals are used only in East Germany. The upper part of the signal is a distant signal, the lower part is the main signal. Signals can be found without the main signal section or without the distant signal section. Hl distant aspects. There is a green and a yellow light, both of which can flash or remain steady. There are four different aspects: Yellow: Expect Stop Flashing Yellow: Expect Proceed with 40 km/h or 60 km/h Flashing Green: Expect Proceed with 100 km/h Green: Expect Proceed with maximum speed Hl main aspects. There is a yellow and a red light on the signal, and additionally a bar with four green and four yellow lights below the signal. No Light: Stop, resolve situation by special procedures (both mandated by signal post's color marking) Red: Stop Yellow: Proceed with speed limit If there is a speed limit (yellow light) it is shown by the green or yellow bar: No bar: 40 km/h Yellow bar: 60 km/h Green bar: 100 km/h Kombinationsignale. After the reunification 1990, a new signalling system using "Kombinationsignale" (English: "combination signals"; abbreviated as Ks) was designed to create a common system for East and West. While West Germany uses signals which simply show the night aspects of the semaphores, East Germany designed new light signals similar to those in other Eastern European countries. Since 2000, new signals have mostly been of the Ks system. There are exceptions; for example, H/V signals were installed on the light rail extension in Kassel post-2000 because it the authorities wanted to avoid the risk of driver confusion on busy lines. The new Ks signals have three lights green, yellow and red, and have only three aspects. Basically, the signal meanings of the Ks signals match the ones of the H/V signals and vice versa, although the aspects may look differently. The PZB and LZB technologies are used the same way as for H/V signals. Hp 0 Stop. One red light. The Ks system uses the same aspect for "stop" as the H/V system. Ks 1 Proceed at given speed, expect proceed at given speed. One green light. Additionally a white number above the signal (see Zs3) can reduce the speed. It shows the tenth of the speed limit from this signal. There may also be a yellow number below the signal (see Zs3v) showing the speed limit from the next signal, only shown if the speed limit at the next signal is reduced compared to the limit at the current signal. If such a yellow number is shown, the green light flashes. Ks 2 Proceed at given speed but expect stop. One yellow light. Speed can be reduced from here as described for Ks 1. Additional signals. A number of additional signals provide additional information to the driver or modify the meaning of other signals. They are named "Zs" (German "Zusatzsignal": Additional signal) plus a number. The aspect names differ in East and West; those shown here are the names used in West Germany. Zs1 Ersatzsignal. Ersatzsignal = Subsidiary signal Three white lights aligned as a triangle or one flashing white light. Meaning: Train may pass this signal with a maximum speed of 40 km/h until the last set of points/switch, after last points/switch regular speed is permitted. This signal may be used when a main signal fails. One possibility is that one or more bulbs don't work, and the necessary aspect cannot be shown. For example, a signal which should show Hp2 (green + yellow, proceed with 40 km/h) could have a broken yellow bulb. It would therefore show Hp1 (green, proceed with maximum speed). This would be dangerous because it implies to the driver that he could run faster than is safe, so it must show something else. Another possibility is that one of the 'proving' devices (devices for proving that a section of line is safe - e.g. that a set of points/switch has moved completely). In this case, the section of track or set of points/switch affected must be manually checked to ensure that all is safe for the passage of trains, and the signal Zs1 would be given. Zs2 Richtungsanzeiger. Richtungsanzeiger = Direction display Shows one single white letter indicating direction. The letter is usually the initial letter of the next station or the next major station. This signal does not give any specific command to the driver, as signalling safety devices mean that being diverted to the wrong route is not necessarily dangerous. It does, however, prove to the driver that the route is correctly set up, and if the route is wrongly set he could ask the signalman to re-set the route before starting the train. Otherwise, if the route was set up wrongly, he would start the train, set off down the wrong route, discover the mistake, need to set back, have the route re-set, etc., all of which would waste a lot of time. Zs2v Richtungsvoranzeiger. Richtungsvoranzeiger = Distant direction display Shows one single yellow letter, telling the driver how the route is set after the next signal. Meaning: Expect Zs2 Zs3 Geschwindigkeitsanzeiger. Geschwindigkeitsanzeiger = Speed display Shows one white number indicating the tenth of the speed allowed from this point. Usually used in combination with main signals. Zs3 are used for speed limits when leaving the main line. Permanent speed restrictions are displayed using Lf plates. Zs3v Geschwindigkeitsvoranzeiger. Geschwindigkeitsvoranzeiger = Distant speed display Shows one yellow number indicating the tenth of the speed allowed from the point where the following Zs3 signal is found. Usually used in combination with distant signals. Zs4/Zs5 Beschleunigungsanzeiger/Verzögerungsanzeiger (obsolete). Beschleunigungsanzeiger = Acceleration display Verzögerungsanzeiger = Delaying display Used to indicate the train driver that he should accelerate or slow down to optimize the occupancy of the line. A train driver could slow down to avoid a signal showing "Stop" for himself, or accelerate to avoid a stop for the following train. Zs6 Gleiswechselanzeiger. Gleiswechselanzeiger = Track change display Indicates that the wrong line (left-hand) is to be used from this signal. Used only on lines where wrong line working is frequent. Zs7 Vorsichtsignal. Vorsichtsignal = Caution signal Three yellow lights aligned as a triangle pointing down. Meaning: Train may pass this signal but must be prepared to stop at any obstacle. The speed has to be chosen according to visibility, but must not exceed 40 km/h. Used only in combination with main signals. This signal is used for the same cases as Zs1. The difference is that for a Zs1 it is checked "for" the driver that the line is clear, whereas at a Zs7 the train driver must check "for himself" that the line is clear. Zs8 Gegengleisfahrt-Ersatzsignal. Gegengleisfahrt-Ersatzsignal = Against track crossing signal replacementl Three white flashing lights aligned as a triangle or a flashing diagonal light Indicates that a signal at danger may be passed when wrong line working is in force. If the signal guards points/switches the speed limit is 40 km/h; otherwise, the limit is 100 km/h to the next station. As with other 'wrong line operation' signals, this is only used where wrong line operation is frequent. Zs10 Ende der Geschwindigkeitsbeschränkung. Ende der Geschwindigkeitsbeschränkung = End of speed restriction A black arrow with white border pointing up. Meaning: End of Zs3 or Hp2 speed restriction Langsamfahrsignale. Langsamfahrsignale = Slow travel signal (speed limit) There are several plates to display speed limits, of two basic types: signs showing the maximum speed and, where the speed limit drops, advance warning signs. The Lf1, Lf4 and Lf6 plates are advance warning boards, while the Lf2, Lf5 and Lf7 announce the start of speed limits. The Lf3 plate marks the end of temporary speed limits, but the end of a permanent speed limit was not usually shown. Changes as of 2007: Lf4/Lf5 are obsolete and are being replaced by Lf6/Lf7; An increase in speed limit will now always be shown by a Lf7. Nebensignale. Nebensignale (possible translations: auxiliary/lateral signals) are signals that do not fit in any other category; some of them give hints rather than requiring action. Ne1 Trapeztafel. Trapeztafel = Trapezoid board Meaning: Stop here to await permission to proceed. Trains must stop here to wait for visual or verbal signal to proceed. Found on lightly-used lines where telephone block operation (without signals) is in force; it effectively replaces a station's entry signal. Ne2 Vorsignaltafel. Vorsignaltafel = Distant signal board Used alone or in combination with a distant signal. Meaning: This is a distant signal. (Even if there is none visible) A main signal, shunting signal or a Ne1 will be found at braking distance. Unless a distant signal, distant signal repeater or main signal is visible, the train driver treats this signal as "Vr0 or Ks2: Expect Stop". Every distant signal is mounted with a Ne2 plate to mark their position (if this plate was not fitted, the driver would not know if he had passed the distant signal or not when showing aspect Vr1, since in this position it is practically invisible). Distant signal repeaters or distant signals mounted in combination with a main signal do not have a Ne2 plate. Ne3 Vorsignalbaken. Vorsignalbaken = Distant signal beacon Used to announce distant signals. Posted 250m, 175m and 100m before distant signal. Generally 3 plates are used, but up to 5 may be used at hard-to-see locations. Fewer than 3 are used if the distance from the previous main signal is too small to install them all. Ne4 Schachbretttafel. Schachbretttafel = Checkerboard Used to indicate that a signal is not in its usual position (i.e. if the signal is further to the right than normal or of ot is on the wrong side of the track to aid visibility). This plate is posted where the signal would be if conditions were normal. Ne5 Haltetafel. Haltetafel = Stop board Used to indicate where a passenger train should stop. It can be combined with additional signs indicating what types of trains should stop here, e.g. "240 m" for trains at most this length. Ne6 Haltepunkttafel. Haltepunkttafel = Stopping point board Meaning: Expect a stopping point within braking distance. This is only used to announce halts that are not stations. Ne7 Schneepflugsignal. Schneepflugsignal = Snowplough signal Gives information to snowplough crews. A yellow or white 'V' shape indicates 'lower snowplough', an inverted yellow or white 'V' indicates 'raise snowplough'. These are placed to warn the crew to raise the snowplough so as to avoid hitting any obstructions at rail level, such as level crossings/grade crossings. Schutzsignale. Schutzsignale = guard signals. These signals guard shunting movements. Shunting movements do not leave a station whereas train movements do. These signals are sometimes combined with main signals, which is the reason why main signals sometimes have one red light and sometimes two. Historically, shunting movements were allowed to ignore main signals. They had to stop only at main signals with two red lights, because a double red light means Hp0 (stop for train movements) and additionally Sh0 (stop for shunting movements). When they were allowed to pass this signal, it showed a red and two white lights, still forbidding train movements (Hp0 + Sh1). After a change, a red always means stop for all movements. Therefore only one red light is needed. A double red has the same meaning and Sh0 is now nearly equal to Hp0 and will be replaced in most cases. Sh0: A white circle with a black line horizontally across it, mounted on a black box; or, two horizontally-mounted red lights. It means "Fahrverbot" (literally "movement forbidden"), and all train and shunting movements must stop short of it. 'Black box' indicators fixed at aspect Sh0 are commonly found mounted on buffer stops. Sh1: A white circle with a black diagonal line across it, mounted on a black box; or, two white lights aligned diagonally (sometimes with a red light as well). It means "shunting movements allowed but train movements forbidden". Sh2: A rectangular red plate used at the end of the track or where the line is blocked, for example by engineering works. It displays a single red light by night. Catenary Signals/"Fahrleitungssignale". All these signals consist of a blue diamond-shaped board with various patterns of white markings. As such, only the white markings are described in this section. As catenary signals, they apply only to electric trains. El1v. Two horizontally-aligned square white dots, on centreline of board. This warns the driver to switch off the locomotive's main switch. El1. Similar to El1v but with a horizontal line under the square dots (dots do not join the line). Locomotive's main switch must be off when this board is passed. El2. A square U-shape. Main switch can be turned on after this board. El3. Two horizontal lines across the board, about one quarter and three-quarters of the way down it. Warns driver to lower the locomotive's pantograph. El4. One horizontal line across the centre of the board. Pantograph must be down when this board is passed. El5. Verical line through the centre of the board. Pantograph may be raised after passing this board. El6. Concentric squares of blue and white. Indicates 'stop for vehicles with raised pantographs'. This sign is most commonly seen at the end of the catenary. Miscellaneous. One white light on Hp signal: This signal has failed or is otherwise out of use. Ignore it. A white light on a H/V distant signal: The distance to main signal is shorter than usual (when signal has a Ne2-plate) or this is a repeater (without Ne2-plate). A white light in the top left corner of a Ks-Signal: The distance to main signal is shorter than usual. A white light in the bottom left corner of a Ks-Signal: This is a repeater. A flashing white light: Not a "Kennlicht" (marker light), see Zs1 Two white lights: Not a "Kennlicht", see Sh1 Examples. Example 1. Entering a station (see image) Left: Secondary line with minimum signalling Right: Main Line with standard signalling Example 2. The lower track is blocked. A train going from left to right cannot use the regular track. It must use the wrong track. The exit-signal of the left station shows Hp0 (one or two red lights: "Stop"), because there is no regular route to the wrong track. The signal shows also Zs8 (three white flashing lights: "Proceed on wrong track"). The signal cannot show Sh1 ("Shunting allowed") in this case, because the ride would end at the Ra10-plate ("Stop for shunting movements"). When approaching the next station, the train will find an Ne2-plate, which substitiutes a distant signal and in this case means "Expect Stop". At the position of the entry-signal of the right station, a Ne4 plate shows that a signal is not where it is expected. Instead, the signal is mounted right of the track. This signal shows Sh1 (two white lights: "Shunting allowed"), the train may enter the station. The signal could also stand on the left side of the track, the Ne4 plate is not used then. Example 3. How Kennlichts are used. On the first signal you see Hp1/Vr1 as described in the H/V section of this article. On the next one you see Hp1/Vr0, which means "proceed, expect stop at braking distance". On the third one, you see a Kennlicht, which means that the main signal is switched off (=there is none). The distant signal shows Vr0 + Kennlicht, which means "expect stop before braking distance". The last one shows the expected "stop". These combinations are used, if the signals are mounted at the half braking distance. Gallery. Footnotes. 1: A white cross (with a black border) on a signal informs the driver to disregard the signal. In all the cases shown, this is due to the newly installed signals not being operative yet [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@61f25b53 [docID]3957538 [hitPos]8 [correct]false [extraScores][F@1601734f , [answer]A Slower Speed of Light" is a freeware video game developed by MIT Game Lab that demonstrates the effects of special relativity by gradually slowing down the speed of light to a walking pace. The game runs on the Unity engine using the OpenRelativity toolkit. Gameplay. In "A Slower Speed of Light", the player controls the ghost of a young child who was killed in an unspecified accident. The child wants to "become one with light", but the speed of light is too fast for the child. This is solved through the use of magic orbs which, as each are collected, slow down the speed of light, until by the end it is at walking speed. These orbs are spread throughout the level. At the beginning of the game, walking around and collecting these orbs is easy; however, as the game progresses, the effects of special relativity become apparent. This gradually increases the difficulty of the game. Effects of Special Relativity. As the game progresses and light becomes slower, the effects of special relativity start to become more apparent. These effects include the Doppler effect (red/blue-shifting of visible light and the shifting of ultraviolet and infrared into the visible spectrum), the searchlight effect (increased brightness in the direction of travel), time dilation (difference between the passage of time perceived by the player and the outside world), the Lorentz transformation (the perceived warping of the environment at near-light speeds), and the runtime effect (seeing objects in the past because of the speed of light). These effects combine as the game progresses to increase the difficulty and challenge the player. OpenRelativity. OpenRelativity is a toolkit designed for use with the Unity game engine. It was developed by MIT Game Lab during the development of "A Slower Speed of Light". The toolkit allows for the accurate simulation of a 3D environment when light is slowed down. It is avaliable for download for free on the MIT site. Use in Education. "A Slower Speed of Light" was developed in hopes of being used as an educational tool to explain special relativity in an easy-to-understand fashion. The game is meant to be used as an interactive learning tool for those interested in physics [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@12925ecb [docID]41452537 [hitPos]8 [correct]false [extraScores][F@4587af8e , [answer]In optics the refractive index or index of refraction "n" of a substance (optical medium) is a dimensionless number that describes how light, or any other radiation, propagates through that medium. It is defined as where "c" is the speed of light in vacuum and "v" is the speed of light in the substance. For example, the refractive index of water is 1.33, meaning that light travels 1.33 times slower in water than it does in vacuum. (See typical values for different materials here.) The historically first occurrence of the refractive index was in Snell's law of refraction, "n"1sin"θ"1= "n"2sin"θ"2, where θ1 and θ2 are the angles of incidence of a ray crossing the interface between two media with refractive indices "n"1 and "n"2. Brewster's angle, the critical angle for total internal reflection, and the reflectivity of a surface also depend on the refractive index, as described by the Fresnel equations. The refractive index can be seen as the factor by which the velocity and the wavelength of the radiation are reduced with respect to their vacuum values: the speed of light in a medium is formula_2 and similarly the wavelength in that medium is formula_3, where formula_4 is the wavelength of that light in vacuum. This implies that vacuum has a refractive index of 1, and that frequency (formula_5) of the wave is not affected during refraction. Historically other reference media (e.g., air at a standardized pressure and temperature) have been common. Refractive index of materials varies with the wavelength. This is called dispersion; it causes the splitting of white light in prisms and rainbows, and chromatic aberration in lenses. In opaque media, the refractive index is a complex number: while the real part describes refraction, the imaginary part accounts for absorption. The concept of refractive index is widely used within the full electromagnetic spectrum, from x-rays to radio waves. It can also be used with wave phenomena other than light (e.g., sound). In this case the speed of sound is used instead of that of light and a reference medium other than vacuum must be chosen. History. Thomas Young was presumably the person who first used, and invented, the name "index of refraction", in 1807. At the same time he changed this value of refractive power into a single number, instead of the traditional ratio of two numbers. The ratio had the disadvantage of different appearances. Newton, who called it the "proportion of the sines of incidence and refraction", wrote it as a ratio of two numbers, like "529 to 396" (or "nearly 4 to 3"; for water). Hauksbee, who called it the "ratio of refraction", wrote it as a ratio with a fixed numerator, like "10000 to 7451.9" (for urine). Hutton wrote it as a ratio with a fixed denominator, like 1.3358 to 1 (water). Young did not use a symbol for the index of refraction, in 1807. In the next years, others started using different symbols: n, m, and µ. Typical values. For visible light most transparent media have refractive indices between 1 and 2. A few examples are given in the table to the right. These values are measured at the yellow doublet sodium D-line, with a wavelength of 589 nanometers, as is conventionally done. Gases at atmospheric pressure have refractive indices close to 1 because of their low density. Almost all solids and liquids have refractive indices above 1.3, with aerogel as the clear exception. Aerogel is a very low density solid that can be produced with refractive index in the range from 1.002 to 1.265. Diamond lies at the other end of the range with a refractive index as high as 2.42. Most plastics have refractive indices in the range from 1.3 to 1.7, but some high-refractive-index polymers can have values as high as 1.76. For infrared light refractive indices can be considerably higher. Germanium is transparent in this region and has a refractive index of about 4, making it an important material for infrared optics. Refractive index below 1. A widespread misconception is that since, according to the theory of relativity, nothing can travel faster than the speed of light in vacuum, the refractive index cannot be lower than 1. This is erroneous since the refractive index measures the phase velocity of light, which does not carry information. The phase velocity is the speed at which the crests of the wave move and can be faster than the speed of light in vacuum, and thereby give a refractive index below 1. This can occur close to resonance frequencies, for absorbing media, in plasmas, and for x-rays. In the x-ray regime the refractive indices are lower than but very close to 1 (exceptions close to some resonance frequencies). As an example, water has a refractive index of 1 − for X-ray radiation at a photon energy of (0.04 nm wavelength). Negative refractive index. Recent research has also demonstrated the existence of materials with a negative refractive index, which can occur if permittivity and permeability have simultaneous negative values. This can be achieved with periodically constructed metamaterials. The resulting negative refraction (i.e., a reversal of Snell's law) offers the possibility of the superlens and other exotic phenomena. Microscopic explanation. At the microscale, an electromagnetic wave's phase speed is slowed in a material because the electric field creates a disturbance in the charges of each atom (primarily the electrons) proportional to the electric susceptibility of the medium. (Similarly, the magnetic field creates a disturbance proportional to the magnetic susceptibility.) As the electromagnetic fields oscillate in the wave, the charges in the material will be "shaken" back and forth at the same frequency. The charges thus radiate their own electromagnetic wave that is at the same frequency, but usually with a phase delay, as the charges may move out of phase with the force driving them (see sinusoidally driven harmonic oscillator). The light wave traveling in the medium is the macroscopic superposition (sum) of all such contributions in the material: the original wave plus the waves radiated by all the moving charges. This wave is typically a wave with the same frequency but shorter wavelength than the original, leading to a slowing of the wave's phase speed. Most of the radiation from oscillating material charges will modify the incoming wave, changing its velocity. However, some net energy will be radiated in other directions or even at other frequencies (see scattering). Depending on the relative phase of the original driving wave and the waves radiated by the charge motion, there are several possibilities: For most materials at visible-light frequencies, the phase is somewhere between 90° and 180°, corresponding to a combination of both refraction and absorption. Dispersion. The refractive index of materials varies with the wavelength (and frequency) of light. This is called dispersion and causes prisms to divide white light into its constituent spectral colors, and explains how rainbows are formed. As the refractive index varies with wavelength, according to Snell's law, so will the refraction angle as light goes from one material to another. This makes different colors go in different directions. Dispersion also causes the focal length of lenses to be wavelength dependent. This is a type of chromatic aberration, which often needs to be corrected for in imaging systems. In regions of the spectrum where the material does not absorb, the refractive index tends to decrease with increasing wavelength, and thus increase with frequency. This is called normal dispersion, in contrast to anomalous dispersion, where the refractive index increases with wavelength. For visible light normal dispersion means that the refractive index is higher for blue light than for red. For optics in the visual range the amount of dispersion of a lens material is often quantified by the Abbe number formula_6. For a more accurate description of the wavelength dependence of the refractive index the Sellmeier equation can be used. It is an empirical formula that works well in describing dispersion. "Sellmeier coefficients" are often quoted instead of the refractive index in tables. Because of dispersion, it is usually important to specify the vacuum wavelength at which a refractive index is measured. Typically, this is done at various well-defined spectral emission lines; for example, "n"D is the refractive index at the Fraunhofer "D" line, the centre of the yellow sodium double emission at 589.29 nm wavelength. Complex index of refraction and absorption. When light passes through a medium, some part of it will always be absorbed. This can be conveniently taken into account by defining a complex index of refraction, Here, the real part of the refractive index formula_8 indicates the phase speed, while the imaginary part formula_9 indicates the amount of absorption loss when the electromagnetic wave propagates through the material. That formula_9 corresponds to absorption can be seen by inserting this refractive index into the expression for electric field of a plane electromagnetic wave traveling in the formula_11-direction. We can do this by relating the wave number to the refractive index through formula_12, with formula_4 being the vacuum wavelength. With complex wave number formula_14 and refractive index formula_15 this can be inserted into the plane wave expression as Here we see that formula_9 gives an exponential decay, as expected from the Beer–Lambert law. Since intensity is proportional to the square of the electric field, the absorption coefficient becomes formula_18. "κ" is often called the extinction coefficient in physics although this has a different definition within chemistry. Both "n" and "κ" are dependent on the frequency. In most circumstances formula_19 (light is absorbed) or formula_20 (light travels forever without loss). In special situations, especially in the gain medium of lasers, it is also possible that formula_21, corresponding to an amplification of the light. An alternative convention uses formula_22 instead of formula_23, but where formula_19 still corresponds to loss. Therefore these two conventions are inconsistent and should not be confused. The difference is related to defining sinusoidal time dependence as formula_25 versus formula_26. See Mathematical descriptions of opacity. Dielectric loss and non-zero DC conductivity in materials cause absorption. Good dielectric materials such as glass have extremely low DC conductivity, and at low frequencies the dielectric loss is also negligible, resulting in almost no absorption (κ ≈ 0). However, at higher frequencies (such as visible light), dielectric loss may increase absorption significantly, reducing the material's transparency to these frequencies. The real and imaginary parts of the complex refractive index are related through the Kramers–Kronig relations. For example, one can determine a material's full complex refractive index as a function of wavelength from an absorption spectrum of the material. For X-ray and extreme ultraviolet radiation the complex refractive index deviates only slightly from unity and usually has a real part smaller than 1. It is therefore normally written as formula_27 (or formula_28). Relations to other quantities. Phase speed. The refractive index is the ratio of the speed of light in vacuum and the phase speed of light in a material, The phase speed is defined as the rate at which the crests of the waveform propagate; that is, the rate at which the phase of the waveform is moving. The "group speed" is the rate at which the "envelope" of the waveform is propagating; that is, the rate of variation of the amplitude of the waveform. Provided the waveform is not distorted significantly during propagation, it is the group speed that represents the rate at which information (and energy) may be transmitted by the wave (for example, the speed at which a pulse of light travels down an optical fiber). For the analytic properties constraining the unequal phase and group speeds in dispersive media, refer to dispersion (optics). Refraction. When light moves from one medium to another as in the figure to the right, it changes direction, i.e. it is refracted. If it goes from a medium with refractive index formula_30 to one with refractive index formula_31, with an incidence angle to the surface normal of formula_32, the transmission angle formula_33 can be calculated from Snell's law: If there is no angle formula_33 fulfilling Snell's law, i.e., the light cannot be transmitted and will instead undergo total internal reflection. Reflectivity. Apart from the transmitted light there is also a reflected part. The reflection angle is equal to the incidence angle, and the amount of light that is reflected is determined by the reflectivity of the surface. The reflectivity can be calculated from the refractive index and the incidence angle with the Fresnel equations, which for normal incidence reduces to For common glass in air, formula_38 and formula_39, and thus about 4% of the incident power is reflected. At other incidence angles the reflectivity will also depend on the polarization of the incoming light. At a certain angle called Brewster's angle, p-polarized light (light with the electric field in the plane of incidence) will be totally transmitted. Brewster's angle can be calculated from the two refractive indices of the interface as Lenses. The focal length of a lens is determined by its refractive index formula_8 and the radii of curvature formula_42 and formula_43 of its surfaces. The power of a thin lens in air is given by the Lensmaker's formula: Dielectric constant. The refractive index of electromagnetic radiation equals where formula_46 is the material's relative permittivity, and "μr" is its relative permeability. For most naturally occurring materials, "μr" is very close to 1 at optical frequencies, therefore "n" is approximately formula_47. The frequency dependent dielectric constant is simply the square of the (complex) refractive index in a non-magnetic medium (one with a relative permeability of unity). The refractive index is used for optics in Fresnel equations and Snell's law; while the dielectric constant is used in Maxwell's equations and electronics. Where formula_48 is the complex dielectric constant with real and imaginary parts formula_49 and formula_50, and formula_8 and formula_52 are the real and imaginary parts of the refractive index, all functions of frequency: Conversion between refractive index and dielectric constant is done by: Density. In general, the refractive index of a glass increases with its density. However, there does not exist an overall linear relation between the refractive index and the density for all silicate and borosilicate glasses. A relatively high refractive index and low density can be obtained with glasses containing light metal oxides such as Li2O and MgO, while the opposite trend is observed with glasses containing PbO and BaO as seen in the diagram at the right. Many oils (such as olive oil) and ethyl alcohol are examples of liquids which are more refractive, but less dense, than water, contrary to the general correlation between density and refractive index. Group index. Sometimes, a "group speed refractive index", usually called the "group index" is defined: where "vg" is the group velocity. This value should not be confused with "n", which is always defined with respect to the phase velocity. When the dispersion is small, the group velocity can be linked to the phase velocity by the relation In this case the group index can thus be written in terms of the wavelength dependence of the refractive index as where formula_61 is the wavelength in the medium. When the refractive index of a medium is known as a function of the vacuum wavelength (instead of the wavelength in the medium), the corresponding expressions for the group velocity and index are (for all values of dispersion) where formula_4 is the wavelength in vacuum. Momentum (Abraham–Minkowski controversy). In 1908, Hermann Minkowski calculated the momentum of a refracted ray, "p", where "E" is energy of the photon, "c" is the speed of light in vacuum and "n" is the refractive index of the medium as follows: In 1909, Max Abraham proposed the following formula for this calculation: A 2010 study suggested that "both" equations are correct, with the Abraham version being the kinetic momentum and the Minkowski version being the canonical momentum, and claims to explain the contradicting experimental results using this interpretation. Other relations. As shown in the Fizeau experiment, when light is transmitted through a moving medium, its speed relative to a stationary observer is: The refractive index of a substance can be related to its polarizability with the Lorentz–Lorenz equation or to the molar refractivities of its constituents by the Gladstone–Dale relation. Refractivity. In atmospheric applications, the refractivity is defined as "N" = ("n" – 1). The 106 factor is chosen because for air, "n" deviates from unity at most a few parts per ten thousand. Nonscalar, nonlinear, or nonhomogeneous refraction. So far, we have assumed that refraction is given by linear equations involving a spatially constant, scalar refractive index. These assumptions can break down in different ways, to be described in the following subsections. Birefringence. In some materials the refractive index depends on the polarization and propagation direction of the light. This is called birefringence or optical anisotropy. In the simplest form, uniaxial birefringence, there is only one special direction in the material. This axis is known as the optical axis of the material. Light with linear polarization perpendicular to this axis will experience an "ordinary" refractive index formula_68 while light polarized in parallel will experience an "extraordinary" refractive index formula_69. The birefringence of the material is the difference between these indices of refraction, formula_70. Light propagating in the direction of the optical axis will not be affected by the birefringence since the refractive index will be formula_68 independent of polarization. For other propagation directions the light will split into two linearly polarized beams. For light traveling perpendicularly to the optical axis the beams will have the same direction. This can be used to change the polarization direction of linearly polarized light or to convert between linear, circular and elliptical polarizations with waveplates. Many crystals are naturally birefringent, but isotropic materials such as plastics and glass can also often be made birefringent by introducing a preferred direction through, e.g., an external force or electric field. This can be utilized in the determination of stresses in structures using photoelasticity. The birefringent material is then placed between crossed polarizers. A change in birefringence will alter the polarization and thereby the fraction of light that is transmitted through the second polarizer. In the more general case of trirefringent materials described by the field of crystal optics, the "dielectric constant" is a rank-2 tensor (a 3 by 3 matrix). In this case the propagation of light cannot simply be described by refractive indices except for polarizations along principal axes. In materials symultaneously lacking time-reversal and spatial inversion symmetry (for example multiferroics), refractive indices can be different even for counter-propagating light beams, which effect is termed as directional birefringence. Nonlinearity. The strong electric field of high intensity light (such as output of a laser) may cause a medium's refractive index to vary as the light passes through it, giving rise to nonlinear optics. If the index varies quadratically with the field (linearly with the intensity), it is called the optical Kerr effect and causes phenomena such as self-focusing and self-phase modulation. If the index varies linearly with the field (which is only possible in materials that do not possess inversion symmetry), it is known as the Pockels effect. Inhomogeneity. If the refractive index of a medium is not constant, but varies gradually with position, the material is known as a gradient-index medium and is described by gradient index optics. Light traveling through such a medium can be bent or focused, and this effect can be exploited to produce lenses, some optical fibers and other devices. Some common mirages are caused by a spatially varying refractive index of air. Refractive index measurement. Homogeneous media. The refractive index of liquids or solids can be measured with refractometers. They typically measure some angle of refraction or the critical angle for total internal reflection. The first laboratory refractometers sold commercially were developed by Ernst Abbe in the late 19th century. The same principles are still used today. In this instrument a thin layer of the liquid to be measured is placed between two prisms. Light is shone through the liquid at incidence angles all the way up to 90°, i.e., light rays parallel to the surface. The second prism should have an index of refraction higher than that of the liquid, so that light only enters the prism at angles smaller than the critical angle for total reflection. This angle can then be measured either by looking through a telescope, or with a digital photodetector placed in the focal plane of a lens. The refractive index formula_8 of the liquid can then be calculated from the maximum transmission angle formula_73 as formula_74, where formula_75 is the refractive index of the prism. This type of devices are commonly used in chemical laboratories for identification of substances and for quality control. Handheld variants are used in agriculture by, e.g., wine makers to determine sugar content in grape juice, and inline process refractometers are used in, e.g., chemical and pharmaceutical industry for process control. In gemology a different type of refractometer is used to measure index of refraction and birefringence of gemstones. The gem is placed on a high refractive index prism and illuminated from below. A high refractive index contact liquid is used to achieve optical contact between the gem and the prism. At small incidence angles most of the light will be transmitted into the gem, but at high angles total internal reflection will occur in the prism. The critical angle is normally measured by looking through a telescope. Refractive index variations. To measure the spatial variation of refractive index in a sample phase-contrast imaging methods are used. These methods measure the variations in phase of the light wave exiting the sample. The phase is proportional to the optical path length the light ray has traversed, and thus gives a measure of the integral of the refractive index along the ray path. The phase cannot be measured directly at optical or higher frequencies, and therefore needs to be converted into intensity by interference with a reference beam. In the visual spectrum this is done using Zernike phase-contrast microscopy, differential interference contrast microscopy (DIC) or interferometry. Zernike phase-contrast microscopy introduces a phase shift to the low spatial frequency components of the image with a phase-shifting annulus in the Fourier plane of the sample, so that higher frequency parts of the image can interfere with the low frequency reference beam. In DIC the illumination is split up into two beams that are given different polarizations, are phase shifted differently, and are shifted transversely with slightly different amounts. After the specimen the two parts are made to interfere giving an image of the derivative of the optical path length in the direction of the difference in transverse shift. In interferometry the illumination is split up into two beams by a partially reflective mirror. One of the beams is let through the sample before they are combined to interfere and give a direct image of the phase shifts. If the optical path length variations are more than a wavelength the image will contain fringes. There exist several x-ray phase-contrast imaging techniques to determine 2D or 3D spatial distribution of refractive index of samples in the x-ray regime. Applications. The refractive index of a material is the most important property of any optical system that uses refraction. It is used to calculate the focusing power of lenses, and the dispersive power of prisms. It can also be used as a useful tool to differentiate between different types of gemstone, due to the unique chatoyance each individual stone displays. Since refractive index is a fundamental physical property of a substance, it is often used to identify a particular substance, confirm its purity, or measure its concentration. Refractive index is used to measure solids (glasses and gemstones), liquids, and gases. Most commonly it is used to measure the concentration of a solute in an aqueous solution. A refractometer is the instrument used to measure refractive index. For a solution of sugar, the refractive index can be used to determine the sugar content (see Brix). In GPS, the index of refraction is utilized in ray-tracing to account for the radio propagation delay due to the Earth's electrically neutral atmosphere. It is also used in Satellite link design for the Computation of radiowave attenuation in the atmosphere [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@15ab6789 [docID]25880 [hitPos]8 [correct]false [extraScores][F@43e91ee2 , [answer]A gimbal mounted mechanical gyroscope remains pointing in the same direction after spinning up, and thus can be used as a rotational reference for an inertial navigation system. With the development of so-called laser gyroscopes and fiber optic gyroscopes based on the Sagnac effect, the bulky mechanical gyroscope is replaced by one having no moving parts in many modern inertial navigation systems. The principles behind the two devices are different, however. A conventional gyroscope relies on the principle of conservation of angular momentum whereas the sensitivity of the ring interferometer to rotation arises from the invariance of the speed of light for all inertial frames of reference. Description and operation. Typically 3 or more mirrors are used, so that counter-propagating light beams follow a closed path such as a triangle or square.(Fig. 1) Alternatively fiber optics can be employed to guide the light through a closed path.(Fig. 2) If the platform on which the ring interferometer is mounted is rotating, the interference fringes are displaced compared to their position when the platform is not rotating. The amount of displacement is proportional to the angular velocity of the rotating platform. The axis of rotation does not have to be inside the enclosed area. The Sagnac effect in a circular loop can be understood as follows. When the loop is rotating, the point of entry/exit moves during the transit time of the light. The backwards-propagating beam covers less distance than the forwards-propagating beam and arrives earlier.(Fig. 3) This creates a shift in the interference pattern. The shift of the interference fringes is thereby proportional to the platform's angular velocity. The rotation thus measured is an absolute rotation, that is, the platform's rotation with respect to an inertial reference frame. History. Early suggestions to build a giant ring interferometer to measure the rotation of the Earth were made by Oliver Lodge in 1897, and then by Albert Abraham Michelson in 1904. They hoped that with such an interferometer, it would be possible to decide between the idea of a stationary aether, and an aether which is completely dragged by the Earth. That is, if the hypothetical aether were carried along by the Earth (or by the interferometer) the result would be negative, while a stationary aether would give a positive result. Max von Laue in 1911 continued the theoretical work of Michelson, and also incorporated special relativity in his calculations. He predicted a positive result (to first order in v/c) for both special relativity and for the stationary aether, because in those theories the speed of light is independent of the velocity of the source, and thus the propagation time for the counter-propagating rays is not the same when viewed from inertial frames of reference; only complete-aether-drag models would give a negative result. While Laue confined his investigations on inertial frames, Paul Langevin (1921/35) and others described the effect when viewed from rotating reference frames (in both special and general relativity, see Born coordinates). In practice, the first interferometry experiment aimed at observing the correlation of angular velocity and phase-shift was performed by the French scientist Georges Sagnac in 1913. Its purpose was to detect "the effect of the relative motion of the ether". Sagnac believed that his results constituted proof of the existence of a stationary aether. However, as explained above, two years earlier, Max von Laue already shown that this effect is consistent with special relativity. An experiment conducted in 1911 by Franz Harress, aimed at making measurements of the Fresnel drag of light propagating through moving glass, was in 1920 recognized by Laue as actually constituting a Sagnac experiment. Not aware of the Sagnac effect, Harress had realized the presence of an "unexpected bias" in his measurements, but was unable to explain its cause. In 1926, an ambitious ring interferometry experiment was set up by Albert Michelson and Henry Gale. The aim was to find out whether the rotation of the Earth has an effect on the propagation of light in the vicinity of the Earth. The Michelson–Gale–Pearson experiment was a very large ring interferometer, (a perimeter of 1.9 kilometer), large enough to detect the angular velocity of the Earth. The outcome of the experiment was that the angular velocity of the Earth as measured by astronomy was confirmed to within measuring accuracy. The ring interferometer of the Michelson-Gale experiment was not calibrated by comparison with an outside reference (which was not possible, because the setup was fixed to the Earth). From its design it could be deduced where the central interference fringe ought to be if there would be zero shift. The measured shift was 230 parts in 1000, with an accuracy of 5 parts in 1000. The predicted shift was 237 parts in 1000. Theory. The shift in interference fringes in a ring interferometer can be viewed simply as a consequence of the different "distances" that light travels due to the rotation of the ring.(Fig. 3) The simplest derivation is for a circular ring rotating at an angular velocity of formula_1, but the result is general for loop geometries with other shapes. If a light source emits in both directions from one point on the rotating ring, light traveling in the same direction as the rotation direction needs to travel more than one circumference around the ring before it catches up with the light source from behind. The time formula_2 that it takes to catch up with the light source is given by: formula_4 is the distance (black bold arrow in Fig. 3) that the mirror has moved in that same time: Eliminating formula_4 from the two equations above we get: Likewise, the light traveling in the opposite direction of the rotation will travel less than one circumference before hitting the light source on the front side. So the time for this direction of light to reach the moving source again is: The time difference is For formula_10, this reduces to where "A" is the area of the ring. Although this simple derivation is for a circular ring, the result is in fact general for any shape of rotating loop with area "A".(Fig. 4) We imagine a screen for viewing fringes placed at the light source (or we use a beamsplitter to send light from the source point to the screen). Given a steady light source, interference fringes will form on the screen with a fringe displacement proportional to the time differences required for the two counter-rotating beams to traverse the circuit. The phase shift is formula_12, which causes fringes to shift in proportion to formula_13 and formula_14 . At non-relativistic speeds, the Sagnac effect is a simple consequence of the source independence of the speed of light. In other words, the Sagnac experiment does not distinguish between pre-relativistic physics and relativistic physics. When light propagates in fibre optic cable, the setup is effectively a combination of a Sagnac experiment and the Fizeau experiment. In glass the speed of light is slower than in vacuum, and the optical cable is the moving medium. In that case the relativistic velocity addition rule applies. Pre-relativistic theories of light propagation cannot account for the Fizeau effect. (By 1900 Lorentz could account for the Fizeau effect, but by that time his theory had evolved to a form where in effect it was mathematically equivalent to special relativity.) Since emitter and detector are traveling at the same speeds, Doppler effects cancel out, so the Sagnac effect does not involve the Doppler effect. In the case of ring laser interferometry, it is important to be aware of this. When the ring laser setup is rotating, the counterpropagating beams undergo frequency shifts in opposite directions. This frequency shift is not a Doppler shift, but is rather an optical cavity resonance effect, as explained below in Ring lasers. The Sagnac effect has stimulated a century long debate on its meaning and interpretation, much of this debate being surprising since the effect is perfectly well understood in the context of special relativity. An essential point that has not been well-understood until recent years, is that rotation is not required for the Sagnac effect to be manifest. What matters is that light moves along a closed circuit, and that an observer is in motion with respect to that circuit. In Fig. 5, the measured phase difference in both a standard fibre optic gyroscope, shown on the left, and a modified fibre optic conveyor, shown on the right, conform to the equation "Δt = 2vL/c2", whose derivation is based on the constant speed of light. It is evident from this formula that the total time delay is equal to the cumulative time delays along the entire length of fibre, regardless whether the fibre is in a rotating section of the conveyor, or a straight section. In addition, it is evident that that there is no connection between the total delay and the area enclosed by the light path. The equation commonly seen in the analysis of a "rotating", circular Sagnac interferometer, "Δt = 4Aω/c2", can be derived from the more general formula by a simple substitution of terms: Let "v = rω, L = 2πr." Then "Δt = 2vL/c2 = 4πr2ω/c2 = 4Aω/c2". Other generalizations. A relay of pulses that circumnavigates the Earth, verifying precise synchronization, is also recognized as a case requiring correction for the Sagnac effect. In 1984 a verification was set up that involved three ground stations and several GPS satellites, with relays of signals both going eastward and westward around the world. In the case of a Sagnac interferometer a measure of difference in arrival time is obtained by producing interference fringes, and observing the fringe shift. In the case of a relay of pulses around the world the difference in arrival time is obtained directly from the actual arrival time of the pulses. In both cases the mechanism of the difference in arrival time is the same: the Sagnac effect. The Hafele-Keating experiment is also recognized as a counterpart to Sagnac effect physics. In the actual Hafele-Keating experiment the mode of transport (long-distance flights) gave rise to time dilation effects of its own, and calculations were needed to separate the various contributions. For the (theoretical) case of clocks that are transported so slowly that time dilation effects arising from the transport are negligible the amount of time difference between the clocks when they arrive back at the starting point will be equal to the time difference that is found for a relay of pulses that travels around the world: 207 nanoseconds. Reference frames. The Sagnac effect is not an artifact of the choice of reference frame. It is independent of the choice of reference frame, as is shown by a calculation that invokes the metric tensor for an observer at the axis of rotation of the ring interferometer and rotating with it yielding the same outcome. If one starts with the Minkowski metric and does the coordinate conversions formula_15 and formula_16 (see Born coordinates), the line element of the resultant metric is where Under this metric, the speed of light tangent to the ring is formula_23 depending on whether the light is moving against or with the rotation of the ring. Note that only the case of formula_24 is inertial. For formula_25 this frame of reference is non-inertial, which is why the speed of light at positions distant from the observer (at formula_26) can vary from formula_27. Practical uses. The Sagnac effect is employed in current technology. One use is in inertial guidance systems. Ring laser gyroscopes are extremely sensitive to rotations, which need to be accounted for if an inertial guidance system is to return accurate results. The ring laser also can detect the sidereal day, which can also be termed "mode 1". Global navigation systems, such as GPS, GLONASS, COMPASS or Galileo, need to take the rotation of the Earth into account in the procedures of using radio signals to synchronize clocks. Ring lasers. Fibre optic gyroscopes are sometimes referred to as 'passive ring interferometers'. A passive ring interferometer uses light entering the setup from outside. The interference pattern that is obtained is a fringe pattern, and what is measured is a phase shift. It is also possible to construct a ring interferometer that is self-contained, based on a completely different arrangement. This is called a ring laser or ring laser gyroscope. The light is generated and sustained by incorporating laser excitation in the path of the light. To understand what happens in a ring laser cavity, it is helpful to discuss the physics of the laser process in a laser setup with continuous generation of light. As the laser excitation is started, the molecules inside the cavity emit photons, but since the molecules have a thermal velocity, the light inside the laser cavity is at first a range of frequencies, corresponding to the statistical distribution of velocities. The process of stimulated emission makes one frequency quickly outcompete other frequencies, and after that the light is very close to monochromatic. For the sake of simplicity, assume that all emitted photons are emitted in a direction parallel to the ring. Fig. 7 illustrates the effect of the ring laser's rotation. In a linear laser, an integer multiple of the wavelength fits the length of the laser cavity. This means that in traveling back and forth the laser light goes through an integer number of "cycles" of its frequency. In the case of a ring laser the same applies: the number of cycles of the laser light's frequency is the same in both directions. This quality of the same number of cycles in both directions is preserved when the ring laser setup is rotating. The image illustrates that there is wavelength shift (hence a frequency shift) in such a way that the number of cycles is the same in both directions of propagation. By bringing the two frequencies of laser light to interference a beat frequency can be obtained; the beat frequency is the difference between the two frequencies. This beat frequency can be thought of as an interference pattern in time. (The more familiar interference fringes of interferometry are a spatial pattern). The period of this beat frequency is linearly proportional to the angular velocity of the ring laser with respect to inertial space. This is the principle of the ring laser gyroscope, widely used in modern inertial navigation systems. Zero point calibration. In passive ring interferometers, the fringe displacement is proportional to the first derivative of angular position; careful calibration is required to determine the fringe displacement that corresponds to zero angular velocity of the ring interferometer setup. On the other hand, ring laser interferometers do not require calibration to determine the output that corresponds to zero angular velocity. Ring laser interferometers are self-calibrating. The beat frequency will be zero if and only if the ring laser setup is non-rotating with respect to inertial space. Fig. 8 illustrates the physical property that makes the ring laser interferometer self-calibrating. The grey dots represent molecules in the laser cavity that act as resonators. Along every section of the ring cavity, the speed of light is the same in both directions. When the ring laser device is rotating, then it rotates with respect to that background. In other words: invariance of the speed of light provides the reference for the self-calibrating property of the ring laser interferometer. Lock-in. Ring laser gyroscopes suffer from an effect known as "lock-in" at low rotation rates (less than 100°/h). At very low rotation rates, the frequencies of the counter-propagating laser modes become almost identical. In this case, crosstalk between the counter-propagating beams can result in injection locking, so that the standing wave "gets stuck" in a preferred phase, locking the frequency of each beam to each other rather than responding to gradual rotation. By rotationally dithering the laser cavity back and forth through a small angle at a rapid rate (hundreds of hertz), lock-in will only occur during the brief instances where the rotational velocity is close to zero; the errors thereby induced approximately cancel each other between alternating dead periods. Fibre optic gyroscopes "versus" ring laser gyroscopes. Fibre optic gyros (FOGs) and ring laser gyros (RLGs) both operate by monitoring the difference in propagation time between beams of light traveling in clockwise and counterclockwise directions about a closed optical path. They differ considerably in various cost, reliability, size, weight, power, and other performance characteristics that need to be considered when evaluating these distinct technologies for a particular application. RLGs are relatively high cost, bulky, and heavy devices that require accurate machining, use of precision mirrors, and assembly under clean room conditions. Their mechanical dithering assemblies add to their weight. Their gas-filled laser tubes degrade relatively rapidly. Although large RLGs are capable of logging in excess of 10,000 hours of operation, miniaturized versions are limited to a few hundred hours of use. Their lasers have relatively high power requirements. Interferometric FOGs are purely solid-state, require no mechanical dithering components, do not require precision machining, are not subject to lock-in, do not require clean-room assembly, have a flexible geometry, and can be made very small. They use many standard components from the telecom industry. These factors result in a much lower cost than RLGs. In addition, the major optical components of FOGs have proven performance in the telecom industry, with lifespans measured in decades. Analog FOGs offer the lowest possible cost but are limited in performance; digital FOGs offer the wide dynamic ranges and accurate scale factor corrections required for stringent applications. Use of longer and larger coils increases sensitivity at the cost of greater sensitivity to temperature variations and vibrations. Overall, however, the best FOGs do not match the performance characteristics of the best RLGs by several orders of magnitude. Zero-area Sagnac interferometer and gravitational wave detection. The Sagnac topology was actually first described by Michelson in 1886, who employed an even-reflection variant of this interferometer in a repetition of the Fizeau experiment. Michelson noted the extreme stability of the fringes produced by this form of interferometer: White-light fringes were observed immediately upon alignment of the mirrors. In dual-path interferometers, white-light fringes are difficult to obtain since the two path lengths must be matched to within a couple of micrometers (the coherence length of the white light). However, being a common path interferometer, the Sagnac configuration inherently matches the two path lengths. Likewise Michelson observed that the fringe pattern would remain stable even while holding a lighted match below the optical path; in most interferometers the fringes would shift wildly due to the refractive index fluctuations from the warm air above the match. Sagnac interferometers are almost completely insensitive to displacements of the mirrors or beam-splitter. This characteristic of the Sagnac topology has led to their use in applications requiring exceptionally high stability. The fringe shift in a Sagnac interferometer due to rotation has a magnitude proportional to the enclosed area of the light path, and this area must be specified in relation to the axis of rotation. Thus the sign of the area of a loop is reversed when the loop is wound in the opposite direction (clockwise or anti-clockwise). A light path that includes loops in both directions, therefore, has a net area given by the difference between the areas of the clockwise and anti-clockwise loops. The special case of two equal but opposite loops is called a "Zero-area" Sagnac interferometer. The result is an interferometer that exhibits the stability of the Sagnac topology while being insensitive to rotation. The Laser Interferometer Gravitational-Wave Observatory (LIGO) consisted of two 4-km Michelson-Fabry-Pérot interferometers, and operated at a power level of about 100 watts of laser power at the beam splitter. A currently ongoing upgrade to Advanced LIGO will require several kilowatts of laser power, and scientists will need to contend with thermal distortion, frequency variation of the lasers, mirror displacement and thermally induced birefringence. A variety of competing optical systems are being explored for third generation enhancements beyond Advanced LIGO. One of these competing proposals is based on the zero-area Sagnac design. With a light path consisting of two loops of the same area, but in opposite directions, an effective area of zero is obtained thus canceling the Sagnac effect in its usual sense. Although insensitive to low frequency mirror drift, laser frequency variation, reflectivity imbalance between the arms, and thermally induced birefringence, this configuration is nevertheless sensitive to passing gravitational waves at frequencies of astronomical interest. However, many considerations are involved in the choice of an optical system, and despite the zero-area Sagnac's superiority in certain areas, there is as yet no consensus choice of optical system for third generation LIGO [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@58777339 [docID]1636296 [hitPos]8 [correct]false [extraScores][F@58c98294 , [answer]A red light camera is a traffic enforcement camera that captures an image of a vehicle which has entered an intersection against a red traffic light. By automatically photographing vehicles that run red lights, the camera produces evidence that assists authorities in their enforcement of traffic laws. Generally the camera is triggered when a vehicle enters the intersection after the traffic light has turned red. Typically, a law enforcement official will review the photographic evidence and determine whether a violation occurred. A citation is then usually mailed to the owner of the vehicle found to be in violation of the law. These cameras are used worldwide, in countries including: Australia, New Zealand, Canada, the United Kingdom, Singapore and the United States. If a proper identification cannot be made in lieu of a ticket, some police departments send out a notice of violation to the owner of the vehicle, requesting identifying information so that a ticket may later be issued. There is debate and ongoing research about the use of red light cameras. Authorities cite public safety as the primary reason that the cameras are installed, while opponents contend their use is more for financial gain than for safety. There have been concerns that red light cameras scare drivers into more sudden stops, perhaps on a yellow light, which is likely to increase rear-end collisions, while some stop lights allow a red-light grace period of a few seconds before the cross-direction turns green. However, the increased incentive to stop could reduce right-angle crashes during turns. Some studies have confirmed many more rear-end collisions where red light cameras have been used, while right-angle crashes decreased, but the overall collision rate has been mixed as to whether the total crashes were higher when using red light cameras. In some areas, the length of the yellow light has been increased to provide a longer warning with a red light camera. There is also concern that the international standard formula used for setting the length of yellow lights opposes the laws of physics, that opposition causing millions of drivers every day to inadvertently run red lights and a fraction of those to crash. History. Red light cameras were first developed in the Netherlands by Gatso. Worldwide, red light cameras have been in use since the 1960s, and were used for traffic enforcement in Israel as early as 1969. The first red light camera system was introduced in 1965, using tubes stretched across the road to detect the violation and subsequently trigger the camera. One of the first developers of these red light camera systems was Gatsometer BV. The cameras first received serious attention in the United States in the 1980s following a highly publicized crash in 1982, involving a red-light runner who collided with an 18-month-old girl in a stroller (or "push-chair") in New York City. Subsequently, a community group worked with the city's Department of Transportation to research automated law-enforcement systems to identify and ticket drivers who run red lights. New York's red-light camera program went into effect in 1993. From the 1980s onward, red light camera usage expanded worldwide, and one of the early camera system developers, Poltech International, supplied Australia, Britain, South Africa, Taiwan, the Netherlands and Hong Kong. American Traffic Systems (subsequently American Traffic Solutions) (ATS) and Redflex Traffic Systems emerged as the primary suppliers of red light camera systems in the US, while Jenoptik became the leading provider of red light cameras worldwide. Initially, all red light camera systems used film, which was delivered to local law enforcement departments for review and approval. The first digital camera system was introduced in Canberra in December 2000, and digital cameras have increasingly replaced the older film cameras in other locations since then. Operation. Red light cameras are typically installed in protective metal boxes attached to poles at intersections, which are often specifically chosen due to high numbers of crashes and/or red-light-running violations. Red light camera systems typically employ two closely spaced inductive loops embedded in the pavement just before the limit line, to measure the speed of vehicles. Using the speed measured, the system predicts if a particular vehicle will not be able to stop before entering the intersection, and takes two photographs of the event. The first photo shows the vehicle just before it enters the intersection, with the light showing red, and the second photo, taken a second or two later, shows the vehicle when it is in the intersection. Details that may be recorded by the camera system (and later presented to the vehicle owner) include: the date and time, the location, the vehicle speed, and the amount of time elapsed since the light turned red and the vehicle passed into the intersection. The event is captured as a series of photographs or a video clip, or both, depending on the technology used, which shows the vehicle before it enters the intersection on a red light signal and its progress through the intersection. The data and images, whether digital or developed from film, are sent to the relevant law enforcement agency. There, the information is typically reviewed by a law enforcement official or police department clerk, who determines if a violation occurred and, if so, approves issuing a citation to the vehicle owner, who may challenge the citation. Studies have shown that 38% of violations occur within 0.25 seconds of the light turning red and 79% within one second. A few red light camera systems allow a "grace period" of up to half a second for drivers who pass through the intersection just as the light turns red. Ohio and Georgia introduced a statute requiring that one second be added to the standard yellow time of any intersection that has a red light camera, which has led to an 80% reduction in tickets since its introduction. New Jersey has the strictest yellow timing provisions in the country as a result of concerns that cameras would be used to generate revenue; they have a statute specifying that the yellow time for an intersection that has a red light camera must be based on the speed at which 85% of the road's traffic moves rather than be based on the road's actual speed limit. Usage. Red light camera usage is widespread in a number of countries worldwide. Netherlands-based Gatso presented red light cameras to the market in 1965, and red light cameras were used for traffic enforcement in Israel as early as 1969. In the early 1970s, red light cameras were used for traffic enforcement in at least one jurisdiction in Europe. Australia began to use them on a wide scale in the 1980s. , expansion of red light camera usage in Australia is ongoing. In some areas of Australia, where the red light cameras are used, there is an online system to check the photograph taken of your vehicle if you receive a ticket. Singapore also began use of red light cameras in the 1980s, and installed the first camera systems during five years, starting in August 1986. In Canada, by 1998, red light cameras were in use in British Columbia and due to be implemented in Manitoba. In Alberta, red light cameras were installed in 1999 in Edmonton and in 2001 in Calgary. The UK first installed cameras in the 1990s, with the earliest locations including eight rail crossings in Scotland where there was greatest demand for enforcement of traffic signals due to fatalities. China. Red light camera usage is extensive in mainland China. , there were 816 red light cameras on 599 roads in Shenzhen. Hong Kong. In Hong Kong, where red light cameras are installed, signs are erected to warn drivers that cameras are present, with the aim of educating drivers to stop for signals. The number of red light cameras in Hong Kong doubled in May 2004, and digital red light cameras were introduced at intersections identified by the police and transport department as having the most violations and greatest risk. The digital cameras were introduced to further deter red-light running. As added assistance to drivers, some of the camera posts were painted orange so that drivers could see them more easily. By 2006, Hong Kong had 96 red light cameras in operation. United Kingdom. In the United Kingdom the authorities often refer to red-light cameras, along with speed cameras, as "safety cameras". They were first used in the early 1990s, with initial deployment by the Department for Environment, Transport and the Regions. All costs were paid by the local authority in which the individual camera was placed, and revenues accrued from fines were paid to the Treasury Consolidated Fund. In 1998 the government handed the powers of collection to local road-safety partnerships, comprising "... local authorities, Magistrates' Courts, the Highways Agency and the police." In a report, published in December 2005, there were a total of 612 red light cameras in England alone, of which 225 were in London. United States. Since the early 1990s, red light cameras have been used in the United States in 26 U.S. states and the District of Columbia. Within some states, the cameras may only be permitted in certain areas. For example, in New York State, the Vehicle and Traffic Law permits red light cameras only within cities with a population above 1 million (i.e. New York City), Rochester, Buffalo, Yonkers, and Nassau and Suffolk Counties. In Florida, a state law went into effect on 1 July 2010, which allows all municipalities in the state to use red light cameras on all state-owned right-of-ways and fine drivers who run red lights, with the aim of enforcing safe driving, according to then-Governor Charlie Crist. The name given to the state law is the Mark Wandall Traffic Safety Act, named for a man who was killed in 2003 by a motorist who ran a red light. In addition to allowing the use of cameras, the law also standardizes driver fines. Major cities throughout the US that use red light cameras include Atlanta, Austin, Baltimore, Baton Rouge, Chicago, Dallas, Denver, Memphis, Miami, New Orleans, New York City, Newark, Philadelphia, Phoenix, Raleigh, San Francisco, Seattle, Toledo and Washington, D.C. Albuquerque has cameras, but in October 2011 local voters approved a ballot measure advising the city council to cease authorizing the red light camera program. The City of Albuquerque ended its red light program on 31 December 2011. Suppliers of red-light cameras in the US include: Affiliated Computer Services (ACS) State and Local Solutions, a Xerox company, of Dallas, Texas; American Traffic Solutions of Scottsdale, Arizona, 1/3 owned by Goldman Sachs; Brekford International Corp., of Hanover, Maryland; CMA Consulting Services, Inc. of Latham, New York; Gatso USA of Beverly, Massachusetts; iTraffic Safety LLC of Ridgeland, South Carolina; Optotraffic, of Lanham, Maryland; Redflex Traffic Systems of Phoenix, Arizona, with its parent company in Australia; RedSpeed-Illinois LLC, of Lombard, Illinois, whose parent company is in Worcestershire, England; SafeSpeed LLC, of Chicago, Illinois, and SENSYS America Inc., of Miami, Florida. Some states have chosen to prohibit the use of red light cameras. These include Arkansas, Maine, Michigan, Mississippi, Montana, Nevada, New Hampshire and West Virginia. In February 2012, the red light camera ordinance in the city of St. Louis was officially declared void by St. Louis Circuit Court Judge Mark Neill. On 9 August 2012, the Cary, North Carolina town council voted to end their program. In February 2013, the San Diego mayor helped remove a red light camera to keep the campaign promise he made during the November 2012 election to eliminate these systems. In the United States, fines are not standardized and vary to a great degree, from $50 in New York City to approximately $500 in California. The cost in California can increase to approximately $600 if the motorist elects to attend traffic school in order to avoid having a demerit point added to his or her driving record. Notice of Traffic Violation. In many California police departments, when a positive identification cannot be made, the registered owner of the vehicle will be mailed a "notice of traffic violation" instead of a real ticket. Also known as "snitch tickets," these notices are used to request identifying information about the driver of the vehicle during the alleged violation. Because these notices have not been filed at court, they carry no legal weight and the registered owner is under no obligation to respond. In California, a genuine ticket will bear the name and address of the local branch of the Superior Court and direct the recipient to contact that Court. In contrast, a "notice of traffic violation" generated by the police will omit court information, using statements like "This is not a notice to appear" and "Do not forward this information to the Court."2013 In Chicago, Illinois, violation videos are uploaded to the City of Chicago's website at cityofchicago.org. A notice of violation is also sent to the driver via mail. In the suburbs, a "Red Light Photo Enforced" sign is mounted on a traffic signal mast arm. Also, many communities in the US upload violation videos prior to notification to driver and offer online payment of red light tickets at payonlineticket.com. Privacy complaints include having the number of page views recorded. Commercial complaints are that payment is not made to the community and the community only gets a percentage of the fine. Studies and politics. A report in 2003 by the National Cooperative Highway Research Program (NCHRP) examined studies from the previous 30 years in Australia, the UK, Singapore and the US, and concluded that red light cameras "improve the overall safety of intersections where they are used." While the report states that evidence is not conclusive (partly due to flaws in the studies), the majority of studies show a reduction in angle crashes, a smaller increase in rear-end crashes, with some evidence of a “spillover” effect of reduced red light running to other intersections within a jurisdiction. These findings are similar to a 2005 meta analysis, which compared the results of 10 controlled before-after studies of red light cameras in the US, Australia and Singapore. The analysis stated that the studies showed a reduction in crashes (up to almost 30%) in which there were injuries, however, evidence was less conclusive for a reduction in total collisions. Studies of red light cameras worldwide show a reduction of crashes involving injury by about 25% to 30%, taking into account increases in rear-end crashes, according to testimony from a meeting of the Virginia House of Delegates Militia, Police, and Public Safety Committee in 2003. These findings are supported by a review of more than 45 international studies carried out in 2010, which found that red light cameras reduce red light violation rates, crashes resulting from red light running, and usually reduce right-angle collisions. In terms of location-specific studies, in Singapore a study from 2003 found that there was "a substantial drop" in red light violations at intersections with red light cameras. In particular the study found that drivers were encouraged to stop more readily in areas with red light cameras in use. A report from civic administrators in Saskatchewan in 2001, when considering red light camera use, referred to studies in the Netherlands and Australia that found a 40% decrease in red light violations and 32% decrease in right-angle crashes where red light cameras were installed. Following the introduction of red light cameras in Western Australia, the number of serious right-angle crashes decreased by 40%, according to an article from the "Canberra Times". In an article from the Xinhua General News Service, the Hong Kong transport department reported that in 2006 the monthly average number of crashes due to red light violations fell 25% and the number of people injured in these crashes decreased by 30%, following an increase in the number of red light cameras in use. North America. In the U.S. and Canada, a number of studies have examined whether red light cameras produce a safety benefit. A 2005 study by the U.S. Federal Highway Administration (FHWA) suggests red light cameras reduce dangerous right-angle crashes. This study also found there can be an increase in the number of rear-end collisions, leading to the total number of collisions remaining unchanged. This FHWA study has been criticized on grounds that one of its co-directors has performed research for the Insurance Institute for Highway Safety (IIHS), a private corporation representing the auto insurance industry that profits significantly from insurance surcharges on drivers ticketed by red light cameras. The FHWA study has also been criticized as containing critical methodological and analytical flaws and failing to explain an increase in fatalities associated with red light camera use:(…)the authors spotlight the statistical difficulties of including the cost of fatalities, while ignoring the practical implications of such events (…) assuming that each angle injury crash had a societal cost of $64,468, when in fact the cost was $82,816 before camera use and $100,176 after camera use(…) IIHS research on the safety effects of red light cameras has also been criticized as biased and methodologically flawed. Not all studies have been favorable to the use of red light cameras. A 2004 study of 17,271 crashes from North Carolina A & T University showed that the presence of red light cameras "increased" the overall number of crashes by 40%. This research received no peer review and is considered flawed by the IIHS. In 2007, the department issued an updated report which showed that the overall number of crashes at intersections with red light cameras increased. This report concluded that the decision to install red light cameras should be made on an intersection-by-intersection basis as some intersections saw decreases in crashes and injuries that justified the use of red light cameras, while others saw increases in crashes, indicating that the cameras were not suitable in that location. This study, too, is considered flawed by the IIHS. Aurora, Colorado experienced mixed results with red light cameras; after starting camera enforcement at 4 intersections, crashes decreased by 60% at one, increased 100% at two, and increased 175% at the fourth. According to the IIHS, most studies suggest the increase in rear-end collisions decreases once drivers have become accustomed to the new dynamics of the intersection. Some locations experience a decrease in rear-end collisions at intersections with red light cameras over time, for instance, in Los Angeles such collisions fell 4.7% from 2008 to 2009. However, a 2010 analysis by the Los Angeles City Controller found L.A.'s red light cameras hadn't demonstrated an improvement in safety, specifically that of the 32 intersections equipped with cameras, 12 saw more crashes than before the cameras were installed, 4 had the same number, and 16 had fewer crashes; also that factors other than the cameras may have been responsible for the reduced crashes at the 16 intersections. And in Winnipeg, Manitoba, crashes were found to have significantly increased in the years following the deployment of red light cameras. In 2010, Arizona completed a study of their statewide 76 photo enforcement cameras and decided they would not renew the program in 2011; lower revenue than expected, mixed public acceptance and mixed accident data were cited. Nevertheless, the FHWA has concluded that the cameras yielded a positive overall cost benefit due to the reduction in more expensive right-angle injury collisions. Other studies have found a greater crash reduction. For example, a 2005 study of the Raleigh, North Carolina, red light camera program conducted by the Institute for Transportation Research and Education at North Carolina State University found right-angle crashes dropped by 42%, rear-end crashes dropped by 25% and total crashes dropped by 17%. In 2010, the IIHS looked at results of a number of studies and found that red light cameras reduce total collisions and particularly reduce the type of crashes that are especially likely to cause injuries. A 2011 IIHS report concluded that the rate of fatal collisions involving red-light running in cities with a population of 200,000 or greater was 24% lower with cameras than it would have been without cameras. Opinions. United States. In the US, the red light camera industry has invested heavily in efforts to lead public opinion. They have made extensive use of Astroturfing, and have distributed poll results showing heavy public support of red light cameras, often without making a prominent disclosure that those polls were commissioned by themselves or their paid lobbyists. Despite the industry's efforts, as of the November 2011 elections, photo enforcement in the US had been defeated in 22 of 23 election contests. Groups who believe that red light cameras reduce crashes and increase safety have formed lobbying groups such as the "Stop Red Light Running Coalition of Florida," which was created to lobby for a state law in Florida allowing red light cameras to be used. Some of these lobbying groups have ties to the red light camera industry. For example, a board member of the Stop… Florida group is Ron Reagan, who is Treasurer of American Traffic Solutions front group the National Coalition for Safer Roads. Melissa Wandall is the President of the National Coalition for Safer Roads. Ms. Wandall lobbied for The Mark Wandall Traffic Safety Program, which brought red light cameras to Florida. Further, this group is "funded in large measure by the traffic camera industry". There are also various groups and individuals, such as the National Motorists Association, who oppose red light cameras on the grounds that the use of these devices raises legal issues and violates the privacy of citizens. They also argue that the use of red light cameras does not increase safety. In the US, AAA Auto Club South argued against the passage of a Florida state law to allow red light cameras, stating that use of red light cameras was primarily for raising money for the state and local government coffers and would not increase road safety. Canada and Europe. In Norway, Spain and the Netherlands, a postal survey in 2003 showed acceptance of the use of red light cameras for traffic enforcement. For some groups, the enforcement of traffic laws is considered the main reason for using the red light cameras. For example, a report from civic administrators in Canada's Saskatoon in 2001 described the cameras as "simply an enforcement tool used to penalize motorists that fail to stop for red traffic signals." Legal questions and restrictions. United States. The argument that red light cameras violate the privacy of citizens, has also been addressed in the US courts. According to a 2009 ruling by the 7th US Circuit Court of Appeals, “no one has a fundamental right to run a red light or avoid being seen by a camera on a public street.” In addition, cameras only take photographs or video when a vehicle has run a red light and, in most states, the camera does not photograph the driver or the occupants of the vehicle. In most areas, red light enforcement cameras are installed and maintained by private firms. Lawsuits have been raised challenging private companies' rights to hand out citations, such as a December 2008 lawsuit challenging the city of Dallas' red light camera program, which was dismissed in March 2009. In most cases, citations are issued by law enforcement officers using the evidence provided by the companies. There have been many instances where cities in the US have been found to have too-short yellow-light intervals at some intersections where red light cameras have been installed. In Tennessee, 176 drivers were refunded for fines paid after it was discovered that the length of the yellow was too short for that location, and motorists were caught running the light in the first second of the red phase. In California, a combined total of 7603 tickets were refunded or dismissed by the cities of Bakersfield, Costa Mesa, East LA, San Carlos, and Union City, because of too-short yellows. Although national guidelines addressing the length of traffic signals are available, traffic signal phase times are determined by the government employees of the city, county or state for that signalized location. While some states set jurisdiction-wide constant durations for yellow-light intervals, a new standard is taking hold. States are required to adopt the 2009 National Manual on Uniform Traffic Control Devices (MUTCD) as their legal state standard for traffic-control devices since 2011. These standards require engineering practices to be used to set yellow-light-timing durations at individual intersections and or corridors. For guidance to state authorities, MUTCD states yellow lights should have a minimum duration of 3 seconds and a maximum duration of 6 seconds. The deadline for compliance is 2014. In the US, if any part of a driver's vehicle has already passed into the intersection when the signal turns red, a violation is not generated. A ticket is only issued if the vehicle enters the intersection while the light is red. Italy. A mention is deserved of the small municipality of Segrate, Italy, which synchronized two nearby traffic lights such that drivers were coerced to either break the speed limit or pass during the red light. This was investigated as a deliberate fraud to increase the income from tickets. It took months before the machines were eventually dismantled by the Guardia di Finanza. Alternatives. A red light camera is not the only countermeasure against red-light running. Others include increasing the visibility distance and conspicuity of the traffic light so it is more likely to attract the driver's attention in time for him or her to stop, re-timing lights so drivers will encounter fewer red ones, increasing the duration of the yellow light between the green and the red, adding a "clearance" phase to the intersection's traffic signals, during which all directions have a red light. It has been posited that the regulatory minimum yellow duration has been decreased over the years, that this is a cause of the increase in red-light running, and that the latter countermeasures amount to a reversion to earlier, longer regulated yellow-light durations [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@244efc35 [docID]1094579 [hitPos]9 [correct]false [extraScores][F@713a8f92 , [answer]Film speed is the measure of a photographic film's sensitivity to light, determined by sensitometry and measured on various numerical scales, the most recent being the ISO system. A closely related ISO system is used to measure the sensitivity of digital imaging systems. Relatively insensitive film, with a correspondingly lower speed index requires more exposure to light to produce the same image density as a more sensitive film, and is thus commonly termed a "slow film". Highly sensitive films are correspondingly termed "fast films". In both digital and film photography, the reduction of exposure corresponding to use of higher sensitivities generally leads to reduced image quality (via coarser film grain or higher image noise of other types). In short, the higher the sensitivity, the grainier the image will be. Film speed measurement systems. Historical systems. Warnerke. The first known practical sensitometer, which allowed measurements of the speed of photographic materials, was invented by the Polish engineer Leon Warnerke - pseudonym of (1837–1900) - in 1880, among the achievements for which he was awarded the Progress Medal of the Photographic Society of Great Britain in 1882. It was commercialized since 1881. The Warnerke Standard Sensitometer consisted of a frame holding an opaque screen with an array of typically 25 numbered, gradually pigmented squares brought into contact with the photographic plate during a timed test exposure under a phosphorescent tablet excited before by the light of a burning Magnesium ribbon. The speed of the emulsion was then expressed in 'degrees' Warnerke (sometimes seen as Warn. or °W.) corresponding with the last number visible on the exposed plate after development and fixation. Each number represented an increase of 1/3 in speed, typical plate speeds were between 10° and 25° Warnerke at the time. His system saw some success but proved to be unreliable due to its spectral sensitivity to light, the fading intensity of the light emitted by the phosphorescent tablet after its excitation as well as high built-tolerances. The concept, however, was later built upon in 1900 by Henry Chapman Jones (1855–1932) in the development of his plate tester and modified speed system. Hurter & Driffield. Another early practical system for measuring the sensitivity of an emulsion was that of Hurter and Driffield (H&D), originally described in 1890, by the Swiss-born Ferdinand Hurter (1844–1898) and British Vero Charles Driffield (1848–1915). In their system, speed numbers were inversely proportional to the exposure required. For example, an emulsion rated at 250 H&D would require ten times the exposure of an emulsion rated at 2500 H&D. The methods to determine the sensitivity were later modified in 1925 (in regard to the light source used) and in 1928 (regarding light source, developer and proportional factor)—this later variant was sometimes called "H&D 10". The H&D system was officially accepted as a standard in the former Soviet Union from 1928 until September 1951, when it was superseded by GOST 2817-50. Scheiner. The "Scheinergrade" (Sch.) system was devised by the German astronomer Julius Scheiner (1858–1913) in 1894 originally as a method of comparing the speeds of plates used for astronomical photography. Scheiner's system rated the speed of a plate by the least exposure to produce a visible darkening upon development. Speed was expressed in degrees Scheiner, originally ranging from 1° Sch. to 20° Sch., where an increment of 19° Sch. corresponded to a hundredfold increase in sensitivity, which meant that an increment of 3° Sch. came close to a doubling of sensitivity. The system was later extended to cover larger ranges and some of its practical shortcomings were addressed by the Austrian scientist Josef Maria Eder (1855–1944) and Flemish-born botanist (1896–1960), (who, in 1919/1920, jointly developed their "Eder–Hecht neutral wedge sensitometer" measuring emulsion speeds in "Eder–Hecht" grades). Still, it remained difficult for manufactures to reliably determine film speeds, often only by comparing with competing products, so that an increasing number of modified semi-Scheiner-based systems started to spread, which no longer followed Scheiner's original procedures and thereby defeated the idea of comparability. Scheiner's system was eventually abandoned in Germany, when the standardized DIN system was introduced in 1934. In various forms, it continued to be in widespread use in other countries for some time. DIN. The DIN system, officially DIN standard 4512 by ' (but still named ' (DNA) at this time), was published in January 1934. It grew out of drafts for a standardized method of sensitometry put forward by ' as proposed by the committee for sensitometry of the ' since 1930 and presented by (1868–1945) and Emanuel Goldberg (1881–1970) at the influential VIII. International Congress of Photography (German: "") held in Dresden from August 3 to 8, 1931. The DIN system was inspired by Scheiner's system, but the sensitivities were represented as the base 10 logarithm of the sensitivity multiplied by 10, similar to decibels. Thus an increase of 20° (and not 19°) represented a hundredfold increase in sensitivity, and a difference of 3° was much closer to the base 10 logarithm of 2 (0.30103…): As in the Scheiner system, speeds were expressed in 'degrees'. Originally the sensitivity was written as a fraction with 'tenths' (for example "18/10° DIN"), where the resultant value 1.8 represented the relative base 10 logarithm of the speed. 'Tenths' were later abandoned with DIN 4512:1957-11, and the example above would be written as "18° DIN". The degree symbol was finally dropped with DIN 4512:1961-10. This revision also saw significant changes in the definition of film speeds in order to accommodate then-recent changes in the American ASA PH2.5-1960 standard, so that film speeds of black-and-white negative film effectively would become doubled, that is, a film previously marked as "18° DIN" would now be labeled as "21 DIN" without emulsion changes. Originally only meant for black-and-white negative film, the system was later extended and regrouped into nine parts, including DIN 4512-1:1971-04 for black-and-white negative film, DIN 4512-4:1977-06 for color reversal film and DIN 4512-5:1977-10 for color negative film. On an international level the German DIN 4512 system has been effectively superseded in the 1980s by ISO 6:1974, ISO 2240:1982, and ISO 5800:1979 where the same sensitivity is written in linear and logarithmic form as "ISO 100/21°" (now again with degree symbol). These ISO standards were subsequently adopted by DIN as well. Finally, the latest DIN 4512 revisions were replaced by corresponding ISO standards, DIN 4512-1:1993-05 by DIN ISO 6:1996-02 in September 2000, DIN 4512-4:1985-08 by DIN ISO 2240:1998-06 and DIN 4512-5:1990-11 by DIN ISO 5800:1998-06 both in July 2002. BSI. The film speed scale recommended by the British Standards Institution (BSI) was almost identical to the DIN system except that the BS number was 10 degrees greater than the DIN number. Weston. Before the advent of the ASA system, the system of "Weston film speed ratings" was introduced by Edward Faraday Weston (1878–1971) and his father Dr. Edward Weston (1850–1936), a British-born electrical engineer, industrialist and founder of the US-based Weston Electrical Instrument Corporation, with the Weston model 617, one of the earliest photo-electric exposure meters, in August 1932. The meter and film rating system were invented by William Nelson Goodwin, Jr., who worked for them and later received a Howard N. Potts Medal for his contributions to engineering. The company tested and frequently published speed ratings for most films of the time. Weston film speed ratings could since be found on most Weston exposure meters and were sometimes referred to by film manufactures and third-parties in their exposure guidelines. Since manufactures were sometimes creative about film speeds, the company went as far as to warn users about unauthorized uses of their film ratings in their "Weston film ratings" booklets. The Weston Cadet (model 852 introduced in 1949), Direct Reading (model 853 introduced 1954) and Master III (models 737 and S141.3 introduced in 1956) were the first in their line of exposure meters to switch and utilize the meanwhile established ASA scale instead. Other models used the original Weston scale up until ca. 1955. The company continued to publish Weston film ratings after 1955, but while their recommended values often differed slightly from the ASA film speeds found on film boxes, these newer Weston values were based on the ASA system and had to be converted for use with older Weston meters by subtracting 1/3 exposure stop as per Weston's recommendation. Vice versa, "old" Weston film speed ratings could be converted into "new" Westons and the ASA scale by adding the same amount, that is, a film rating of 100 Weston (up to 1955) corresponded with 125 ASA (as per ASA PH2.5-1954 and before). This conversion was not necessary on Weston meters manufactured and Weston film ratings published since 1956 due to their inherent use of the ASA system; however the changes of the ASA PH2.5-1960 revision may be taken into account when comparing with newer ASA or ISO values. General Electric. Prior to the establishment of the ASA scale and similar to Weston film speed ratings another manufacturer of photo-electric exposure meters, General Electric, developed its own rating system of so-called "General Electric film values" (often abbreviated as "G-E" or "GE") around 1937. Film speed values for use with their meters were published in regularly updated "General Electric Film Values" leaflets and in the "General Electric Photo Data Book". General Electric switched to use the ASA scale in 1946. Meters manufactured since February 1946 were equipped with the ASA scale (labeled "Exposure Index") already. For some of the older meters with scales in "Film Speed" or "Film Value" (e.g. models DW-48, DW-49 as well as early DW-58 and GW-68 variants), replaceable hoods with ASA scales were available from the manufacturer. The company continued to publish recommended film values after that date, however, they were now aligned to the ASA scale. ASA. Based on earlier research work by Loyd Ancile Jones (1884–1954) of Kodak and inspired by the systems of Weston film speed ratings and General Electric film values, the American Standards Association (now named ANSI) defined a new method to determine and specify film speeds of black-and-white negative films in 1943. ASA Z38.2.1-1943 was revised in 1946 and 1947 before the standard grew into ASA PH2.5-1954. Originally, ASA values were frequently referred to as "American standard speed numbers" or "ASA exposure-index numbers". (See also: Exposure Index (EI).) The ASA scale was arithmetic, that is, a film denoted as having a film speed of 200 ASA was twice as fast as a film with 100 ASA. The ASA standard underwent a major revision in 1960 with ASA PH2.5-1960, when the method to determine film speed was refined and previously applied safety factors against under-exposure were abandoned, effectively doubling the nominal speed of many black-and-white negative films. For example, an Ilford HP3 that had been rated at 200 ASA before 1960 was labeled 400 ASA afterwards without any change to the emulsion. Similar changes were applied to the DIN system with DIN 4512:1961-10 and the BS system with BS 1380:1963 in the following years. In addition to the established arithmetic speed scale, ASA PH2.5-1960 also introduced logarithmic ASA grades (100 ASA = 5° ASA), where a difference of 1° ASA represented a full exposure stop and therefore the doubling of a film speed. For some while, ASA grades were also printed on film boxes, and they saw life in the form of the APEX speed value "Sv" (without degree symbol) as well. ASA PH2.5-1960 was revised as ANSI PH2.5-1979, without the logarithmic speeds, and later replaced by NAPM IT2.5-1986 of the National Association of Photographic Manufacturers, which represented the US adoption of the international standard ISO 6. The latest issue of ANSI/NAPM IT2.5 was published in 1993. The standard for color negative film was introduced as ASA PH2.27-1965 and saw a string of revisions in 1971, 1976, 1979 and 1981, before it finally became ANSI IT2.27-1988 prior to its withdrawal. Color reversal film speeds were defined in ANSI PH2.21-1983, which was revised in 1989 before it became ANSI/NAPM IT2.21 in 1994, the US adoption of the ISO 2240 standard. On an international level, the ASA system was superseded by the ISO film speed system between 1982 and 1987, however, the arithmetic ASA speed scale continued to live on as the linear speed value of the ISO system. GOST. ' (Cyrillic: ') was an arithmetic film speed scale defined in GOST 2817-45 and GOST 2817-50. It was used in the former Soviet Union since October 1951, replacing Hurter & Driffield (H&D, Cyrillic: ХиД) numbers, which had been used since 1928. GOST 2817-50 was similar to the ASA standard, having been based on a speed point at a density 0.2 above base plus fog, as opposed to the ASA's 0.1. GOST markings are only found on pre-1987 photographic equipment (film, cameras, lightmeters, etc.) of Soviet Union manufacture. On 1 January 1987, the GOST scale was realigned to the ISO scale with GOST 10691-84, This evolved into multiple parts including GOST 10691.6-88 and GOST 10691.5-88, which both became functional on 1 January 1991. Current system: ISO. The ASA and DIN film speed standards have been combined into the ISO standards since 1974. The current International Standard for measuring the speed of color negative film is ISO 5800:2001 (first published in 1979, revised in November 1987) from the International Organization for Standardization (ISO). Related standards ISO 6:1993 (first published in 1974) and ISO 2240:2003 (first published in July 1982, revised in September 1994, and corrected in October 2003) define scales for speeds of black-and-white negative film and color reversal film, respectively. The determination of ISO speeds with digital still-cameras is described in ISO 12232:2006 (first published in August 1998, revised in April 2006, and corrected in October 2006). The ISO system defines both an arithmetic and a logarithmic scale. The arithmetic ISO scale corresponds to the arithmetic ASA system, where a doubling of film sensitivity is represented by a doubling of the numerical film speed value. In the logarithmic ISO scale, which corresponds to the DIN scale, adding 3° to the numerical value constitutes a doubling of sensitivity. For example, a film rated ISO 200/24° is twice as sensitive as one rated ISO 100/21°. Commonly, the logarithmic speed is omitted; for example, "ISO 100" denotes "ISO 100/21°", while logarithmic ISO speeds are written as "ISO 21°" as per the standard. Conversion between current scales. Conversion from arithmetic speed "S" to logarithmic speed "S"° is given by and rounding to the nearest integer; the log is base 10. Conversion from logarithmic speed to arithmetic speed is given by and rounding to the nearest standard arithmetic speed in Table 1 below. Table notes: Determining film speed. Film speed is found from a plot of optical density vs. log of exposure for the film, known as the "D"–log "H" curve or Hurter–Driffield curve. There typically are five regions in the curve: the base + fog, the toe, the linear region, the shoulder, and the overexposed region. For black-and-white negative film, the “speed point” m is the point on the curve where density exceeds the base + fog density by 0.1 when the negative is developed so that a point n where the log of exposure is 1.3 units greater than the exposure at point m has a density 0.8 greater than the density at point m. The exposure "H"m, in lux-s, is that for point m when the specified contrast condition is satisfied. The ISO arithmetic speed is determined from: This value is then rounded to the nearest standard speed in Table 1 of ISO 6:1993. Determining speed for color negative film is similar in concept but more complex because it involves separate curves for blue, green, and red. The film is processed according to the film manufacturer’s recommendations rather than to a specified contrast. ISO speed for color reversal film is determined from the middle rather than the threshold of the curve; it again involves separate curves for blue, green, and red, and the film is processed according to the film manufacturer’s recommendations. Applying film speed. Film speed is used in the exposure equations to find the appropriate exposure parameters. Four variables are available to the photographer to obtain the desired effect: lighting, film speed, f-number (aperture size), and shutter speed (exposure time). The equation may be expressed as ratios, or, by taking the logarithm (base 2) of both sides, by addition, using the APEX system, in which every increment of 1 is a doubling of exposure; this increment is commonly known as a "stop". The effective f-number is proportional to the ratio between the lens focal length and aperture diameter, the diameter itself being proportional to the square root of the aperture area. Thus, a lens set to allows twice as much light to strike the focal plane as a lens set to 2. Therefore, each f-number factor of the square root of two (approximately 1.4) is also a stop, so lenses are typically marked in that progression: 1.4, 2, 2.8, 4, 5.6, 8, 11, 16, 22, 32, etc. The ISO arithmetic speed has a useful property for photographers without the equipment for taking a metered light reading. Correct exposure will usually be achieved for a frontlighted scene in bright sun if the aperture of the lens is set to f/16 and the shutter speed is the reciprocal of the ISO film speed (e.g. 1/100 second for 100 ISO film). This known as the sunny 16 rule. Exposure index. Exposure index, or EI, refers to speed rating assigned to a particular film and shooting situation in variance to the film's actual speed. It is used to compensate for equipment calibration inaccuracies or process variables, or to achieve certain effects. The exposure index may simply be called the "speed setting", as compared to the speed "rating". For example, a photographer may rate an ISO 400 film at EI 800 and then use push processing to obtain printable negatives in low-light conditions. The film has been exposed at EI 800. Another example occurs where a camera's shutter is miscalibrated and consistently overexposes or underexposes the film; similarly, a light meter may be inaccurate. One may adjust the EI rating accordingly in order to compensate for these defects and consistently produce correctly exposed negatives. Reciprocity. Upon exposure, the amount of light energy that reaches the film determines the effect upon the emulsion. If the brightness of the light is multiplied by a factor and the exposure of the film decreased by the same factor by varying the camera's shutter speed and aperture, so that the energy received is the same, the film will be developed to the same density. This rule is called reciprocity. The systems for determining the sensitivity for an emulsion are possible because reciprocity holds. In practice, reciprocity works reasonably well for normal photographic films for the range of exposures between 1/1000 second to 1/2 second. However, this relationship breaks down outside these limits, a phenomenon known as reciprocity failure. Film sensitivity and grain. The size of silver halide grains in the emulsion affects film sensitivity; which is related to granularity because larger grains give film greater sensitivity to light. Fine-grain film, such as film designed for portraiture or copying original camera negatives, is relatively insensitive, or "slow", because it requires brighter light or a longer exposure than a "fast" film. Fast films, used for photographing in low light or capturing high-speed motion, produce comparatively grainy images. Kodak has defined a "Print Grain Index" (PGI) to characterize film grain (color negative films only), based on perceptual just-noticeable difference of graininess in prints. They also define "granularity", a measurement of grain using an RMS measurement of density fluctuations in uniformly exposed film, measured with a microdensitometer with 48 micrometre aperture. Granularity varies with exposure — underexposed film looks grainier than overexposed film. Marketing anomalies. Some high-speed black-and-white films, such as Ilford Delta 3200 and Kodak T-MAX P3200, are marketed with film speeds in excess of their true ISO speed as determined using the ISO testing method. For example, the Ilford product is actually an ISO 1000 film, according to its data sheet. The manufacturers do not indicate that the 3200 number is an ISO rating on their packaging. Kodak and Fuji also marketed E6 films designed for pushing (hence the "P" prefix), such as Ektachrome P800/1600 and Fujichrome P1600, both with a base speed of ISO 400. Digital camera ISO speed and exposure index. In digital camera systems, an arbitrary relationship between exposure and sensor data values can be achieved by setting the signal gain of the sensor. The relationship between the sensor data values and the lightness of the finished image is also arbitrary, depending on the parameters chosen for the interpretation of the sensor data into an image color space such as sRGB. For digital photo cameras ("digital still cameras"), an exposure index (EI) rating—commonly called "ISO" setting—is specified by the manufacturer such that the sRGB image files produced by the camera will have a lightness similar to what would be obtained with film of the same EI rating at the same exposure. The usual design is that the camera's parameters for interpreting the sensor data values into sRGB values are fixed, and a number of different EI choices are accommodated by varying the sensor's signal gain in the analog realm, prior to conversion to digital. Some camera designs provide at least some EI choices by adjusting the sensor's signal gain in the digital realm. A few camera designs also provide EI adjustment through a choice of lightness parameters for the interpretation of sensor data values into sRGB; this variation allows different tradeoffs between the range of highlights that can be captured and the amount of noise introduced into the shadow areas of the photo. Digital cameras have far surpassed film in terms of sensitivity to light, with ISO equivalent speeds of up to 204800, a number that is unfathomable in the realm of conventional film photography. Faster processors, as well as advances in software noise reduction techniques allow this type of processing to be executed the moment the photo is captured, allowing photographers to store images that have a higher level of refinement and would have been prohibitively time consuming to process with earlier generations of digital camera hardware. The ISO 12232:2006 standard. The ISO standard ISO 12232:2006 gives digital still camera manufacturers a choice of five different techniques for determining the exposure index rating at each sensitivity setting provided by a particular camera model. Three of the techniques in ISO 12232:2006 are carried over from the 1998 version of the standard, while two new techniques allowing for measurement of JPEG output files are introduced from CIPA DC-004. Depending on the technique selected, the exposure index rating can depend on the sensor sensitivity, the sensor noise, and the appearance of the resulting image. The standard specifies the measurement of light sensitivity of the entire digital camera system and not of individual components such as digital sensors, although Kodak has reported using a variation to characterize the sensitivity of two of their sensors in 2001. The "Recommended Exposure Index" (REI) technique, new in the 2006 version of the standard, allows the manufacturer to specify a camera model’s EI choices arbitrarily. The choices are based solely on the manufacturer’s opinion of what EI values produce well-exposed sRGB images at the various sensor sensitivity settings. This is the only technique available under the standard for output formats that are not in the sRGB color space. This is also the only technique available under the standard when multi-zone metering (also called "pattern" metering) is used. The "Standard Output Sensitivity" (SOS) technique, also new in the 2006 version of the standard, effectively specifies that the average level in the sRGB image must be 18% gray plus or minus 1/3 stop when the exposure is controlled by an automatic exposure control system calibrated per ISO 2721 and set to the EI with no exposure compensation. Because the output level is measured in the sRGB output from the camera, it is only applicable to sRGB JPEG—and not to output files in raw image format. It is not applicable when multi-zone metering is used. The CIPA DC-004 standard requires that Japanese manufacturers of digital still cameras use either the REI or SOS techniques, and DC-008 updates the Exif specification to differentiate between these values. Consequently, the three EI techniques carried over from ISO 12232:1998 are not widely used in recent camera models (approximately 2007 and later). As those earlier techniques did not allow for measurement from images produced with lossy compression, they cannot be used at all on cameras that produce images only in JPEG format. The "saturation-based" technique is closely related to the SOS technique, with the sRGB output level being measured at 100% white rather than 18% gray. The saturation-based value is effectively 0.704 times the SOS value. Because the output level is measured in the sRGB output from the camera, it is only applicable to sRGB images—typically TIFF—and not to output files in raw image format. It is not applicable when multi-zone metering is used. The two "noise-based" techniques have rarely been used for consumer digital still cameras. These techniques specify the highest EI that can be used while still providing either an “excellent” picture or a “usable” picture depending on the technique chosen. Measurements and calculations. ISO speed ratings of a digital camera are based on the properties of the sensor and the image processing done in the camera, and are expressed in terms of the luminous exposure "H" (in lux seconds) arriving at the sensor. For a typical camera lens with an effective focal length "f" that is much smaller than the distance between the camera and the photographed scene, "H" is given by where "L" is the luminance of the scene (in candela per m²), "t" is the exposure time (in seconds), "N" is the aperture f-number, and is a factor depending on the transmittance "T" of the lens, the vignetting factor "v"("θ"), and the angle "θ" relative to the axis of the lens. A typical value is "q" = 0.65, based on "θ" = 10°, "T" = 0.9, and "v" = 0.98. Saturation-based speed. The "saturation-based speed" is defined as where formula_9 is the maximum possible exposure that does not lead to a clipped or bloomed camera output. Typically, the lower limit of the saturation speed is determined by the sensor itself, but with the gain of the amplifier between the sensor and the analog-to-digital converter, the saturation speed can be increased. The factor 78 is chosen such that exposure settings based on a standard light meter and an 18-percent reflective surface will result in an image with a grey level of 18%/√2 = 12.7% of saturation. The factor √2 indicates that there is half a stop of headroom to deal with specular reflections that would appear brighter than a 100% reflecting white surface. Noise-based speed. The "noise-based speed" is defined as the exposure that will lead to a given signal-to-noise ratio on individual pixels. Two ratios are used, the 40:1 ("excellent image quality") and the 10:1 ("acceptable image quality") ratio. These ratios have been subjectively determined based on a resolution of 70 pixels per cm (180 DPI) when viewed at 25 cm (10 inch) distance. The signal-to-noise ratio is defined as the standard deviation of a weighted average of the luminance and color of individual pixels. The noise-based speed is mostly determined by the properties of the sensor and somewhat affected by the noise in the electronic gain and AD converter. Standard output sensitivity (SOS). In addition to the above speed ratings, the standard also defines the "standard output sensitivity" (SOS), how the exposure is related to the digital pixel values in the output image. It is defined as where formula_11 is the exposure that will lead to values of 118 in 8-bit pixels, which is 18 percent of the saturation value in images encoded as sRGB or with gamma = 2.2. Discussion. The standard specifies how speed ratings should be reported by the camera. If the noise-based speed (40:1) is "higher" than the saturation-based speed, the noise-based speed should be reported, rounded "downwards" to a standard value (e.g. 200, 250, 320, or 400). The rationale is that exposure according to the lower saturation-based speed would not result in a visibly better image. In addition, an exposure latitude can be specified, ranging from the saturation-based speed to the 10:1 noise-based speed. If the noise-based speed (40:1) is "lower" than the saturation-based speed, or undefined because of high noise, the saturation-based speed is specified, rounded upwards to a standard value, because using the noise-based speed would lead to overexposed images. The camera may also report the SOS-based speed (explicitly as being an SOS speed), rounded to the nearest standard speed rating. For example, a camera sensor may have the following properties: formula_12, formula_13, and formula_14. According to the standard, the camera should report its sensitivity as The SOS rating could be user controlled. For a different camera with a noisier sensor, the properties might be formula_15, formula_16, and formula_17. In this case, the camera should report as well as a user-adjustable SOS value. In all cases, the camera should indicate for the white balance setting for which the speed rating applies, such as daylight or tungsten (incandescent light). Despite these detailed standard definitions, cameras typically do not clearly indicate whether the user "ISO" setting refers to the noise-based speed, saturation-based speed, or the specified output sensitivity, or even some made-up number for marketing purposes. Because the 1998 version of ISO 12232 did not permit measurement of camera output that had lossy compression, it was not possible to correctly apply any of those measurements to cameras that did not produce sRGB files in an uncompressed format such as TIFF. Following the publication of CIPA DC-004 in 2006, Japanese manufacturers of digital still cameras are required to specify whether a sensitivity rating is REI or SOS. As should be clear from the above, a greater SOS setting for a given sensor comes with some loss of image quality, just like with analog film. However, this loss is visible as image noise rather than grain. Current (January 2010) APS and 35mm sized digital image sensors, both CMOS and CCD based, do not produce significant noise until about "ISO 1600 [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@61f25b53 [docID]168568 [hitPos]9 [correct]false [extraScores][F@10dae00f , [answer]In 2011, the OPERA experiment mistakenly observed neutrinos appearing to travel faster than light. Even before the mistake was discovered, the result was considered anomalous because speeds higher than that of light in a vacuum are generally thought to violate special relativity, a cornerstone of the modern understanding of physics for over a century. OPERA scientists announced the results of the experiment in with the stated intent of promoting further inquiry and debate. Later the team reported two flaws in their equipment set-up that had caused errors far outside of their original confidence interval: a fiber optic cable attached improperly, which caused the apparently faster-than-light measurements, and a clock oscillator ticking too fast. The errors were first confirmed by OPERA after a ScienceInsider report; accounting for these two sources of error eliminated the faster-than-light results. In March 2012, the collocated ICARUS experiment reported neutrino velocities consistent with the speed of light in the same short-pulse beam OPERA had measured in November 2011. ICARUS used a partly different timing system from OPERA and measured seven different neutrinos. In addition, the Gran Sasso experiments BOREXINO, ICARUS, LVD and OPERA all measured neutrino velocity with a short-pulsed beam in May, and obtained agreement with the speed of light. On June 8, 2012 CERN research director Sergio Bertolucci declared on behalf of the four Gran Sasso teams, including OPERA, that the speed of neutrinos is consistent with that of light. The press release, made from the 25th International Conference on Neutrino Physics and Astrophysics in Kyoto, states that the original OPERA results were wrong, due to equipment failures. On July 12, 2012 OPERA updated their paper by including the new sources of errors in their calculations. They found agreement of neutrino speed with the speed of light. Detection. The experiment created a form of neutrinos, muon neutrinos, at CERN's older SPS accelerator, on the Franco–Swiss border, and detected them at the LNGS lab in Gran Sasso, Italy. OPERA researchers used common-view GPS, derived from standard GPS, to measure the times and place coordinates at which the neutrinos were created and detected. As computed, the neutrinos' average time of flight turned out to be less than what light would need to travel the same distance in a vacuum. In a two-week span up to , the OPERA team repeated the measurement with a different way of generating neutrinos, which helped measure travel time of each detected neutrino separately. This eliminated some possible errors related to matching detected neutrinos to their creation time. The OPERA collaboration stated in their initial press release that further scrutiny and independent tests were necessary to definitely confirm or refute the results. First results. In a analysis of their data, scientists of the OPERA collaboration reported evidence that neutrinos they produced at CERN in Geneva and recorded at the OPERA detector at Gran Sasso, Italy, had traveled faster than light. The neutrinos were calculated to have arrived approximately 60.7 nanoseconds (60.7 billionths of a second) sooner than light would have if traversing the same distance in a vacuum. After six months of cross checking, on , the researchers announced that neutrinos had been observed traveling at faster-than-light speed. Similar results were obtained using higher-energy (28 GeV) neutrinos, which were observed to check if neutrinos' velocity depended on their energy. The particles were measured arriving at the detector faster than light by approximately one part per 40,000, with a 0.2-in-a-million chance of being wrong, if the error were distributed as a bell curve (significance of six sigma). This measure included estimates for both errors in measuring and errors from the statistical procedure used. It was, however, a measure of precision, not accuracy, which could be influenced by elements such as incorrect computations or wrong readouts of instruments. For particle physics experiments involving collision data, the standard for a discovery announcement is a five-sigma error limit, looser than the observed six-sigma limit. The preprint of the research stated "observed deviation of the neutrino velocity from "c" of light in vacuum would be a striking result pointing to new physics in the neutrino sector" and referred to the "early arrival time of CNGS muon neutrinos" as an "anomaly". OPERA spokesperson Antonio Ereditato explained that the OPERA team had "not found any instrumental effect that could explain the result of the measurement". James Gillies, a spokesperson for CERN, said on September 22 that the scientists were "inviting the broader physics community to look at what they done and really scrutinize it in great detail, and ideally for someone elsewhere in the world to repeat the measurements". Internal replication. In November, OPERA published refined results where they noted their chances of being wrong as even less, thus tightening their error bounds. Neutrinos arrived approximately 57.8 ns earlier than if they had traveled at light-speed, giving a relative speed difference of approximately one part per 42,000 against that of light. The new significance level became 6.2 sigma. The collaboration submitted its results for peer-reviewed publication to the Journal of High Energy Physics. In the same paper, the OPERA collaboration also published the results of a repeat experiment running from to . They detected twenty neutrinos consistently indicating an early neutrino arrival of approximately 62.1 ns, in agreement with the result of the main analysis. Measurement errors. In February 2012, the OPERA collaboration announced two possible sources of error that could have significantly influenced the results. In March 2012 an LNGS seminar was held, confirming the fiber cable was not fully screwed in during data gathering. LVD researchers compared the timing data for cosmic high-energy muons hitting both the OPERA and the nearby LVD detector between 2007–2008, 2008–2011, and 2011–2012. The shift obtained for the 2008–2011 period agreed with the OPERA anomaly. The researchers also found photographs showing the cable had been loose by October 13, 2011. Correcting for the two newly found sources of error, results for neutrino speed appear to be consistent with the speed of light. End results. On July 12, 2012 the OPERA collaboration published the end results of their measurements between 2009–2011. The difference between the measured and expected arrival time of neutrinos (compared to the speed of light) was approximately . This is consistent with no difference at all, thus the speed of neutrinos is consistent with the speed of light within the margin of error. Also the re-analysis of the 2011 bunched beam rerun gave a similar result. Independent replication. In March 2012, the co-located ICARUS experiment refuted the OPERA results by measuring neutrino velocity to be that of light. ICARUS measured speed for seven neutrinos in the same short-pulse beam OPERA had checked in November 2011, and found them, on average, traveling at the speed of light. The results were from a trial run of neutrino-velocity measurements slated for May. In May 2012, a new bunched beam rerun was initiated by CERN. Then in June 2012, it was announced by CERN that the four Gran Sasso experiments OPERA, ICARUS, LVD, and BOREXINO measured neutrino speeds consistent with the speed of light, indicating that the initial OPERA result was due to equipment errors. In addition, Fermilab has stated that the detectors for the MINOS project are being upgraded. Fermilab scientists closely analyzed and placed bounds on the errors in their timing system. In June 8, 2012 MINOS announced that according to preliminary results, the neutrino speed is consistent with the speed of light. The measurement. The OPERA experiment was designed to capture how neutrinos switch between different identities, but Autiero realized the equipment could be used to precisely measure neutrino speed too. An earlier result from the MINOS experiment at Fermilab demonstrated that the measurement was technically feasible. The principle of the OPERA neutrino velocity experiment was to compare travel time of neutrinos against travel time of light. The neutrinos in the experiment emerged at CERN and flew to the OPERA detector. The researchers divided this distance by the speed of light in vacuum to predict what the neutrino travel time should be. They compared this expected value to the measured travel time. Overview. The OPERA team used an already existing beam of neutrinos traveling continuously from CERN to LNGS, the CERN Neutrinos to Gran Sasso beam, for the measurement. Measuring speed meant measuring the distance traveled by the neutrinos from their source to where they were detected, and the time taken by them to travel this length. The source at CERN was more than away from the detector at LNGS (Gran Sasso). The experiment was tricky because there was no way to time an individual neutrino, necessitating more complex steps. As shown in Fig. 1, CERN generates neutrinos by slamming protons, in pulses of length 10.5 microseconds (10.5 millionths of a second), into a graphite target to produce intermediate particles, which decay into neutrinos. OPERA researchers measured the protons as they passed a section called the beam current transducer (BCT) and took the transducer's position as the neutrinos' starting point. The protons did not actually create neutrinos for another kilometer, but because both protons and the intermediate particles moved almost at light speed, the error from the assumption was acceptably low. The clocks at CERN and LNGS had to be in sync, and for this the researchers used high-quality GPS receivers, backed up with atomic clocks, at both places. This system timestamped both the proton pulse and the detected neutrinos to a claimed accuracy of 2.3 nanoseconds. But the timestamp could not be read like a clock. At CERN, the GPS signal came only to a receiver at a central control room, and had to be routed with cables and electronics to the computer in the neutrino-beam control room which recorded the proton pulse measurement (Fig. 3). The delay of this equipment was 10,085 nanoseconds and this value had to be added to the time stamp. The data from the transducer arrived at the computer with a 580 nanoseconds delay, and this value had to be subtracted from the time stamp. To get all the corrections right, physicists had to measure exact lengths of the cables and the latencies of the electronic devices. On the detector side, neutrinos were detected by the charge they induced, not by the light they generated, and this involved cables and electronics as part of the timing chain. Fig. 4 shows the corrections applied on the OPERA detector side. Since neutrinos could not be accurately tracked to the specific protons producing them, an averaging method had to be used. The researchers added up the measured proton pulses to get an average distribution in time of the individual protons in a pulse. The time at which neutrinos were detected at Gran Sasso was plotted to produce another distribution. The two distributions were expected to have similar shapes, but be separated by 2.4 milliseconds, the time it takes to travel the distance at light speed. The experimenters used an algorithm, maximum likelihood, to search for the time shift that best made the two distributions to coincide. The shift so calculated, the statistically measured neutrino arrival time, was approximately 60 nanoseconds shorter than the 2.4 milliseconds neutrinos would have taken if they traveled just at light speed. In a later experiment, the proton pulse width was shortened to 3 nanoseconds, and this helped the scientists to narrow the generation time of each detected neutrino to that range. Measuring distance. Distance was measured by accurately fixing the source and detector points on a global coordinate system (ETRF2000). CERN surveyors used GPS to measure the source location. On the detector side, the OPERA team worked with a geodesy group from the Sapienza University of Rome to locate the detector's center with GPS and standard map-making techniques. To link the surface GPS location to the coordinates of the underground detector, traffic had to be partially stopped on the access road to the lab. Combining the two location measurements, the researchers calculated the distance, to an accuracy of 20 cm within the 730 km path. Measuring trip time. The travel time of the neutrinos had to be measured by tracking the time they were created, and the time they were detected, and using a common clock to ensure the times were in sync. As Fig. 1 shows, the time measuring system included the neutrino source at CERN, the detector at LNGS (Gran Sasso), and a satellite element common to both. The common clock was the time signal from multiple GPS satellites visible from both CERN and LNGS. CERN's beams-department engineers worked with the OPERA team to provide a travel time measurement between the source at CERN and a point just before the OPERA detector's electronics, using accurate GPS receivers. This included timing the proton beams' interactions at CERN, and timing the creation of intermediate particles eventually decaying into neutrinos (see Fig. 3). Researchers from OPERA measured the remaining delays and calibrations not included in the CERN calculation: those shown in Fig. 4. The neutrinos were detected in an underground lab, but the common clock from the GPS satellites was visible only above ground level. The clock value noted above-ground had to be transmitted to the underground detector with an 8 km fiber cable. The delays associated with this transfer of time had to be accounted for in the calculation. How much the error could vary (the standard deviation of the errors) mattered to the analysis, and had to be calculated for each part of the timing chain separately. Special techniques were used to measure the length of the fiber and its consequent delay, required as part of the overall calculation. In addition, to sharpen resolution from the standard GPS 100 nanoseconds to the 1 nanosecond range metrology labs achieve, OPERA researchers used Septentrio’s precise PolaRx2eTR GPS timing receiver, along with consistency checks across clocks (time calibration procedures) which allowed for common-view time transfer. The PolaRx2eTR allowed measurement of the time offset between an atomic clock and each of the Global Navigation Satellite System satellite clocks. For calibration, the equipment was taken to the Swiss Metrology Institute (METAS). In addition, highly stable cesium clocks were installed both at LNGS and CERN to cross-check GPS timing and to increase its precision. After OPERA found the superluminal result, the time calibration was rechecked both by a CERN engineer and the German Institute of Metrology (PTB). Time-of-flight was eventually measured to an accuracy of 10 nanoseconds. The final error bound was derived by combining the variance of the error for the individual parts. The analysis. The OPERA team analyzed the results in different ways and using different experimental methods. Following the initial main analysis released in September, three further analyses were made public in November. In the main November analysis, all the existing data were reanalyzed to allow adjustments for other factors, such as the Sagnac effect in which the Earth's rotation affects the distance traveled by the neutrinos. Then an alternative analysis adopted a different model for the matching of the neutrinos to their creation time. The third analysis of November focused on a different experimental setup ('the rerun') which changed the way the neutrinos were created. In the initial setup, every detected neutrino would have been produced sometime in a 10,500 nanoseconds (10.5 microseconds) range, since this was the duration of the proton beam spill generating the neutrinos. It was not possible to isolate neutrino production time further within the spill. Therefore, in their main statistical analyses, the OPERA group generated a model of the proton waveforms at CERN, took the various waveforms together, and plotted the chance of neutrinos being emitted at various times (the global probability density function of the neutrino emission times). They then compared this plot against a plot of the arrival times of the 15,223 detected neutrinos. This comparison indicated neutrinos had arrived at the detector 57.8 nanoseconds faster than if they had been traveling at the speed of light in vacuum. An alternative analysis in which each detected neutrino was checked against the waveform of its associated proton spill (instead of against the global probability density function) led to a compatible result of approximately 54.5 nanoseconds. The November main analysis, which showed an early arrival time of 57.8 nanoseconds, was conducted blind to avoid observer bias, whereby those running the analysis might inadvertently fine-tune the result toward expected values. To this end, old and incomplete values for distances and delays from the year 2006 were initially adopted. With the final correction needed not yet known, the intermediate expected result was also an unknown. Analysis of the measurement data under those 'blind' conditions gave an early neutrino arrival of 1043.4 nanoseconds. Afterward, the data were analyzed again taking into consideration the complete and actual sources of errors. If neutrino and light speed were the same, a subtraction value of 1043.4 nanoseconds should have been obtained for the correction. However, the actual subtraction value amounted to only 985.6 nanoseconds, corresponding to an arrival time 57.8 nanoseconds earlier than expected. Two facets of the result came under particular scrutiny within the neutrino community: the GPS synchronization system, and the profile of the proton beam spill that generated neutrinos. The second concern was addressed in the November rerun: for this analysis, OPERA scientists repeated the measurement over the same baseline using a new CERN proton beam which circumvented the need to make any assumptions about the details of neutrino production during the beam activation, such as energy distribution or production rate. This beam provided proton pulses of 3 nanoseconds each with up to 524 nanosecond gaps. This meant a detected neutrino could be tracked uniquely to its generating 3 nanoseconds pulse, and hence its start and end travel times could be directly noted. Thus, the neutrino's speed could now be calculated without having to resort to statistical inference. In addition to the four analyses mentioned earlier—September main analysis, November main analysis, alternative analysis, and the rerun analysis—the OPERA team also split the data by neutrino energy and reported the results for each set of the September and November main analyses. The rerun analysis had too few neutrinos to consider splitting the set further. Reception by the physics community. After the initial report of apparent superluminal velocities of neutrinos, most physicists in the field were quietly skeptical of the results, but prepared to adopt a wait-and-see approach. Experimental experts were aware of the complexity and difficulty of the measurement, so an extra unrecognized measurement error was still a real possibility, despite the care taken by the OPERA team. However, because of the widespread interest, several well-known experts did make public comments. Nobel laureates Steven Weinberg, George Smoot III, and Carlo Rubbia, and other physicists not affiliated with the experiment, including Michio Kaku, expressed skepticism about the accuracy of the experiment on the basis that the results challenged a long-held theory consistent with the results of many other tests of special relativity. Nevertheless, Ereditato, the OPERA spokesperson, stated that no one had an explanation that invalidated the experiment's results. Previous experiments of neutrino speed played a role in the reception of the OPERA result by the physics community. Those experiments did not detect statistically significant deviations of neutrino speeds from the speed of light. For instance, Astronomer Royal Martin Rees and theoretical physicists Lawrence Krauss and Stephen Hawking stated neutrinos from the SN 1987A supernova explosion arrived almost at the same time as light, indicating no faster-than-light neutrino speed. John Ellis, theoretical physicist at CERN, believed it difficult to reconcile the OPERA results with the SN 1987A observations. Observations of this supernova restricted 10 MeV anti-neutrino speed to less than 20 parts per billion (ppb) over lightspeed. This was one of the reasons most physicists suspected the OPERA team had made an error. Physicists affiliated with the experiment had refrained from interpreting the result, stating in their paper: Andrew Cohen and Glashow have predicted that superluminal neutrinos would radiate electrons and positrons and lose energy through vacuum Cherenkov effects, where a particle traveling faster than light decays continuously into other slower particles. However, this energy attrition was absent both in the OPERA experiment and in the colocated ICARUS experiment, which uses the same CNGS beam as OPERA. This discrepancy was seen by Cohen and Glashow to present "a significant challenge to the superluminal interpretation of the OPERA data". Many other scientific papers on the anomaly were published as arXiv preprints or in peer reviewed journals. Some of them criticized the result, while others tried to find theoretical explanations, replacing or extending special relativity and the standard model. Discussions within the OPERA collaboration. In the months after the initial announcement, tensions emerged in the OPERA collaboration. A vote of no confidence among the more than thirty group team leaders failed, but spokesperson Ereditato and physics coordinator Autiero resigned their leadership positions anyway on March 30, 2012. In a resignation letter, Ereditato claimed that their results were "excessively sensationalized and portrayed with not always justified simplification" and defended the collaboration, stating, "The OPERA Collaboration has always acted in full compliance with scientific rigor: both when it announced the results and when it provided an explanation for them [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@12925ecb [docID]33338392 [hitPos]9 [correct]false [extraScores][F@33915fa4 , [answer]In general relativity, an event horizon is a boundary in spacetime beyond which events cannot affect an outside observer. In layman's terms, it is defined as "the point of no return" i.e. the point at which the gravitational pull becomes so great as to make escape impossible. The most common case of an event horizon is that surrounding a black hole. Light emitted from beyond the event horizon can never reach the outside observer. Likewise, any object approaching the horizon from the observer's side appears to slow down and never quite pass through the horizon, with its image becoming more and more redshifted as time elapses. The traveling object, however, experiences no strange effects and does, in fact, pass through the horizon in a finite amount of proper time. More specific types of horizon include the related but distinct absolute and apparent horizons found around a black hole. Still other distinct notions include the Cauchy and Killing horizon; the photon spheres and ergospheres of the Kerr solution; particle and cosmological horizons relevant to cosmology; and isolated and dynamical horizons important in current black hole research. __TOC__ Existence and Evolution of the Particle Horizon. A nice and simple example of event horizon emerges from some cases of the FLRW cosmological model. In that context, the universe can be approximated as composed by non-interacting constituents, each one being a perfect fluid with density formula_1, partial pressure formula_2 and state equation formula_3, such that they add up to the total density formula_4 and total pressure formula_5. Let us now define the following functions: Any function with a zero subscript denote the function evaluated at the present time formula_12 (or equivalently formula_13). The last term can be taken to be formula_14 including the curvature state equation. It can be proved that the Hubble function is given by where formula_16. Notice that the addition ranges over all possible partial constituents and in particular there can be countably infinitely many. With this notation we have: where formula_18 is the lowest formula_19 (possibly formula_20). The evolution of the event horizon for an expanding universe (formula_21) is: where formula_23 is the speed of light and can be taken to be formula_14 (natural units). Notice that the derivative is made with respect to the FLRW-time formula_25, while the functions are evaluated at the redshift formula_10 which are related as stated before. We have an analogous but slightly different result for Particle horizon. Event horizon of a black hole. One of the best-known examples of an event horizon derives from general relativity's description of a black hole, a celestial object so massive that no nearby matter or radiation can escape its gravitational field. Often, this is described as the boundary within which the black hole's escape velocity is greater than the speed of light. However, a more accurate description is that within this horizon, all lightlike paths (paths that light could take) and hence all paths in the forward light cones of particles within the horizon, are warped so as to fall farther into the hole. Once a particle is inside the horizon, moving into the hole is as inevitable as moving forward in time, and can actually be thought of as equivalent to doing so, depending on the spacetime coordinate system used. The surface at the Schwarzschild radius acts as an event horizon in a non-rotating body that fits inside this radius (although a rotating black hole operates slightly differently). The Schwarzschild radius of an object is proportional to its mass. Theoretically, any amount of matter will become a black hole if compressed into a space that fits within its corresponding Schwarzschild radius. For the mass of the Sun this radius is approximately 3 kilometers and for the Earth it is about 9 millimeters. In practice, however, neither the Earth nor the Sun has the necessary mass and therefore the necessary gravitational force, to overcome electron and neutron degeneracy pressure. The minimal mass required for a star to be able to collapse beyond these pressures is the Tolman-Oppenheimer-Volkoff limit, which is approximately three solar masses. Black hole event horizons are especially noteworthy for three reasons. First, there may be examples near enough (on an astronomical scale) to study. Second, black holes tend to pull in matter from their environment, which provides examples where matter about to pass through an event horizon is expected to be observable. Third, the description of black holes given by general relativity is known to be an approximation and it is expected that quantum gravity effects become significant in the vicinity of the event horizon. This allows observations of matter in the vicinity of a black hole's event horizon to be used to indirectly study general relativity and proposed extensions to it. Particle horizon of the observable universe. The particle horizon of the observable universe is the boundary that represents the maximum distance at which events can "currently" be observed. For events beyond that distance, light has not had time to reach our location, even if it were emitted at the time the universe began. How the particle horizon changes with time depends on the nature of the expansion of the universe. If the expansion has certain characteristics, there are parts of the universe that will never be observable, no matter how long the observer waits for light from those regions to arrive. The boundary past which events cannot ever be observed is an event horizon, and it represents the maximum extent of the particle horizon. The criterion for determining whether a particle horizon for the universe exists is as follows. Define a comoving distance formula_27 by In this equation, "a" is the scale factor, "c" is the speed of light, and "t0" is the age of the universe. If formula_29 (i.e. points arbitrarily as far away as can be observed), then no event horizon exists. If formula_30, a horizon is present. Examples of cosmological models without an event horizon are universes dominated by matter or by radiation. An example of a cosmological model with an event horizon is a universe dominated by the cosmological constant (a de Sitter universe). Apparent horizon of an accelerated particle. If a particle is moving at a constant velocity in a non-expanding universe free of gravitational fields, any event that occurs in that universe will eventually be observable by the particle, because the forward light cones from these events intersect the particle's world line. On the other hand, if the particle is accelerating, in some situations light cones from some events never intersect the particle's world line. Under these conditions, an apparent horizon is present in the particle's (accelerating) reference frame, representing a boundary beyond which events are unobservable. For example, this occurs with a uniformly accelerated particle. A space-time diagram of this situation is shown in the figure to the right. As the particle accelerates, it approaches, but never reaches, the speed of light with respect to its original reference frame. On the space-time diagram, its path is a hyperbola, which asymptotically approaches a 45 degree line (the path of a light ray). An event whose light cone's edge is this asymptote or is farther away than this asymptote can never be observed by the accelerating particle. In the particle's reference frame, there appears to be a boundary behind it from which no signals can escape (an apparent horizon). While approximations of this type of situation can occur in the real world (in particle accelerators, for example), a true event horizon is never present, as the particle must be accelerated indefinitely (requiring arbitrarily large amounts of energy and an arbitrarily large apparatus). Interacting with an event horizon. A misconception concerning event horizons, especially black hole event horizons, is that they represent an immutable surface that destroys objects that approach them. In practice, all event horizons appear to be some distance away from any observer, and objects sent towards an event horizon never appear to cross it from the sending observer's point of view (as the horizon-crossing event's light cone never intersects the observer's world line). Attempting to make an object near the horizon remain stationary with respect to an observer requires applying a force whose magnitude increases unbounded (becoming infinite) the closer it gets. For the case of a horizon perceived by a uniformly accelerating observer in empty space, the horizon seems to remain a fixed distance from the observer no matter how its surroundings move. Varying the observer's acceleration may cause the horizon to appear to move over time, or may prevent an event horizon from existing, depending on the acceleration function chosen. The observer never touches the horizon and never passes a location where it appeared to be. For the case of a horizon perceived by an occupant of a de Sitter Universe, the horizon always appears to be a fixed distance away for a non-accelerating observer. It is never contacted, even by an accelerating observer. For the case of the horizon around a black hole, observers stationary with respect to a distant object will all agree on where the horizon is. While this seems to allow an observer lowered towards the hole on a rope (or rod) to contact the horizon, in practice this cannot be done. The proper distance to the horizon is finite, so the length of rope needed would be finite as well, but if the rope were lowered slowly (so that each point on the rope was approximately at rest in Schwarzschild coordinates), the proper acceleration (G-force) experienced by points on the rope closer and closer to the horizon would approach infinity, so the rope would be torn apart. If the rope is lowered quickly (perhaps even in freefall), then indeed the observer at the bottom of the rope can touch and even cross the event horizon. But once this happens it is impossible to pull the bottom of rope back out of the event horizon, since if the rope is pulled taut, the forces along the rope increase without bound as they approach the event horizon and at some point the rope must break. Furthermore, the break must occur not at the event horizon, but at a point where the second observer can observe it. An observer crossing a black hole event horizon can calculate the moment they have crossed it, but will not actually see or feel anything special happen at that moment. In terms of visual appearance, observers who fall into the hole perceive the black region constituting the horizon as lying at some apparent distance below them, and never experience crossing this visual horizon. Other objects that had entered the horizon along the same radial path but at an earlier time would appear below the observer but still above the visual position of the horizon, and if they had fallen in recently enough the observer could exchange messages with them before either one was destroyed by the gravitational singularity. Increasing tidal forces (and eventual impact with the hole's singularity) are the only locally noticeable effects. Beyond general relativity. The description of event horizons given by general relativity is thought to be incomplete. When the conditions under which event horizons occur are modeled using a more comprehensive picture of the way the universe works, that includes both relativity and quantum mechanics, event horizons are expected to have properties that are different from those predicted using general relativity alone. At present, it is expected that the primary impact of quantum effects is for event horizons to possess a temperature and so emit radiation. For black holes, this manifests as Hawking radiation, and the larger question of how the black hole possesses a temperature is part of the topic of black hole thermodynamics. For accelerating particles, this manifests as the Unruh effect, which causes space around the particle to appear to be filled with matter and radiation. A complete description of event horizons is expected to, at minimum, require a theory of quantum gravity. One such candidate theory is M-theory. Another such candidate theory is Loop Quantum Gravity [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@15ab6789 [docID]29320146 [hitPos]9 [correct]false [extraScores][F@687aa3d1 , [answer]In physics, special relativity is a fundamental theory concerning space and time, developed by Albert Einstein in 1905 as a modification of Galilean relativity. (See "History of special relativity" for a detailed account and the contributions of Hendrik Lorentz and Henri Poincaré.) The theory was able to explain some pressing theoretical and experimental issues in the physics of the time involving light and electrodynamics, such as the failure of the 1887 Michelson–Morley experiment, which aimed to measure differences in the relative speed of light due to the Earth's motion through the hypothetical, and now discredited, luminiferous aether. The aether was then considered to be the medium of propagation of electromagnetic waves such as light. Einstein postulated that the speed of light in free space is the same for all observers, regardless of their motion relative to the light source, where we may think of an observer as an imaginary entity with a sophisticated set of measurement devices, at rest with respect to itself, that perfectly records the positions and times of all events in space and time. This postulate stemmed from the assumption that Maxwell's equations of electromagnetism, which predict a specific speed of light in a vacuum, hold in any inertial frame of reference rather than, as was previously believed, just in the frame of the aether. This prediction contradicted the laws of classical mechanics, which had been accepted for centuries, by arguing that time and space are not fixed and in fact change to maintain a constant speed of light regardless of the relative motions of sources and observers. Einstein's approach was based on thought experiments, calculations, and the principle of relativity, which is the notion that all physical laws should appear the same (that is, take the same basic form) to all inertial observers. Today, the result is that the speed of light defines the metre as "the length of the path travelled by light in vacuum during a time interval of of a second." This relates that the speed of light is "by convention" (approximately 1.079 billion kilometres per hour, or 671 million miles per hour). The predictions of special relativity are almost identical to those of Galilean relativity for most everyday phenomena, in which speeds are much lower than the speed of light, but it makes different, non-obvious predictions for objects moving at very high speeds. These predictions have been experimentally tested on numerous occasions since the theory's inception and were confirmed by those experiments. The major predictions of special relativity are: Special relativity predicts a non-linear velocity addition formula which prevents speeds greater than that of light from being observed. In 1908, Hermann Minkowski reformulated the theory based on different postulates of a more geometrical nature. This approach considers space and time as being different components of a single entity, the spacetime, which is "divided" in different ways by observers in relative motion. Likewise, energy and momentum are the components of the four-momentum, and the electric and magnetic field are the components of the electromagnetic tensor. As Galilean relativity is now considered an approximation of special relativity valid for low speeds, special relativity is considered an approximation of the theory of general relativity valid for weak gravitational fields. General relativity postulates that physical laws should appear the same to "all" observers (an accelerating frame of reference being equivalent to one in which a gravitational field acts), and that gravitation is the effect of the curvature of spacetime caused by energy (including mass). Reference frames and Galilean relativity: a classical prelude. A reference frame is simply a selection of what constitutes a stationary object. Once the velocity of a certain object is arbitrarily defined to be zero, the velocity of everything else in the universe can be measured relative to that object. One oft-used example is the difference in measurements of objects on a train as made by an observer on the train compared to those made by one standing on a nearby platform as it passes. Consider the seats on the train car in which the passenger observer is sitting. The distances between these objects and the passenger observer do not change. Therefore, this observer measures all of the seats to be at rest, since he is stationary from his own perspective. The observer standing on the platform would see exactly the same objects but interpret them very differently. The distances between themself and the seats on the train car are changing, and so they conclude that they are moving forward, as is the whole train. Thus for one observer the seats are at rest, while for the other the seats are moving, and both are correct, since they are using different definitions of "at rest" and "moving". Each observer has a distinct "frame of reference" in which velocities are measured, "the rest frame of the platform" and "the rest frame of the train" – or simply "the platform frame" and "the train frame". Why can't we select one of these frames to be the "correct" one? Or more generally, why is there not a frame we can select to be the basis for all measurements, an "absolutely stationary" frame? Aristotle imagined the Earth lying at the centre of the universe (the geocentric model), unmoving as other objects moved about it. In this worldview, one could select the surface of the Earth as the absolute frame. However, as the geocentric model was challenged and finally fell in the 1500s, it was realised that the Earth was not stationary at all, but both rotating on its axes as well as orbiting the Sun. In this case the Earth is clearly not the absolute frame. But perhaps there is some other frame one could select, perhaps the Sun's? Galileo challenged this idea and argued that the concept of an absolute frame, and thus absolute velocity, was unreal; all motion was relative. Galileo gave the common-sense "formula" for adding velocities: if In modern terms, we expand the application of this concept from velocity to all physical measurements – according to what we now call the Galilean transformation, there is no absolute frame of reference. An observer on the train has no measurement that distinguishes whether the train is moving forward at a constant speed, or the platform is moving backwards at that same speed. The only meaningful statement is that the train and platform are moving relative to each other, and any observer can choose to define what constitutes a speed equal to zero. When considering trains moving by platforms it is generally convenient to select the frame of reference of the platform, but such a selection would "not" be convenient when considering planetary motion and is not intrinsically more valid. One can use this formula to explore whether or not any possible measurement would remain the same in different reference frames. For instance, if the passenger on the train threw a ball forward, he would measure one velocity for the ball, and the observer on the platform another. After applying the formula above, though, both would agree that the velocity of the ball is the same once corrected for a different choice of what speed is considered zero. This means that motion is "invariant". Laws of classical mechanics, like Newton's second law of motion, all obey this principle because they have the same form after applying the transformation. As Newton's law involves the derivative of velocity, any constant velocity added in a Galilean transformation to a different reference frame contributes nothing (the derivative of a constant is zero). This means that the Galilean transformation and the addition of velocities only apply to frames that are moving at a constant velocity. Since objects tend to retain their current velocity due to a property we call inertia, frames that refer to objects with constant speed are known as inertial reference frames. The Galilean transformation, then, does not apply to accelerations, only velocities, and classical mechanics is "not" invariant under acceleration. This mirrors the real world, where acceleration is easily distinguishable from smooth motion in any number of ways. For example, if an observer on a train saw a ball roll backward off a table, he would be able to infer that the train was accelerating forward, since the ball remains at rest unless acted upon by an external force. Therefore, the only explanation is that the train has moved underneath the ball, resulting in an apparent motion of the ball. Addition of a time-varying velocity, corresponding to an accelerated reference frame, changed the formula (see pseudo-force). Both the Aristotelian and Galilean views of motion contain an important assumption. Motion is defined as the change of position over time, but both of these quantities, position and time, are not defined within the system. It is assumed, explicitly in the Greek worldview, that space and time lie outside physical existence and are absolute even if the objects within them are measured relative to each other. The Galilean transformations can only be applied because both observers are assumed to be able to measure the same time and space, regardless of their frames' relative motions. So in spite of there being no absolute motion, it is assumed there is some, perhaps unknowable, absolute space and time. Classical physics and electromagnetism. Through the era between Newton and around the start of the 20th century, the development of classical physics had made great strides. Newton's application of the inverse square law to gravity was the key to unlocking a wide variety of physical events, from heat to light, and calculus made the direct calculation of these effects tractable. Over time, new mathematical techniques, notably the Lagrangian, greatly simplified the application of these physical laws to more complex problems. As electricity and magnetism were better explored, it became clear that the two concepts were related. Over time, this work culminated in Maxwell's equations, a set of four equations that could be used to calculate the entirety of electromagnetism. One of the most interesting results of the application of these equations was that it was possible to construct a self-sustaining wave of electrical and magnetic fields that could propagate through space. When reduced, the math demonstrated that the speed of propagation was dependent on two universal constants, and their ratio was the speed of light. Light was an electromagnetic wave. Under the classic model, waves are displacements within a medium. In the case of light, the waves were thought to be displacements of a special medium known as the luminiferous aether, which extended through all space. This being the case, light travels in its own frame of reference, the frame of the aether. According to the Galilean transform, we should be able to measure the difference in velocities between the aether's frame and any other – a universal frame at last. Designing an experiment to actually carry out this measurement proved very difficult, however, as the speeds and timing involved made accurate measurement difficult. The measurement problem was eventually solved with the Michelson–Morley experiment. To everyone's surprise, no relative motion was seen. Either the aether was travelling at the same velocity as the Earth, difficult to imagine given the Earth's complex motion, or there was no aether. Follow-up experiments tested various possibilities, and by the start of the 20th century it was becoming increasingly difficult to escape the conclusion that the aether did not exist. These experiments all showed that light simply did not follow the Galilean transformation. And yet it was clear that physical objects emitted light, which led to unsolved problems. If one were to carry out the experiment on the train by "throwing light" instead of balls, if light does not follow the Galilean transformation then the observers should "not" agree on the results. Yet it was apparent that the universe disagreed; physical systems known to be at great speeds, like distant stars, had physics that were as similar to our own as measurements allowed. Some sort of transformation had to be acting on light, or better, a single transformation for both light and matter. The development of a suitable transformation to replace the Galilean transformation is the basis of special relativity. Invariance of length: the Euclidean picture. In special relativity, space and time are joined into a unified four-dimensional continuum called spacetime. To gain a sense of what spacetime is like, we must first look at the Euclidean space of classical Newtonian physics. This approach to explaining the theory of special relativity begins with the concept of "length". In everyday experience, it seems that the length of objects remains the same no matter how they are rotated or moved from place to place; as a result the simple length of an object doesn't appear to change or is invariant. However, as is shown in the illustrations below, what is actually being suggested is that length seems to be invariant in a three-dimensional coordinate system. The length of a line in a two-dimensional Cartesian coordinate system is given by Pythagoras' theorem: One of the basic theorems of vector algebra is that the length of a vector does not change when it is rotated. However, a closer inspection tells us that this is only true if we consider rotations confined to the plane. If we introduce rotation in the third dimension, then we can tilt the line out of the plane. In this case the projection of the line on the plane will get shorter. Does this mean the line's length changes? – obviously not. The world is three-dimensional and in a 3D Cartesian coordinate system the length is given by the three-dimensional version of Pythagoras's theorem: This is invariant under all rotations. The apparent violation of invariance of length only happened because we were "missing" a dimension. It seems that, provided all the directions in which an object can be tilted or arranged are represented within a coordinate system, the length of an object does not change under rotations. With time and space considered to be outside the realm of physics itself, under classical mechanics a 3-dimensional coordinate system is enough to describe the world. Note that invariance of length is not ordinarily considered a "principle" or law, not even a theorem. It is simply a statement about the fundamental nature of space itself. Space as we ordinarily conceive it is called a three-dimensional Euclidean space, because its geometrical structure is described by the principles of Euclidean geometry. The formula for distance between two points is a fundamental property of a Euclidean space, it is called the Euclidean metric tensor (or simply the Euclidean metric). In general, distance formulas are called metric tensors. Note that rotations are fundamentally related to the concept of length. In fact, one may define length or distance to be that which stays the same (is invariant) under rotations, or define rotations to be that which keep the length invariant. Given any one, it is possible to find the other. If we know the distance formula, we can find out the formula for transforming coordinates in a rotation. If, on the other hand, we have the formula for rotations then we can find out the distance formula. The Minkowski formulation: introduction of spacetime. After Einstein derived special relativity formally from the (at first sight counter-intuitive) assumption that the speed of light is the same to all observers, Hermann Minkowski built on mathematical approaches used in non-euclidean geometry and on the mathematical work of Lorentz and Poincaré. Minkowski showed in 1908 that Einstein's new theory could also be explained by replacing the concept of a separate "space and time" with a four-dimensional continuum called "spacetime". This was a groundbreaking concept, and Roger Penrose has said that relativity was not truly complete until Minkowski reformulated Einstein's work. The concept of a four-dimensional space is hard to visualise. It may help at the beginning to think simply in terms of coordinates. In three-dimensional space, one needs three real numbers to refer to a point. In the Minkowski space, one needs four real numbers (three space coordinates and one time coordinate) to refer to a point at a particular instant of time. This point, specified by the four coordinates, is called an event. The distance between two different events is called the spacetime interval. A path through the four-dimensional spacetime (usually known as Minkowski space) is called a world line. Since it specifies both position and time, a particle having a known world line has a completely determined trajectory and velocity. This is just like graphing the displacement of a particle moving in a straight line against the time elapsed. The curve contains the complete motional information of the particle. In the same way as the measurement of distance in 3D space needed all three coordinates, we must include time as well as the three space coordinates when calculating the distance in Minkowski space (henceforth called M). In a sense, the spacetime interval provides a combined estimate of how far apart two events occur in space as well as the time that elapses between their occurrence. But there is a problem; time is related to the space coordinates, but they are not equivalent. Pythagoras' theorem treats all coordinates on an equal footing (see Euclidean space for more details). We can exchange two space coordinates without changing the length, but we can not simply exchange a space coordinate with time – they are fundamentally different. It is an entirely different thing for two events to be separated in space and to be separated in time. Minkowski proposed that the formula for distance needed a change. He found that the correct formula was actually quite simple, differing only by a sign from Pythagoras' theorem: where "c" is a constant and "t" is the time coordinate. Multiplication by "c", which has the dimensions , converts the time to units of length and this constant has the same value as the speed of light. So the spacetime interval between two distinct events is given by There are two major points to be noted. Firstly, time is being measured in the same units as length by multiplying it by a constant conversion factor. Secondly, and more importantly, the time-coordinate has a different sign than the space coordinates. This means that in the four-dimensional spacetime, one coordinate is different from the others and influences the distance differently. This new "distance" may be zero or even negative. This new distance formula, called the metric of the spacetime, is at the heart of relativity. This distance formula is called the metric tensor of M. This minus sign means that a lot of our intuition about distances can not be directly carried over into spacetime intervals. For example, the spacetime interval between two events separated both in time and space may be zero (see below). From now on, the terms distance formula and metric tensor will be used interchangeably, as will be the terms Minkowski metric and spacetime interval. In Minkowski spacetime the spacetime interval is the invariant length, the ordinary 3D length is not required to be invariant. The spacetime interval must stay the same under rotations, but ordinary lengths can change. Just like before, we were missing a dimension. Note that everything thus far is merely definitions. We define a four-dimensional mathematical construct which has a special formula for distance, where distance means that which stays the same under rotations (alternatively, one may define a rotation to be that which keeps the distance unchanged). Now comes the physical part. Rotations in Minkowski space have a different interpretation than ordinary rotations. These rotations correspond to transformations of reference frames. Passing from one reference frame to another corresponds to rotating the Minkowski space. An intuitive justification for this is given below, but mathematically this is a dynamical postulate just like assuming that physical laws must stay the same under Galilean transformations (which seems so intuitive that we don't usually recognise it to be a postulate). Since by definition rotations must keep the distance same, passing to a different reference frame must keep the spacetime interval between two events unchanged. This requirement can be used to derive an explicit mathematical form for the transformation that must be applied to the laws of physics (compare with the application of Galilean transformations to classical laws) when shifting reference frames. These transformations are called the Lorentz transformations. Just like the Galilean transformations are the mathematical statement of the principle of Galilean relativity in classical mechanics, the Lorentz transformations are the mathematical form of Einstein's principle of relativity. Laws of physics must stay the same under Lorentz transformations. Maxwell's equations and Dirac's equation satisfy this property, and hence they are relativistically correct laws (but classically incorrect, since they don't transform correctly under Galilean transformations). With the statement of the Minkowski metric, the common name for the distance formula given above, the theoretical foundation of special relativity is complete. The entire basis for special relativity can be summed up by the geometric statement "changes of reference frame correspond to rotations in the 4D Minkowski spacetime, which is defined to have the distance formula given above". The unique dynamical predictions of SR stem from this geometrical property of spacetime. Special relativity may be said to be the physics of Minkowski spacetime. In this case of spacetime, there are six independent rotations to be considered. Three of them are the standard rotations on a plane in two directions of space. The other three are rotations in a plane of both space and time: These rotations correspond to a change of velocity, and the Minkowski diagrams devised by him describe such rotations. As has been mentioned before, one can replace distance formulas with rotation formulas. Instead of starting with the invariance of the Minkowski metric as the fundamental property of spacetime, one may state (as was done in classical physics with Galilean relativity) the mathematical form of the Lorentz transformations and require that physical laws be invariant under these transformations. This makes no reference to the geometry of spacetime, but will produce the same result. This was in fact the traditional approach to SR, used originally by Einstein himself. However, this approach is often considered to offer less insight and be more cumbersome than the more natural Minkowski formalism. Reference frames and Lorentz transformations: relativity revisited. Changes in reference frame, represented by velocity transformations in classical mechanics, are represented by rotations in Minkowski space. These rotations are called Lorentz transformations. They are different from the Galilean transformations because of the unique form of the Minkowski metric. The Lorentz transformations are the relativistic equivalent of Galilean transformations. Laws of physics, in order to be relativistically correct, must stay the same under Lorentz transformations. The physical statement that they must be the same in all inertial reference frames remains unchanged, but the mathematical transformation between different reference frames changes. Newton's laws of motion are invariant under Galilean rather than Lorentz transformations, so they are immediately recognisable as non-relativistic laws and must be discarded in relativistic physics. The Schrödinger equation is also non-relativistic. Maxwell's equations are written using vectors and at first glance appear to transform correctly under Galilean transformations. But on closer inspection, several questions are apparent that can not be satisfactorily resolved within classical mechanics (see History of special relativity). They are indeed invariant under Lorentz transformations and are relativistic, even though they were formulated before the discovery of special relativity. Classical electrodynamics can be said to be the first relativistic theory in physics. To make the relativistic character of equations apparent, they are written using four-component vector-like quantities called four-vectors. Four-vectors transform correctly under Lorentz transformations, so equations written using four-vectors are inherently relativistic. This is called the manifestly covariant form of equations. Four-vectors form a very important part of the formalism of special relativity. Einstein's postulate: the constancy of the speed of light. Einstein's postulate that the speed of light is a constant comes out as a natural consequence of the Minkowski formulation. Proposition 1: Proof: Proposition 2: Proof: The paths of light rays have a zero spacetime interval, and hence all observers will obtain the same value for the speed of light. Therefore, when assuming that the universe has four dimensions that are related by Minkowski's formula, the speed of light appears as a constant, and does not need to be assumed (postulated) to be constant as in Einstein's original approach to special relativity. Clock delays and rod contractions: more on Lorentz transformations. Another consequence of the invariance of the spacetime interval is that clocks will appear to go slower on objects that are moving relative to the observer. This is very similar to how the 2D projection of a line rotated into the third-dimension appears to get shorter. Length is not conserved simply because we are ignoring one of the dimensions. Let us return to the example of John and Bill. John observes the length of Bill's spacetime interval as: whereas Bill doesn't think he has traveled in space, so writes: The spacetime interval, "s"2, is invariant. It has the same value for all observers, no matter who measures it or how they are moving in a straight line. This means that Bill's spacetime interval equals John's observation of Bill's spacetime interval so: and hence So, if John sees a clock that is at rest in Bill's frame record one second, John will find that his own clock measures between these same ticks an interval "t", called coordinate time, which is greater than one second. It is said that clocks in motion slow down, relative to those on observers at rest. This is known as "relativistic time dilation of a moving clock". The time that is measured in the rest frame of the clock (in Bill's frame) is called the proper time of the clock. In special relativity, therefore, changes in reference frame affect time also. Time is no longer absolute. There is no universally correct clock; time runs at different rates for different observers. Similarly it can be shown that John will also observe measuring rods at rest on Bill's planet to be shorter in the direction of motion than his own measuring rods. This is a prediction known as "relativistic length contraction of a moving rod". If the length of a rod at rest on Bill's planet is "X", then we call this quantity the proper length of the rod. The length "x" of that same rod as measured on John's planet, is called coordinate length, and given by These two equations can be combined to obtain the general form of the Lorentz transformation in one spatial dimension: or equivalently: where the Lorentz factor is given by The above formulas for clock delays and length contractions are special cases of the general transformation. Alternatively, these equations for time dilation and length contraction (here obtained from the invariance of the spacetime interval), can be "obtained" directly "from" the Lorentz transformation by setting for time dilation, meaning that the clock is at rest in Bill's frame, or by setting for length contraction, meaning that John must measure the distances to the end points of the moving rod at the same time. A consequence of the Lorentz transformations is the modified velocity-addition formula: Simultaneity and clock desynchronisation. The last consequence of Minkowski's spacetime is that clocks will appear to be out of phase with each other along the length of a moving object. This means that if one observer sets up a line of clocks that are all synchronised so they all read the same time, then another observer who is moving along the line at high speed will see the clocks all reading different times. This means that observers who are moving relative to each other see different events as simultaneous. This effect is known as "Relativistic Phase" or the "Relativity of Simultaneity". Relativistic phase is often overlooked by students of special relativity, but if it is understood, then phenomena such as the twin paradox are easier to understand. Observers have a set of simultaneous events around them that they regard as composing the present instant. The relativity of simultaneity results in observers who are moving relative to each other having different sets of events in their present instant. The net effect of the four-dimensional universe is that observers who are in motion relative to you seem to have time coordinates that lean over in the direction of motion, and consider things to be simultaneous that are not simultaneous for you. Spatial lengths in the direction of travel are shortened, because they tip upwards and downwards, relative to the time axis in the direction of travel, akin to a skew or shear of three-dimensional space. Great care is needed when interpreting spacetime diagrams. Diagrams present data in two dimensions, and cannot show faithfully how, for instance, a zero length spacetime interval appears. General relativity: a peek forward. Unlike Newton's laws of motion, relativity is not based upon dynamical postulates. It does not assume anything about motion or forces. Rather, it deals with the fundamental nature of spacetime. It is concerned with describing the geometry of the backdrop on which all dynamical phenomena take place. In a sense therefore, it is a meta-theory, a theory that lays out a structure that all other theories must follow. In truth, special relativity is only a special case. It assumes that spacetime is flat. That is, it assumes that the structure of Minkowski space and the Minkowski metric tensor is constant throughout. In general relativity, Einstein showed that this is not true. The structure of spacetime is modified by the presence of matter. Specifically, the distance formula given above is no longer generally valid except in space free from mass. However, just like a curved surface can be considered flat in the infinitesimal limit of calculus, a curved spacetime can be considered flat at a small scale. This means that the Minkowski metric written in the differential form is generally valid. One says that the Minkowski metric is valid locally, but it fails to give a measure of distance over extended distances. It is not valid globally. In fact, in general relativity the global metric itself becomes dependent on the mass distribution and varies through space. The central problem of general relativity is to solve the famous Einstein field equations for a given mass distribution and find the distance formula that applies in that particular case. Minkowski's spacetime formulation was the conceptual stepping stone to general relativity. His fundamentally new outlook allowed not only the development of general relativity, but also to some extent quantum field theories. Mass–energy equivalence. As we increase an object's energy by accelerating it, such that its speed approaches the speed of light from an observer's point of view, its total (relativistic) mass increases, thereby making it more and more difficult to accelerate it from within the observer's frame of reference. This ultimately leads to the concept of mass-energy equivalence. Any object that has mass when at rest (in a given inertial frame of reference), equivalently has "rest energy" as can be calculated using Einstein's equation "E"="mc"2. Rest energy, being a form of energy, is interconvertible with other forms of energy. As with any energy transformation, the total amount of energy does not increase or decrease in such a process. From this perspective, the amount of matter in the universe contributes to its total energy. Similarly, the total of amount of energy of any system also manifests as an equivalent total amount of mass, not limited to the case of the relativistic mass of a moving body. For example, adding 25 kilowatt-hours (90 megajoules) of "any" form(s) of energy to an object increases its mass by 1 microgram. If you had a sensitive enough mass balance or scale, this mass increase could be measured. Our Sun (or a nuclear bomb) converts nuclear potential energy to other forms of energy; its total mass doesn't decrease due to that in itself because it still contains the same total energy in different forms, but its mass does decrease when the energy escapes out to its surroundings, largely as radiant energy. Applications. There is a common perception that relativistic physics is not needed for practical purposes or in everyday life. This is not true. Without relativistic effects, gold would look silvery, rather than yellow. Many technologies are critically dependent on relativistic physics: The postulates of special relativity. Einstein developed special relativity on the basis of two postulates: Special relativity can be derived from these postulates, as was done by Einstein in 1905. Einstein's postulates are still applicable in the modern theory but the origin of the postulates is more explicit. It was shown above how the existence of a universally constant velocity (the speed of light) is a consequence of modeling the universe as a particular four-dimensional space having certain specific properties. The principle of relativity is a result of Minkowski structure being preserved under Lorentz transformations, which are "postulated" to be the physical transformations of inertial reference frames [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@58777339 [docID]1602594 [hitPos]9 [correct]false [extraScores][F@57fcc98b , [answer]Constant acceleration is a proposed form of space travel. It entails that the propulsion system of whatever kind operates continuously with a constant thrust — for the first half of the journey it constantly pushes the spacecraft towards its destination, and for the last half of the journey it constantly uses backthrust, so that the spaceship arrives at the destination at a standstill. Constant-acceleration drives. Constant acceleration is notable for several reasons: Interplanetary travel. Using a chemical rocket, even within the solar system a constant acceleration is not practical due to fuel limitations. Therefore spacecraft with chemical rockets use the boost-and-coast method, which results in long journey times. However, with other rockets mankind is close to employing constant acceleration technologies to journeys around the solar system (though with even longer journey times). One example of this is the VASIMR propulsion system currently being developed by NASA and former astronaut Franklin Chang-Diaz. The current implementations have high fuel efficiencies but feeble thrust. But when drives can deliver constant accelerations in the .1G to .5G range, journeys between planets will take days not years. Interstellar travel. Over interstellar distances a spaceship using significant constant acceleration will approach the speed of light, so special relativity effects become important such as the difference in time flow between ship time and planetary time. Feasibility. Humans are currently not launching spaceships to the stars because doing so is too difficult and too expensive with current technology. Constant acceleration drives are not an exception to this fact. A major limiting factor for constant acceleration drives is having enough fuel. Imagine a horse strong enough to pull a wagon carrying enough hay to feed it on a journey from New York City to Los Angeles. Constant acceleration won't be feasible until the specific impulse for fuel (in layman's terms, the fuel's economy) has become much higher. There are two broad categories for ways to solve this problem: one is higher efficiency fuel (the motor ship approach) and the other is drawing propulsion energy from the environment as the ship passes through it (the sailing ship approach). Two possibilities for the motor ship approach are nuclear and matter-antimatter based fuels. One possibility for the sailing ship approach is discovering something equivalent to the parallelogram of forces between wind and water which allows sails to propel a sailing ship. Picking up fuel along the way—the ramjet approach—will lose efficiency as the space craft's speed increases relative to the planetary reference. This happens because the fuel must be accelerated to the spaceship's velocity before its energy can be extracted and that will cut the fuel efficiency dramatically. A related issue is drag. If the near light speed space craft is interacting with matter or energy that is moving slowly in the planetary reference frame—solar wind, magnetic fields, cosmic microwave background radiation—this will cause drag which will bleed off a portion of the engine's acceleration. A second big issue facing ships using constant acceleration for interstellar travel is colliding with matter and radiation while en route. In mid-journey any matter the ship strikes will be impacting at near light speed, so the impact will be dramatic. These are big issues. They won't be solved quickly or easily. Interstellar traveling speeds. If a space ship is using constant acceleration over interstellar distances, it will approach the speed of light for the middle part of its journey when viewed from the planetary frame of reference. This means that the interesting effects of relativity will become important. The most important effect is that time will appear to pass at different rates in the ship frame and the planetary frame, and this means that the ship's speed and journey time will appear different in the two frames. Planetary reference frame. From the planetary frame of reference, the ship's speed will appear to be limited by the speed of light—it can approach the speed of light, but never reach it. If a ship is using 0.5g constant acceleration or greater, it will appear to get near the speed of light in about a year, and have traveled about half a light year in distance. For the middle of the journey the ship's speed will be roughly the speed of light, and it will slow down again to zero over a year at the end of the journey. As a rule of thumb, a constant acceleration ship journey time will be the distance in light years to the destination, plus one year. This rule of thumb will give answers that are shorter than the correct answer, but reasonably accurate no matter what the G force is as long as it is above, say, a half G. Ship reference frame. From the frame of reference of those on the ship the acceleration will not change as the journey goes on. Instead the planetary reference frame will look more and more relativistic. This means that for voyagers on the ship the journey will appear to be much shorter than what planetary observers see. This is something many readers don't understand well, so it bears repeating: The journey times as experienced by those on the ship are not limited by the speed of light. Instead what they experience is the planetary reference frame getting relativistic. A Half Myth: It gets harder to push a ship faster as it gets closer to the speed of light. This is a half myth because it depends on the frame of reference. This is true for those watching from the planetary reference frame. For those experiencing the journey-those in the ship reference frame-this is not true. For both the planetary frame, and in the ship reference frame the ship will change speed in a Newtonian way—push it a little and it speeds up a little, push it a lot and it speeds up a lot. However, in the planetary frame the ship will appear to be gaining mass due to its high kinetic energy, and the Mass-energy equivalence principle. Should the engines be giving a constant thrust, this will result in progressively smaller acceleration due to the higher mass it is required to accelerate. From the ship's frame, the acceleration would continue at the same rate. However, due to the Lorentz contraction The galaxy around the ship would appear to become squashed in the direction of travel, and a destination many light years away, would appear to become much closer. Traveling to this destination at sub luminal speeds would become practical for the onboard travellers. Ultimately, from the ships frame, it would be possible to reach anywhere in the visible universe, before the ship has time to accelerate to light speed. In fiction. "Tau Zero", a hard science fiction novel by Poul Anderson, has a spaceship using a constant acceleration drive. Spacecraft in Joe Haldeman's novel "The Forever War" make extensive use of constant acceleration; they require elaborate safety equipment to keep their occupants alive at high acceleration (up to 25 G), and accelerate at 1 G even when "at rest" to provide humans with a comfortable level of gravity. 'In the "The Known Space" Universe constructed by Larry Niven spaceships use constant acceleration drives in the form of a bussard ramjet for interstellar travel. In "The Sparrow," by Mary Doria Russell, interstellar travel is achieved by converting a small asteroid into a constant acceleration spacecraft. Force is applied by ion engines fed with material mined from the asteroid itself. In the Revelation Space series by Alastair Reynolds, interstellar commerce depends upon "lighthugger" starships which can accelerate indefinitely at 1G. The effects of relativistic travel are an important plot point in several stories, informing the psychologies and politics of the lighthuggers' "ultranaut" crews for example [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@12925ecb [docID]26957755 [hitPos]10 [correct]false [extraScores][F@7af77265 , [answer]The relativistic Doppler effect is the change in frequency (and wavelength) of light, caused by the relative motion of the source and the observer (as in the classical Doppler effect), when taking into account effects described by the special theory of relativity. The relativistic Doppler effect is different from the non-relativistic Doppler effect as the equations include the time dilation effect of special relativity and do not involve the medium of propagation as a reference point. They describe the total difference in observed frequencies and possess the required Lorentz symmetry. Visualization. In Diagram 2, the blue point represents the observer, and the arrow represents the observer's velocity vector. When the observer is stationary, the "x","y"-grid appears yellow and the "y"-axis appears as a black vertical line. Increasing the observer's velocity to the right shifts the colors and the aberration of light distorts the grid. When the observer looks forward (right on the grid), points appear green, blue, and violet (blueshift) and grid lines appear farther apart. If the observer looks backward (left on the grid), then points appear red (redshift) and lines appear closer together. The grid has not changed, but its "appearance" for the observer has. Diagram 3 illustrates that the grid distortion is a relativistic optical effect, separate from the underlying Lorentz contraction which is the same for an object moving toward an observer or away. Analogy. Understanding relativistic Doppler effect requires understanding the Doppler effect, time dilation, and the aberration of light. As a simple analogy of the Doppler effect, consider two people playing catch. Imagine that a stationary pitcher tosses one ball each second (1 Hz) at one meter per second to a catcher who is standing still. The stationary catcher will receive one ball per second (1 Hz). Then the catcher walks away from the pitcher at 0.5 meters per second and catches a ball every 2 seconds (0.5 Hz). Finally, the catcher walks towards the pitcher at 0.5 meters per second and catches three balls every two seconds (1.5 Hz). The same would be true if the pitcher moved toward or away from the catcher. By analogy, the relativistic Doppler effect shifts the frequency of light as the emitter or observer moves toward or away from the other. To understand the aberration effect, again imagine two people playing catch on two parallel conveyor belts (moving sidewalks) moving in opposite direction. The pitcher must aim differently depending on the speed and the spacing of the belts, and where the catcher is. The catcher will see the balls coming at a different angle than the pitcher chose to throw them. These angle changes depend on: 1) the instantaneous angle between the pitcher-catcher line and the relative velocity vector, and 2) the pitcher-catcher velocity relative to the speed of the ball. By analogy, the aberration of light depends on: 1) the instantaneous angle between the emitter-observer line and the relative velocity vector, and 2) the emitter-observer velocity relative to the speed of light. Motion along the line of sight. Assume the observer and the source are moving "away" from each other with a relative velocity formula_1 (formula_1 is "negative" if the observer and the source are moving "toward" each other). Considering the problem in the reference frame of the source, suppose one wavefront arrives at the observer. The next wavefront is then at a distance formula_3 away from him (where formula_4 is the wavelength, formula_5 is the frequency of the wave the source emitted, and formula_6 is the speed of light). Since the wavefront moves with velocity formula_6 and the observer escapes with velocity formula_8, the time (as measured in the reference frame of the source) between crest arrivals at the observer is where formula_10 is the velocity of the observer in terms of the speed of light (see beta (velocity)). Due to the relativistic time dilation, the observer will measure this time to be where is the Lorentz factor. The corresponding observed frequency is The ratio is called the Doppler factor of the source relative to the observer. (This terminology is particularly prevalent in the subject of astrophysics: see relativistic beaming.) The corresponding wavelengths are related by and the resulting redshift can be written as In the non-relativistic limit (when formula_18) this redshift can be approximated by corresponding to the classical Doppler effect. Systematic derivation for inertial observers. Let us repeat the derivation more systematically in order to show how the Lorentz equations can be used explicitly to derive a relativistic Doppler shift equation for waves that themselves are not relativistic. Let there be two inertial frames of reference, formula_20 and formula_21, constructed so that the axes of formula_20 and formula_21 coincide at formula_24, where formula_25 is the time as measured in formula_20 and formula_27 is the time as measured in formula_21. Let formula_21 be in motion relative to formula_20 with constant velocity formula_8; without loss of generality, we will take this motion to be directed only along the x-axis. Thus, the Lorentz transformation equations take the form where formula_32 and formula_33, and formula_34 is the speed of light in a vacuum. The derivation begins with what the observer in formula_21 trivially sees. We imagine a signal source is positioned stationary at the origin, formula_36, of the formula_21 system. We will take this signal source to produce its first pulse at time formula_38 (this is "event 1") and its second pulse at time formula_39 (this is "event 2"), where formula_40 is the frequency of the signal source as the observer in formula_21 sees it. We then simply use the Lorentz transformation equations to see when and where the observer in formula_20 sees these two events as occurring: The period between the pulses as measured by the formula_20 observer is "not", however, formula_44 because "event 2" occurs at a different point in space to "event 1" as observed by the formula_20 observer (that is, formula_46) — we must factor in the time taken for the pulse to travel from formula_47 to formula_48. Note that this complication is "not" relativistic in nature: this is the ultimate cause of the Doppler effect and is also present in the classical treatment. This transit time is equal to the difference formula_49 divided by the speed of the pulse as the formula_20 observer sees it. If the pulse moves at speed formula_51 in formula_21 (negative because it moves in the negative x-direction, towards the formula_20 observer at formula_54), then the speed of the pulse moving "towards" the observer at formula_54, as formula_20 sees it, is: using the Lorentz equation for the velocities, above. Thus, the period between the pulses that the observer in formula_20 measures is: Replacing formula_59 with formula_60 and simplifying, we get the required result that gives the relativistic Doppler shift of "any" moving wave in terms of the stationary frequency, formula_40: Ignoring the relativistic effects by taking formula_18 or formula_64 (equivalent to formula_65) gives the classical Doppler formula: For electromagnetic radiation where formula_67 the formula becomes or in terms of wavelength: where formula_70 is the wavelength of the source at the origin formula_36 as the observer in formula_21 sees it. For electromagnetic radiation, the limit to classical mechanics, formula_64, is instructive. The Doppler effect formula simply becomes formula_74. This is the correct result for classical mechanics, although it is clearly in disagreement with experiment. It is correct since classical mechanics regards the maximum speed of interaction — for electrodynamics, the speed of light — to be infinite. The Doppler effect, classical or relativistic, occurs because the wave source has time to move by the time that previous waves encounter the observer. This means that the subsequent waves are emitted further away (or closer) to the observer than they otherwise would be if the source were not in motion. The effect of this is to stretch (or compress) the wavelength of the wave as the observer encounters them. If however the waves travel instantaneously, the fact that the source is further away (or closer) makes no difference because the waves arrive at the observer no later or earlier than they would anyway since they arrive instantaneously. Thus, classical mechanics predicts that there should be no Doppler effect for light waves, whereas the relativistic theory gives the correct answer, as confirmed by experiment. Transverse Doppler effect. The "transverse Doppler effect" is the nominal redshift or blueshift predicted by special relativity that occurs when the emitter and receiver are at the point of closest approach. Light emitted at this instant will be redshifted. Light received at this instant will be blueshifted. Assuming the objects are not accelerated, light emitted when the objects are closest together will be received some time later, at reception the amount of redshift will be Light received when the objects are closest together was emitted some time earlier, at reception the amount of blueshift is Classical theory does not make a specific prediction for either of these two cases, as the shift depends on the motions relative to the medium. The transverse Doppler effect is a consequence of the relativistic Doppler effect. In the frame of the receiver, θ0 represents the angle between the direction of the emitter at emission, and the observed direction of the light at reception. In the case when θ0 = π/2, the light was emitted at the moment of closest approach, and one obtains the transverse redshift The transverse Doppler effect is one of the main novel predictions of the special theory. As Einstein put it in 1907: according to special relativity the moving object's emitted frequency is reduced by the Lorentz factor, so that the received frequency is reduced by the same factor. Reciprocity. Sometimes the question arises as to how the transverse Doppler effect can lead to a redshift as seen by the "observer" whilst another observer moving with the emitter would "also" see a redshift of light sent (perhaps accidentally) from the receiver. It is essential to understand that the concept "transverse" is not reciprocal. Each participant understands that when the light reaches him transversely as measured in terms of that person's rest frame, the other had emitted the light afterward as measured in the other person's rest frame. In addition, each participant measures the other's frequency as reduced ("time dilation"). These effects combined make the observations fully reciprocal, thus obeying the principle of relativity. Experimental verification. In practice, experimental verification of the transverse effect usually involves looking at the longitudinal changes in frequency or wavelength due to motion for approach and recession: by comparing these two ratios together we can rule out the relationships of "classical theory" and prove that the real relationships are "redder" than those predictions. The transverse Doppler shift is central to the interpretation of the peculiar astrophysical object SS 433. The first longitudinal experiments were carried out by Herbert E. Ives and Stilwell in (1938), and many other longitudinal tests have been performed since with much higher precision. Also a direct transverse experiment has verified the redshift effect for a detector actually "aimed" at 90 degrees to the object. Motion in an arbitrary direction. If, in the reference frame of the observer, the source is moving away with velocity formula_1 at an angle formula_80 relative to the direction from the observer to the source (at the time when the light is emitted), the frequency changes as In the particular case when formula_82 and formula_83 one obtains the transverse Doppler effect: Due to the finite speed of light, the light ray (or photon, if you like) perceived by the observer as coming at angle formula_85, was, in the reference frame of the source, emitted at a different angle formula_86. formula_87 and formula_88 are tied to each other via the relativistic aberration formula: Therefore, Eq. (1) can be rewritten as For example, a photon emitted at the right angle in the reference frame of the emitter (formula_91) would be seen blue-shifted by the observer: In the non-relativistic limit, both formulæ (1) and (2) give Accelerated motion. For general accelerated motion, or when the motions of the source and receiver are analyzed in an arbitrary inertial frame, the distinction between source and emitter motion must again be taken into account. The Doppler shift when observed from an arbitrary inertial frame: formula_94 where: If formula_97 is parallel to formula_95, then formula_102, which causes the frequency measured by the receiver formula_103 to increase relative to the frequency emitted at the source formula_104. Similarly, if formula_97 is anti-parallel to formula_95, formula_107, which causes the frequency measured by the receiver formula_103 to decrease relative to the frequency emitted at the source formula_104. This is the classical Doppler effect multiplied by the ratio of the receiver and source Lorentz factors. Due to the possibility of refraction, the light's direction at emission is generally not the same as its direction at reception. In refractive media, the light's path generally deviates from the straight distance between the points of emission and reception. The Doppler effect depends on the component of the emitter's velocity parallel to the light's direction at emission, and the component of the receiver's velocity parallel to the light's direction at absorption. This does not contradict Special Relativity. The transverse Doppler effect can be analyzed from a reference frame where the source and receiver have equal and opposite velocities. In such a frame the ratio of the Lorentz factors is always 1, and all Doppler shifts appear to be classical in origin. In general, the observed frequency shift is an invariant, but the relative contributions of time dilation and the Doppler effect are frame dependent [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@15ab6789 [docID]408026 [hitPos]10 [correct]false [extraScores][F@6bc3bd83 , [answer]The laws of science or scientific laws are statements that describe, predict, and perhaps explain "why", a range of phenomena behave as they appear to in nature. The term "law" has diverse usage in many cases: approximate, accurate, broad or narrow theories, in all natural scientific disciplines (physics, chemistry, biology, geology, astronomy etc.). An analogous term for a scientific law is a principle. Scientific laws: Laws differ from hypotheses and postulates, which are proposed during the scientific process before and during validation by experiment and observation. These are not laws since they have not been verified to the same degree and may not be sufficiently general, although they may lead to the formulation of laws. A law is a more solidified and formal statement, distilled from repeated experiment. Although the nature of a scientific law is a question in philosophy and although scientific laws describe nature mathematically, scientific laws are practical conclusions reached by the scientific method; they are intended to be neither laden with ontological commitments nor statements of logical absolution. Fundamentally, "all" scientific laws follow from physics, laws which occur in other sciences ultimately follow from physical laws. Often, from mathematically fundamental viewpoints, universal constants emerge from scientific laws. Conservation laws. Conservation and symmetry. Most significant laws in science are conservation laws. These fundamental laws follow from homogeneity of space, time and phase, in other words "symmetry". Continuity and transfer. Conservation laws can be expressed using the general continuity equation (for a conserved quantity) can be written in differential form as: where ρ is some quantity per unit volume, J is the flux of that quantity (change in quantity per unit time per unit area). Intuitively, the divergence (denoted ∇•) of a vector field is a measure of flux diverging radially outwards from a point, so the negative is the amount piling up at a point, hence the rate of change of density in a region of space must be the amount of flux leaving or collecting in some region (see main article for details). In the table below, the fluxes, flows for various physical quantities in transport, and their associated continuity equations, are collected for comparison. More general equations are the convection–diffusion equation and Boltzmann transport equation, which have their roots in the continuity equation. Laws of classical mechanics. Principle of least action. All of classical mechanics, including Newton's laws, Lagrange's equations, Hamilton's equations, etc., can be derived from this very simple principle: where formula_3 is the action; the integral of the Lagrangian of the physical system between two times "t"1 and "t"2. The kinetic energy of the system is "T" (a function of the rate of change of the configuration of the system), and potential energy is "V" (a function of the configuration and its rate of change). The configuration of a system which has "N" degrees of freedom is defined by generalized coordinates q = ("q"1, "q"2, ... "qN"). There are generalized momenta conjugate to these coordinates, p = ("p"1, "p"2, ..., "pN"), where: The action and Lagrangian both contain the dynamics of the system for all times. The term "path" simply refers to a curve traced out by the system in terms of the generalized coordinates in the configuration space, i.e. the curve q("t"), parameterized by time (see also parametric equation for this concept). The action is a "functional" rather than a "function", since it depends on the Lagrangian, and the Lagrangian depends on the path q("t"), so the action depends on the "entire" "shape" of the path for all times (in the time interval from "t"1 to "t"2). Between two instants of time, there are infinitely many paths, but one for which the action is stationary (to first order) is the true path. The stationary value for the "entire continuum" of Lagrangian values corresponding to some path, "not just one value" of the Lagrangian, is required (in other words its "not" as simple as "differentiating a function and setting it to zero, then solving the equations to find the points of maxima and minima etc", rather this idea is applied to the entire "shape" of the function, see calculus of variations for more details on this procedure). Notice "L" is "not" the total energy "E" of the system due to the difference, rather than the sum: The following general approaches to classical mechanics are summarized below in the order of establishment. They are equivalent formulations, Newton's is very commonly used due to simplicity, but Hamilton's and Lagrange's equations are more general, and their range can extend into other branches of physics with suitable modifications. From the above, any equation of motion in classical mechanics can be derived. Equations describing fluid flow in various situations can be derived, using the above classical equations of motion and often conservation of mass, energy and momentum. Some elementary examples follow. Laws of gravitation and relativity. Modern laws. Postulates of special relativity are not "laws" in themselves, but assumptions of their nature in terms of "relative motion". Often two are stated as "the laws of physics are the same in all inertial frames" and "the speed of light is constant". However the second is redundant, since the speed of light is predicted by Maxwell's equations. Essentially there is only one. The said posulate leads to the Lorentz transformations – the transformation law between two frame of references moving relative to each other. For any 4-vector this replaces the Galilean transformation law from classical mechanics. The Lorentz transformations reduce to the Galilean transformations for low velocities much less than the speed of light "c". The magnitudes of 4-vectors are invariants - "not" "conserved", but the same for all inertial frames (i.e. every observer in an inertial frame will agree on the same value), in particular if "A" is the four-momentum, the magnitude can derive the famous invariant equation for mass-energy and momentum conservation (see invariant mass): in which the (more famous) mass-energy equivalence "E" = "mc"2 is a special case. General relativity is governed by the Einstein field equations, which describe the curvature of space-time due to mass-energy equivalent to the gravitational field. Solving the equation for the geometry of space warped due to the mass distribution gives the metric tensor. Using the geodesic equation, the motion of masses falling along the geodesics can be calculated. In a relatively flat spacetime due to weak gravitational fields, gravitational analogues of Maxwell's equations can be found; the GEM equations, to describe an analogous "gravitomagnetic field". They are well established by the theory, and experimental tests form ongoing research. These equations can be modified to include magnetic monopoles, and are consistent with our observations of monopoles either existing or not existing; if they do not exist, the generalized equations reduce to the ones above, if they do, the equations become fully symmetric in electric and magnetic charges and currents. Indeed there is a duality transformation where electric and magnetic charges can be "rotated into one another", and still satisfy Maxwell's equations. These laws were found before the formulation of Maxwell's equations. They are not fundamental, since they can be derived from Maxwell's Equations. Coulomb's Law can be found from Gauss' Law (electrostatic form) and the Biot-Savart Law can be deduced from Ampere's Law (magnetostatic form). Lenz' Law and Faraday's Law can be incorporated into the Maxwell-Faraday equation. Nonetheless they are still very effective for simple calculations. Photonics. Classically, optics is based on a variational principle: light travels from one point in space to another in the shortest time. In geometric optics laws are based on approximations in Euclidean geometry (such as the paraxial approximation). In physical optics, laws are based on physical properties of materials. In actuality, optical properties of matter are significantly more complex and require quantum mechanics. Laws of quantum mechanics. Quantum mechanics has its roots in postulates, these lead to results which are not usually called "laws", but have the same status, in that all of quantum mechanics follows from them. One postulate that a particle (or a system of many particles) is described by a wavefunction, and this satisfies a quantum wave equation: namely the Schrödinger equation (which can be written as a non-relativistic wave equation, or a relativistic wave equation). Solving this wave equation predicts the time-evolution of the system's behaviour, analogous to solving Newton's laws in classical mechanics. Other postulates change the idea of physical observables; using quantum operators; some measurements can't be made at the same instant of time (Uncertainty principles), particles are fundamentally indistinguishable. Another postulate; the wavefunction collapse postulate, counters the usual idea of a measurement in science. Radiation laws. Applying electromagnetism, thermodynamics, and quantum mechanics, to atoms and molecules, some laws of electromagnetic radiation and light are as follows. Laws of chemistry. Chemical laws are those laws of nature relevant to chemistry. Historically, observations lead to many empirical laws, though now it is known that chemistry has its foundations in quantum mechanics. The most fundamental concept in chemistry is the law of conservation of mass, which states that there is no detectable change in the quantity of matter during an ordinary chemical reaction. Modern physics shows that it is actually energy that is conserved, and that energy and mass are related; a concept which becomes important in nuclear chemistry. Conservation of energy leads to the important concepts of equilibrium, thermodynamics, and kinetics. Additional laws of chemistry elaborate on the law of conservation of mass. Joseph Proust's law of definite composition says that pure chemicals are composed of elements in a definite formulation; we now know that the structural arrangement of these elements is also important. Dalton's law of multiple proportions says that these chemicals will present themselves in proportions that are small whole numbers (i.e. 1:2 for Oxygen:Hydrogen ratio in water); although in many systems (notably biomacromolecules and minerals) the ratios tend to require large numbers, and are frequently represented as a fraction. More modern laws of chemistry define the relationship between energy and its transformations [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@58777339 [docID]71110 [hitPos]10 [correct]false [extraScores][F@1cd56565 , [answer]The Michelson–Morley experiment was performed in 1887 by Albert Michelson and Edward Morley at what is now Case Western Reserve University in Cleveland, Ohio. It attempted to detect the relative motion of matter through the stationary luminiferous aether ("aether wind"). The negative results are generally considered to be the first strong evidence against the then prevalent aether theory, and initiated a line of research that eventually led to special relativity, in which the stationary aether concept has no role. The experiment has been referred to as "the moving-off point for the theoretical aspects of the Second Scientific Revolution". Michelson–Morley type experiments have been repeated many times with steadily increasing sensitivity. These include experiments from 1902 to 1905, and a series of experiments in the 1920s. In addition, recent resonator experiments have confirmed the absence of any aether wind at the 10−17 level. Together with the Ives–Stilwell and Kennedy–Thorndike experiments, the Michelson–Morley experiment forms one of the fundamental tests of special relativity theory. Detecting the aether. Physics theories of the late 19th century assumed that just as surface water waves must have a supporting substance, "i.e." a "medium", to move across (in this case water), and audible sound requires a medium to transmit its wave motions (such as air or water), so light must also require a medium, the "luminiferous aether", to transmit its wave motions. Because light can travel through a vacuum, it was assumed that even a vacuum must be filled with aether. Since the speed of light is so great, and as material bodies pass through the aether without obvious friction or drag, the aether was assumed to have a highly unusual combination of properties. Designing experiments to test the properties of the aether was a high priority of 19th century physics. Earth orbits around the Sun at a speed of around 30 km/s (18.75 mi/s) or over 108,000 km/hr (67,500 mi/hr). The Sun itself is traveling about the Galactic Center at an even greater speed, and there is motion of the galaxy with respect to larger structures in the universe. Since the Earth is in motion, two main possibilities were considered: (1) The aether is stationary and only partially dragged by Earth (proposed by Augustin-Jean Fresnel in 1818), or (2) the aether is completely dragged by Earth and thus shares its motion at Earth's surface (proposed by George Gabriel Stokes in 1844). In addition, James Clerk Maxwell (1865) recognized the electromagnetic nature of light and developed what are now called Maxwell's equations, but these equations were still interpreted as describing the motion of waves through an aether, whose state of motion was unknown. Eventually, Fresnel's idea of an (almost) stationary aether was preferred because it appeared to be confirmed by the Fizeau experiment (1851) and the aberration of star light. According to this hypothesis, Earth and the aether are in relative motion, implying that a so-called "aether wind" (Fig. 2) should exist. Although it would be possible, in theory, for the Earth's motion to match that of the aether at one moment in time, it was not possible for the Earth to remain at rest with respect to the aether at all times, because of the variation in both the direction and the speed of the motion. At any given point on the Earth's surface, the magnitude and direction of the wind would vary with time of day and season. By analyzing the return speed of light in different directions at various different times, it was thought to be possible to measure the motion of the Earth relative to the aether. The expected relative difference in the measured speed of light was quite small, given that the velocity of the Earth in its orbit around the Sun was about one hundredth of one percent of the speed of light. During the mid-19th century it was thought that it should be possible to measure aether wind effects of first order, "i.e." effects proportional to "v/c" ("v" being Earth's velocity, "c" the speed of light). But no direct measurement of the speed of light was possible with the accuracy required. For instance, the Fizeau–Foucault apparatus could measure the speed of light to perhaps 5% accuracy, which was quite inadequate for measuring directly a first-order 0.01% change in the speed of light. A number of physicists therefore attempted to make measurements of indirect first-order effects not of the speed of light itself, but of variations in the speed of light (see First order aether-drift experiments). The Hoek experiment, for example, was intended to detect interferometric fringe shifts due to speed differences of oppositely propagating light waves through water at rest. The results of such experiments were all negative. This could be explained by using Fresnel's dragging coefficient, according to which the aether and thus light are partially dragged by moving matter. Partial aether-dragging would thwart attempts to measure any first order change in the speed of light. As pointed out by Maxwell (1878), only experimental arrangements capable of measuring second order effects would have any hope of detecting aether drift, "i.e." effects proportional to "v"2/"c"2. Existing experimental setups, however, were not sensitive enough to measure effects of that size. 1881 and 1887 experiments. Michelson experiment (1881). Michelson had a solution to the problem of how to construct a device sufficiently accurate to detect aether flow. In 1877, while teaching at his alma mater, the United States Naval Academy in Annapolis, Michelson conducted his first known light speed experiments as a part of a classroom demonstration. In 1881, he left active U.S. Naval service while in Germany concluding his studies. In that year, Michelson used a prototype experimental device to make several more measurements. The device he designed, later known as a Michelson interferometer, sent yellow light from a sodium flame (for alignment), or white light (for the actual observations), through a half-silvered mirror that was used to split it into two beams traveling at right angles to one another. After leaving the splitter, the beams traveled out to the ends of long arms where they were reflected back into the middle by small mirrors. They then recombined on the far side of the splitter in an eyepiece, producing a pattern of constructive and destructive interference whose transverse displacement would depend on the relative time it takes light to transit the longitudinal "vs." the transverse arms. If the Earth is traveling through an aether medium, a beam reflecting back and forth parallel to the flow of aether would take longer than a beam reflecting perpendicular to the aether because the time gained from traveling downwind is less than that lost traveling upwind. Michelson expected that the Earth's motion would produce a fringe shift equal to .04 fringes—that is, of the separation between areas of the same intensity. He did not observe the expected shift; the greatest average deviation that he measured (in the northwest direction) was only 0.018 fringes; most of his measurements were much less. His conclusion was that Fresnel's hypothesis of a stationary aether with partial aether dragging would have to be rejected, and thus he confirmed Stokes' hypothesis of complete aether dragging. However, Alfred Potier (and later Hendrik Lorentz) pointed out to Michelson that he had made an error of calculation, and that the expected fringe shift should have been only 0.02 fringes. Michelson's apparatus was subject to experimental errors far too large to say anything conclusive about the aether wind. For a definitive measurement of the aether wind, a much more accurate and tightly controlled experiment would have to be carried out. Nevertheless the prototype was successful in demonstrating that the basic method was feasible. Michelson–Morley experiment (1887). In 1885, Michelson began a collaboration with Edward Morley, spending considerable time and money to confirm with higher accuracy Fizeau's 1851 experiment on Fresnel's drag coefficient, to improve on Michelson's 1881 experiment, and to establish the wavelength of light as a standard of length. At this time Michelson was professor of physics at the Case School of Applied Science, and Morley was professor of chemistry at Western Reserve University, which shared a campus with the Case School on the eastern edge of Cleveland. Michelson suffered a nervous breakdown in September 1885, from which he recovered by October 1885. Morley ascribed this breakdown to the intense work of Michelson during the preparation of the experiments. In 1886, Michelson and Morley successfully confirmed Fresnel's drag coefficient – this result was also considered as a confirmation of the stationary aether concept. This result strengthened their hope of finding the aether wind. Michelson and Morley created an improved version of the Michelson experiment with more than enough accuracy to detect this hypothetical effect. The experiment was performed in several periods of concentrated observations between April and July 1887, in Adelbert Dormitory of WRU (later renamed Pierce Hall, demolished in 1962). As shown in Fig. 5, the light was repeatedly reflected back and forth along the arms of the interferometer, increasing the path length to 11 m. At this length, the drift would be about 0.4 fringes. To make that easily detectable, the apparatus was assembled in a closed room in the basement of the heavy stone dormitory, eliminating most thermal and vibrational effects. Vibrations were further reduced by building the apparatus on top of a large block of sandstone (Fig. 1), about a foot thick and five feet square, which was then floated in an annular trough of mercury. They estimated that effects of about 1/100 of a fringe would be detectable. Michelson and Morley and other early experimentalists using interferometric techniques in an attempt to measure the properties of the luminiferous aether, used (partially) monochromatic light only for initially setting up their equipment, always switching to white light for the actual measurements. The reason is that measurements were recorded visually. Purely monochromatic light would result in a uniform fringe pattern. Lacking modern means of environmental temperature control, experimentalists struggled with continual fringe drift even though the interferometer might be set up in a basement. Since the fringes would occasionally disappear due to vibrations by passing horse traffic, distant thunderstorms and the like, it would be easy for an observer to "get lost" when the fringes returned to visibility. The advantages of white light, which produced a distinctive colored fringe pattern, far outweighed the difficulties of aligning the apparatus due to its low coherence length. As Dayton Miller wrote, "White light fringes were chosen for the observations because they consist of a small group of fringes having a central, sharply defined black fringe which forms a permanent zero reference mark for all readings." Use of partially monochromatic light (yellow sodium light) during initial alignment enabled the researchers to locate the position of equal path length, more or less easily, before switching to white light. The mercury pool allowed the device to be easily turned, so that given a single steady push, it would slowly rotate inertially through the entire range of possible angles to the "aether wind", while measurements were continuously observed by looking through the eyepiece. Even over a period of minutes, it was presumed that some sort of effect would be noticed, since one of the arms would inevitably turn into the direction of the wind and the other away. It was expected that the effect would be graphable as a sine wave with two peaks and two troughs per rotation of the device. This result could have been expected because during each full rotation, each arm would be parallel to the wind twice (facing into and away from the wind giving identical readings) and perpendicular to the wind twice. Additionally, due to the Earth's rotation, the wind would be expected to show periodic changes in direction and magnitude during the course of a sidereal day. Because of the motion of the Earth around the Sun, it was expected that yearly cycles would also be detectable in the measured data. Most famous "failed" experiment. After all this thought and preparation, the experiment became what has been called the most famous failed experiment in history. Instead of providing insight into the properties of the aether, Michelson and Morley's article in the "American Journal of Science" reported the measurement to be as small as one-fortieth of the expected displacement (see Fig. 7), but "since the displacement is proportional to the square of the velocity" they concluded that the measured velocity was "probably less than one-sixth" of the expected velocity of the Earth's motion in orbit and "certainly less than one-fourth." Although this small "velocity" was measured, it was considered far too small to be used as evidence of speed relative to the aether, and it was understood to be within the range of an experimental error that would allow the speed to actually be zero. (Afterward, Michelson and Morley ceased their aether drift measurements and started to use their newly developed technique to establish the wavelength of light as a standard of length.) From the standpoint of the then current aether models, the experimental results were conflicting. The Fizeau experiment and its 1886 repetition by Michelson and Morley apparently confirmed the stationary aether with partial aether dragging, and refuted complete aether dragging. On the other hand, the much more precise Michelson–Morley experiment (1887) apparently confirmed complete aether dragging and refuted the stationary aether. In addition, the Michelson–Morley null result was further substantiated by the null results of other second-order experiments of different kind, namely the Trouton–Noble experiment (1903) and the Experiments of Rayleigh and Brace (1902–1904). These problems and their solution led to the development of the Lorentz transformation and special relativity. Light path analysis and consequences. Observer resting in the aether. The beam travel time in the longitudinal direction can be derived as follows: Light is sent from the source and propagates with the speed of light formula_1 in the aether. It passes through the half-silvered mirror at the origin at formula_2. The reflecting mirror is at that moment at distance formula_3 (the length of the interferometer arm) and is moving with velocity formula_4. The beam hits the mirror at time formula_5 and thus travels the distance formula_6. At this time, the mirror has traveled the distance formula_7. Thus formula_8 and consequently the travel time formula_9. The same consideration applies to the backward journey, with the sign of formula_4 reversed, resulting in formula_11 and formula_12. The total travel time formula_13 is: Michelson obtained this expression correctly in 1881, however, in transverse direction he obtained the incorrect expression because he overlooked that the aether wind also affects the transverse beam travel time. This was corrected by Alfred Potier (1882) and Lorentz (1886). The derivation in the transverse direction can be given as follows (analoguous to the derivation of time dilation using a light clock): The beam is propagating at the speed of light formula_1 and hits the mirror at time formula_19, traveling the distance formula_20. At the same time, the mirror has traveled the distance formula_21 in x direction. So in order to hit the mirror, the travel path of the beam is formula_3 in y direction (assuming equal-length arms) and formula_21 in the x direction. This inclined travel path follows from the transformation from the interferometer rest frame to the aether rest frame. Therefore the Pythagorean theorem gives the actual beam travel distance of formula_24. Thus formula_25 and consequently the travel time formula_26, which is the same for the backward journey. The total travel time formula_27 is: The time difference between "Tl" and "Tt" before rotation is given by By multiplying with "c", the corresponding length difference before rotation is and after rotation Dividing formula_32 by the wavelength λ, the fringe shift "n" is found: Since "L"≈11 meters and λ≈500 nanometers, the expected fringe shift "n" was ≈0.44. So the result would be a delay in one of the light beams that could be detected when the beams were recombined through interference. Any slight change in the spent time would then be observed as a shift in the positions of the interference fringes. The negative result led Michelson to the conclusion that there is no measurable aether drift. Observer comoving with the interferometer. If the same situation is described from the view of an observer co-moving with the interferometer, then the effect of aether wind is similar to the effect experienced by a swimmer, who tries to move with velocity formula_1 against a river flowing with velocity formula_4. In the longitudinal direction the swimmer first moves upstream, so his velocity is diminished due to the river flow to formula_36. On his way back moving downstream, his velocity is increased to formula_37. This gives the beam travel times formula_5 and formula_39 as mentioned above. In the transverse direction, the swimmer has to compensate for the river flow by moving at a certain angle against the flow direction, in order to sustain his exact transverse direction of motion and to reach the other side of the river at the correct location. This diminishes his speed to formula_40, and gives the beam travel time formula_19 as mentioned above. Mirror reflection. The classical analysis predicted a relative phase shift between the longitudinal and transverse beams which in Michelson and Morley's apparatus should have been readily measurable. What is not often appreciated (since there was no means of measuring it), is that motion through the hypothetical aether should also have caused the two beams to diverge as they emerged from the interferometer by about 10-8 radians. For an apparatus in motion, the classical analysis requires that the beam-splitting mirror be slightly offset from an exact 45° if the longitudinal and transverse beams are to emerge from the apparatus exactly superimposed. In the relativistic analysis, Lorentz-contraction of the beam splitter in the direction of motion causes it to become more perpendicular by precisely the amount necessary to compensate for the angle discrepancy of the two beams. Length contraction and Lorentz transformation. A first step to explaining the Michelson and Morley experiment's null result was found in the FitzGerald–Lorentz contraction hypothesis, now simply called length contraction or Lorentz contraction, first proposed by George FitzGerald (1889) and Hendrik Lorentz (1892). According to this law all objects physically contract by formula_42 along the line of motion (originally thought to be relative to the aether), formula_43 being the Lorentz factor. This hypothesis was partly motivated by Oliver Heaviside's discovery in 1888, that electrostatic fields are contracting in the line of motion. But since there was no reason at that time to assume that binding forces in matter are of electric origin, length contraction of matter in motion with respect to the aether was considered an Ad hoc hypothesis. If length contraction of formula_3 is inserted into the above formula for formula_45, then the light propagation time in the longitudinal direction becomes equal to that in the transverse direction: However, length contraction is only a special case of the more general relation, according to which the transverse length is larger than the longitudinal length by the ratio formula_47. This can be achieved in many ways. If formula_48 is the moving longitudinal length and formula_49 the moving transverse length, formula_50 being the rest lengths, then it is given: formula_52 can be arbitrarily chosen, so there are infinitely many combinations to explain the Michelson–Morley null result. For instance, if formula_53 the relativistic value of length contraction of formula_54 occurs, but if formula_55 then no length contraction but an elongation of formula_56 occurs. This hypothesis was later extended by Joseph Larmor (1897), Lorentz (1904) and Henri Poincaré (1905), who developed the complete Lorentz transformation including time dilation in order to explain the Trouton–Noble experiment, the Experiments of Rayleigh and Brace, and Kaufmann's experiments. It has the form It remained to define the value of formula_52, which was shown by Lorentz (1904) to be unity. In general, Poincaré (1905) demonstrated that only formula_53 allows this transformation to form a group, so it is the only choice compatible with the principle of relativity, "i.e." making the stationary aether undetectable. Given this, length contraction and time dilation obtain their exact relativistic values. Special Relativity. Albert Einstein formulated the theory of special relativity by 1905, deriving the Lorentz transformation and thus length contraction and time dilation from the relativity postulate and the constancy of the speed of light, thus removing the "ad hoc" character from the contraction hypothesis. Einstein emphasized the kinematic foundation of the theory and the modification of the notion of space and time, with the stationary aether playing no role anymore in his theory. He also pointed out the group character of the transformation. Einstein was motivated by Maxwell's theory of electromagnetism (in the form as it was given by Lorentz in 1895) and the lack of evidence for the luminiferous aether. This allows a more elegant and intuitive explanation of the Michelson-Morley null result. In a comoving frame the null result is self-evident, since the apparatus can be considered as at rest in accordance with the relativity principle, thus the beam travel times are the same. In a frame relative to which the apparatus is moving, the same reasoning applies as described above in "Length contraction and Lorentz transformation", except the word "aether" has to be replaced by "non-comoving inertial frame". The extent to which the null result of the Michelson–Morley experiment influenced Einstein is disputed. Alluding to some statements of Einstein, many historians argue that it played no significant role in his path to special relativity, while other statements of Einstein probably suggest that he was influenced by it. In any case, the null result of the Michelson–Morley experiment helped the notion of the constancy of the speed of light gain widespread and rapid acceptance. It was later shown by Howard Percy Robertson (1949) and others (see Robertson–Mansouri–Sexl test theory), that it is possible to derive the Lorentz transformation entirely from the combination of three experiments. First, the Michelson–Morley experiment showed that the speed of light is independent of the "orientation" of the apparatus, establishing the relationship between longitudinal (β) and transverse (δ) lengths. Then in 1932, Roy Kennedy and Edward Thorndike modified the Michelson–Morley experiment by making the path lengths of the split beam unequal, with one arm being very short. The Kennedy–Thorndike experiment took place for many months as the Earth moved around the sun. Their negative result showed that the speed of light is independent of the "velocity" of the apparatus in different inertial frames. In addition it established that besides length changes, corresponding time changes must also occur, "i.e." it established the relationship between longitudinal lengths (β) and time changes (α). So both experiments do not provide the individual values of these quantities. This uncertainty corresponds to the undefined factor formula_52 as described above. It was clear due to theoretical reasons (the group character of the Lorentz transformation as required by the relativity principle) that the individual values of length contraction and time dilation must assume their exact relativistic form. But a direct measurement of one of these quantities was still desirable to confirm the theoretical results. This was achieved by the Ives–Stilwell experiment (1938), measuring α in accordance with time dilation. Combining this value for α with the Kennedy–Thorndike null result shows that β must assume the value of relativistic length contraction. Combining β with the Michelson–Morley null result shows that δ must be zero. Therefore, the Lorentz transformation with formula_53 is an unavoidable consequence of the combination of these three experiments. Special relativity is generally considered the solution to all negative aether drift (or isotropy of the speed of light) measurements, including the Michelson–Morley null result. Many high precision measurements have been conducted as tests of special relativity and modern searches for Lorentz violation in the photon, electron, nucleon, or neutrino sector, all of them confirming relativity. Incorrect alternatives. As mentioned above, Michelson initially believed that his experiment would confirm Stokes' theory, according to which the aether was fully dragged in the vicinity of the earth (see Aether drag hypothesis). However, complete aether drag contradicts the observed aberration of light and was contradicted by other experiments as well. In addition, Lorentz showed in 1886 that Stokes's attempt to explain aberration is contradictory. Furthermore, the assumption that the aether is not carried in the vicinity, but only "within" matter, was very problematic as shown by the Hammar experiment (1935). Hammar directed one leg of his interferometer through a heavy metal pipe plugged with lead. If aether were dragged by mass, it was theorized that the mass of the sealed metal pipe would have been enough to cause a visible effect. Once again, no effect was seen, so aether-drag theories are considered to be disproven. Walter Ritz's Emission theory (or ballistic theory), was also consistent with the results of the experiment, not requiring aether. The theory postulates that light has always the same velocity in respect to the source. However de Sitter noted that emitter theory predicted several optical effects that were not seen in observations of binary stars in which the light from the two stars could be measured in a spectrometer. If emission theory were correct, the light from the stars should experience unusual fringe shifting due to the velocity of the stars being added to the speed of the light, but no such effect could be seen. Also terrestrial tests using interferometry and particle accelerators have been made that were inconsistent with source dependence of the speed of light. In addition, Emission theory fails the Ives-Stilwell experiment. Subsequent experiments. Although Michelson and Morley went on to different experiments after their first publication in 1887, both remained active in the field. Other versions of the experiment were carried out with increasing sophistication. Morley was not convinced of his own results, and went on to conduct additional experiments with Dayton Miller from 1902 to 1904. Again, the result was negative within the margins of error. Miller worked on increasingly larger interferometers, culminating in one with a 32 m (effective) arm length that he tried at various sites including on top of a mountain at the Mount Wilson observatory. To avoid the possibility of the aether wind being blocked by solid walls, his mountaintop observations used a special shed with thin walls, mainly of canvas. From noisy, irregular data, he consistently extracted a small positive signal that varied with each rotation of the device, with the sidereal day, and on a yearly basis. His measurements in the 1920s amounted to approximately 10 km/s instead of the nearly 30 km/s expected from the Earth's orbital motion alone. He remained convinced this was due to partial entrainment or aether dragging, though he did not attempt a detailed explanation. He ignored critiques demonstrating the inconsistency of his results and the refutation by the Hammar experiment. Miller's findings were considered important at the time, and were discussed by Michelson, Lorentz and others at a meeting reported in 1928. There was general agreement that more experimentation was needed to check Miller's results. Miller later built a non-magnetic device to eliminate magnetostriction, while Michelson built one of non-expanding Invar to eliminate any remaining thermal effects. Other experimenters from around the world increased accuracy, eliminated possible side effects, or both. So far, no one has been able to replicate Miller's results, and modern experimental accuracies have ruled them out. Roberts (2006) has pointed out that the primitive data reduction techniques used by Miller and other early experimenters, including Michelson and Morley, were capable of "creating" apparent periodic signals even when none existed in the actual data. After reanalyzing Miller's original data using modern techniques of quantitative error analysis, Roberts found Miller's apparent signals to be statistically insignificant. Using a special optical arrangement involving a 1/20 wave step in one mirror, Roy J. Kennedy (1926) and K.K. Illingworth (1927) (Fig. 8) converted the task of detecting fringe shifts from the relatively insensitive one of estimating their lateral displacements to the considerably more sensitive task of adjusting the light intensity on both sides of a sharp boundary for equal luminance. If they observed unequal illumination on either side of the step, such as in Fig. 8e, they would add or remove calibrated weights from the interferometer until both sides of the step were once again evenly illuminated, as in Fig. 8d. The number of weights added or removed provided a measure of the fringe shift. Different observers could detect changes as little as 1/300 to 1/1500 of a fringe. Kennedy also carried out an experiment at Mount Wilson, finding only about 1/10 the drift measured by Miller and no seasonal effects. In 1930, Georg Joos conducted an experiment using an automated interferometer with 21-meter-long arms forged from pressed quartz having very low thermal coefficient of expansion, that took continuous photographic strip recordings of the fringes through dozens of revolutions of the apparatus. Displacements of 1/1000 of a fringe could be measured on the photographic plates. No periodic fringe displacements were found, placing an upper limit to the aether wind of 1.5 km/s. In the table below, the expected values are related to the relative speed between Earth and Sun of 30 km/s. With respect to the speed of the solar system around the galactic center of about 220 km/s, or the speed of the solar system relative to the CMB rest frame of about 368 km/s, the null results of those experiments are even more obvious. Recent experiments. Optical tests. Optical tests of the isotropy of the speed of light became commonplace. New technologies, including the use of lasers and masers, have significantly improved measurement precision. (In the following table, only Essen (1955), Jaseja (1964), and Shamir/Fox (1969) are experiments of Michelson–Morley type, "i.e." comparing two perpendicular beams. The other optical experiments employed different methods.) Recent optical resonator experiments. Over the last several years, there has been a resurgence in interest in performing precise Michelson–Morley type experiments using lasers, masers, cryogenic optical resonators, etc. This is in large part due to predictions of quantum gravity that suggest that special relativity may be violated at scales accessible to experimental study. The first of these highly accurate experiments was conducted by Brillet & Hall (1979), in which they analyzed a laser frequency stabilized to a resonance of a rotating optical Fabry–Pérot cavity. They set a limit on the anisotropy of the speed of light resulting from the Earth's motions of Δ"c/c" ≈ 10−15, where Δ"c" is the difference between the speed of light in the "x"- and "y"-directions. As of 2009, optical and microwave resonator experiments have improved this limit to Δ"c/c" ≈ 10−17. In some of them, the devices were rotated or remained stationary, and some were combined with the Kennedy–Thorndike experiment. In particular, Earth's direction and velocity (ca. 368 km/s) relative to the CMB rest frame are ordinarily used as references in these searches for anisotropies. Other tests of Lorentz invariance. Examples of other experiments not based on the Michelson–Morley principle, "i.e." non-optical isotropy tests achieving an even higher level of precision, are Clock comparison or Hughes–Drever experiments. In Drever's 1961 experiment, 7Li nuclei in the ground state, which has total angular momentum "J"=3/2, were split into four equally spaced levels by a magnetic field. Each transition between a pair of adjacent levels should emit a photon of equal frequency, resulting in a single, sharp spectral line. However, since the nuclear wave functions for different "MJ" have different orientations in space relative to the magnetic field, any orientation dependence, whether from an aether wind or from a dependence on the large-scale distribution of mass in space (see Mach's principle), would perturb the energy spacings between the four levels, resulting in an anomalous broadening or splitting of the line. No such broadening was observed. Modern repeats of this kind of experiment have provided some of the most accurate confirmations of the principle of Lorentz invariance [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@12925ecb [docID]91100 [hitPos]11 [correct]false [extraScores][F@24be854b , [answer]The Planck pressure is the unit of pressure, denoted by pP, in the system of natural units known as Planck units. formula_1 4.63309 × 10113 Pa where formula_2 is the Planck force formula_3 is the speed of light in a vacuum formula_4 is the reduced Planck's constant formula_5 is the gravitational constant formula_6 is the Planck length [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@15ab6789 [docID]3858492 [hitPos]11 [correct]false [extraScores][F@4c5f2a02 , [answer]The "Annus mirabilis" papers (from Latin "annus mīrābilis", "extraordinary year") are the papers of Albert Einstein published in the "Annalen der Physik" scientific journal in 1905. These four articles contributed substantially to the foundation of modern physics and changed views on space, time, mass, and energy. The "annus mirabilis" is often called the "miracle year" in English or "Wunderjahr" in German. Background. At the time the papers were written, Einstein did not have easy access to a complete set of scientific reference materials, although he did regularly read and contribute reviews to "Annalen der Physik". Additionally, scientific colleagues available to discuss his theories were few. He worked as an examiner at the Patent Office in Bern, Switzerland, and he later said of a co-worker there, Michele Besso, that he "could not have found a better sounding board for his ideas in all of Europe". In addition, co-workers and the other members of the self-styled "Olympian Academy" (Maurice Solovine and Paul Habicht) and his wife, Mileva Marić, may have had some influence on Einstein's work, but how much is unclear. Through these papers, Einstein tackles some of the era's most important physics questions and problems. In 1900, a lecture titled "Nineteenth-Century Clouds over the Dynamical Theory of Heat and Light", by Lord Kelvin, suggested that physics had no satisfactory explanations for the results of the Michelson-Morley experiment and for black body radiation. As introduced, special relativity provided an account for the results of the Michelson-Morley experiments. Einstein's theories for the photoelectric effect extended the quantum theory which Max Planck had developed in his successful explanation of black body radiation. Despite the greater fame achieved by his other works, such as that on special relativity, it was his work on the photoelectric effect which won him his Nobel Prize in 1921: "For services to theoretical physics and especially for the discovery of the law of the photoelectric effect." The Nobel committee had waited patiently for experimental confirmation of special relativity; however none was forthcoming until the time dilation experiments of Ives and Stilwell (1938), (1941) and Rossi and Hall (1941). Papers. Photoelectric effect. The paper, "On a Heuristic Viewpoint Concerning the Production and Transformation of Light", received March 18 and published June 9, proposed the idea of "energy quanta". This idea, motivated by Max Planck's earlier derivation of the law of black body radiation, assumes that luminous energy can be absorbed or emitted only in discrete amounts, called "quanta". Einstein states, In explaining the photoelectric effect, the hypothesis that energy consists of "discrete packets", as Einstein illustrates, can be directly applied to black bodies, as well. The idea of light quanta contradicts the wave theory of light that follows naturally from James Clerk Maxwell's equations for electromagnetic behavior and, more generally, the assumption of infinite divisibility of energy in physical systems. Einstein noted that the photoelectric effect depended on the wavelength, and hence the frequency of the light. At too low a frequency, even intense light produced no electrons. However, once a certain frequency was reached, even low intensity light produced electrons. He compared this to Planck's hypothesis that light could be emitted only in packets of energy given by "hf", where "h" is Planck's constant and "f" is the frequency. He then postulated that light travels in packets whose energy depends on the frequency, and therefore only light above a certain frequency would bring sufficient energy to liberate an electron. Even after experiments confirmed that Einstein's equations for the photoelectric effect were accurate, his explanation was not universally accepted. Niels Bohr, in his 1922 Nobel address, stated, "The hypothesis of light-quanta is not able to throw light on the nature of radiation." By 1921, when Einstein was awarded the Nobel Prize and his work on photoelectricity was mentioned by name in the award citation, some physicists accepted that the equation (formula_1) was correct and light quanta were possible. In 1923, Arthur Compton's X-ray scattering experiment helped more of the scientific community to accept this formula. The theory of light quanta was a strong indicator of wave-particle duality, a fundamental principle of quantum mechanics. A complete picture of the theory of photoelectricity was realized after the maturity of quantum mechanics. Brownian motion. The article "Über die von der molekularkinetischen Theorie der Wärme geforderte Bewegung von in ruhenden Flüssigkeiten suspendierten Teilchen" ("On the Motion of Small Particles Suspended in a Stationary Liquid, as Required by the Molecular Kinetic Theory of Heat"), received May 11 and published July 18, delineated a stochastic model of Brownian motion. Einstein derived expressions for the mean squared displacement of particles. Using the kinetic theory of fluids, which at the time was controversial, the article established the phenomenon, which was lacking a satisfactory explanation even decades after the first observation, provided empirical evidence for the reality of the atom. It also lent credence to statistical mechanics, which had been controversial at that time, as well. Before this paper, atoms were recognized as a useful concept, but physicists and chemists debated whether atoms were real entities. Einstein's statistical discussion of atomic behavior gave experimentalists a way to count atoms by looking through an ordinary microscope. Wilhelm Ostwald, one of the leaders of the anti-atom school, later told Arnold Sommerfeld that he had been convinced of the existence of atoms by Einstein's complete explanation of Brownian motion. Special relativity. Einstein's "Zur Elektrodynamik bewegter Körper" ("On the Electrodynamics of Moving Bodies"), his third paper that year, was received on June 30 and published September 26. It reconciles Maxwell's equations for electricity and magnetism with the laws of mechanics by introducing major changes to mechanics close to the speed of light. This later became known as Einstein's special theory of relativity. The paper mentions the names of only five other scientists, Isaac Newton, James Clerk Maxwell, Heinrich Hertz, Christian Doppler, and Hendrik Lorentz. It does not have any references to any other publications. Many of the ideas had already been published by others, as detailed in history of special relativity and relativity priority dispute. However, Einstein's paper introduces a theory of time, distance, mass, and energy that was consistent with electromagnetism, but omitted the force of gravity. At the time, it was known that Maxwell's equations, when applied to moving bodies, led to asymmetries (Moving magnet and conductor problem), and that it had not been possible to discover any motion of the Earth relative to the 'light medium'. Einstein puts forward two postulates to explain these observations. First, he applies the principle of relativity, which states that the laws of physics remain the same for any non-accelerating frame of reference (called an inertial reference frame), to the laws of electrodynamics and optics as well as mechanics. In the second postulate, Einstein proposes that the speed of light has the same value in all inertial frames of reference, independent of the state of motion of the emitting body. Special relativity is thus consistent with the result of the Michelson–Morley experiment, which had not detected a medium of conductance (or aether) for light waves unlike other known waves that require a medium (such as water or air). Einstein may not have known about that experiment, but states, The speed of light is fixed, and thus "not" relative to the movement of the observer. This was impossible under Newtonian classical mechanics. Einstein argues, It had previously been proposed, by George FitzGerald in 1889 and by Lorentz in 1892, independently of each other, that the Michelson-Morley result could be accounted for if moving bodies were contracted in the direction of their motion. Some of the paper's core equations, the Lorentz transforms, had been published by Joseph Larmor (1897, 1900), Hendrik Lorentz (1895, 1899, 1904) and Henri Poincaré (1905), in a development of Lorentz's 1904 paper. Einstein's presentation differed from the explanations given by FitzGerald, Larmor, and Lorentz, but was similar in many respects to the formulation by Poincaré (1905). His explanation arises from two axioms. First, Galileo's idea that the laws of nature should be the same for all observers that move with constant speed relative to each other. Einstein writes, The second is the rule that the speed of light is the same for every observer. The theory, now called the special theory of relativity, distinguishes it from his later general theory of relativity, which considers all observers to be equivalent. Special relativity gained widespread acceptance remarkably quickly, confirming Einstein's comment that it had been "ripe for discovery" in 1905. Acknowledging the role of Max Planck in the early dissemination of his ideas, Einstein wrote in 1913 "The attention that this theory so quickly received from colleagues is surely to be ascribed in large part to the resoluteness and warmth with which he intervened for this theory". In addition, the improved mathematical formulation of the theory by Hermann Minkowski in 1907 was influential in gaining acceptance for the theory. Also, and most importantly, the theory was supported by an ever-increasing body of confirmatory experimental evidence. Mass–energy equivalence. On November 21 "Annalen der Physik" published a fourth paper (received September 27), "Ist die Trägheit eines Körpers von seinem Energieinhalt abhängig?" ("Does the Inertia of a Body Depend Upon Its Energy Content?"), in which Einstein developed an argument for arguably the most famous equation in the field of physics: "E" = "mc"2. Einstein considered the equivalency equation to be of paramount importance because it showed that a massive particle possesses an energy, the "rest energy", distinct from its classical kinetic and potential energies. The paper is based on James Clerk Maxwell's and Heinrich Rudolf Hertz's investigations and, in addition, the axioms of relativity, as Einstein states, The equation sets forth that energy of a body at rest ("E") equals its mass ("m") times the speed of light ("c") squared, or "E" = "mc"2. The mass-energy relation can be used to predict how much energy will be released or consumed by nuclear reactions; one simply measures the mass of all constituents and the mass of all the products and multiplies the difference between the two by "c"2. The result shows how much energy will be released or consumed, usually in the form of light or heat. When applied to certain nuclear reactions, the equation shows that an extraordinarily large amount of energy will be released, much larger than in the combustion of chemical explosives, where the mass difference is hardly measurable at all. This explains why nuclear weapons produce such phenomenal amounts of energy, as they release binding energy during nuclear fission and nuclear fusion, and also convert a much larger portion of subatomic mass to energy. Commemoration. The International Union of Pure and Applied Physics (IUPAP) resolved to commemorate the 100th year of the publication of Einstein's extensive work in 1905 as the 'World Year of Physics 2005'. This was subsequently endorsed by the United Nations [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@58777339 [docID]2038941 [hitPos]11 [correct]false [extraScores][F@763fadec , [answer]A light cone is the path that a flash of light, emanating from a single event (localized to a single point in space and a single moment in time) and traveling in all directions, would take through spacetime. If we imagine the light confined to a two-dimensional plane, the light from the flash spreads out in a circle after the event E occurs, and if we graph the growing circle with the vertical axis of the graph representing time, the result is a cone, known as the future light cone. The past light cone behaves like the future light cone in reverse, a circle which contracts in radius at the speed of light until it converges to a point at the exact position and time of the event E. In reality, there are three space dimensions, so the light would actually form an expanding or contracting sphere in 3D space rather than a circle in 2D, and the light cone would actually be a four-dimensional version of a cone whose cross-sections form 3D spheres (analogous to a normal three-dimensional cone whose cross-sections form 2D circles), but the concept is easier to visualize with the number of spatial dimensions reduced from three to two. Because signals and other causal influences cannot travel faster than light (see special relativity and quantum entanglement), the light cone plays an essential role in defining the concept of causality: for a given event E, the set of events that lie on or inside the past light cone of E would also be the set of all events that could send a signal that would have time to reach E and influence it in some way. For example, at a time ten years before E, if we consider the set of all events in the past light cone of E which occur at that time, the result would be a sphere (2D: disk) with a radius of ten light-years centered on the future position E will occur. So, any point on or inside the sphere could send a signal moving at the speed of light or slower that would have time to influence the event E, while points outside the sphere at that moment would not be able to have any causal influence on E. Likewise, the set of events that lie on or inside the "future" light cone of E would also be the set of events that could receive a signal sent out from the position and time of E, so the future light cone contains all the events that could potentially be causally influenced by E. Events which lie neither in the past or future light cone of E cannot influence or be influenced by E in relativity. Mathematical construction. In special relativity, a light cone (or null cone) is the surface describing the temporal evolution of a flash of light in Minkowski spacetime. This can be visualized in 3-space if the two horizontal axes are chosen to be spatial dimensions, while the vertical axis is time. The light cone is constructed as follows. Taking as event formula_1 a flash of light (light pulse) at time formula_2, all events that can be reached by this pulse from formula_1 form the future light cone of formula_1, while those events that can send a light pulse to formula_1 form the past light cone of formula_1. Given an event formula_7, the light cone classifies all events in space+time into 5 distinct categories: The above classifications hold true in any frame of reference; that is, an event judged to be in the light cone by one observer, will also be judged to be in the same light cone by all other observers, no matter their frame of reference. This is why the concept is so powerful. Keep in mind, we're talking about an event, a specific location at a specific time. To say that one event cannot affect another means that there isn't enough time for light to get from one to the other. Light from each event will eventually (after some time) make it to the old location of the other event, but since that's at a later time, it's not the same event. As time progresses, the future light cone of a given event will eventually grow to encompass more and more locations (in other words, the 3D sphere that represents the cross-section of the 4D light cone at a particular moment in time becomes larger at later times). Likewise, if we imagine running time backwards from a given event, the event's past light cone would likewise encompass more and more locations at earlier and earlier times. The further locations will of course be at more distant times, for example if we are considering the past light cone of an event which takes place on Earth today, a star 10,000 light years away would only be inside the past light cone at times 10,000 years or more in the past. The past light cone of an event on present-day Earth, at its very edges, includes very distant objects (every object in the observable universe), but only as they looked long ago, when the universe was young. Two events at different locations, at the same time (according to a specific frame of reference), are always outside of each other's past and future light cones; light cannot travel instantaneously. Other observers, of course, might see the events happening at different times and at different locations, but one way or another, the two events will likewise be seen to be outside of each other's cones. If using a system of units where the speed of light in vacuum is defined as exactly 1, for example if space is measured in light-seconds and time is measured in seconds, then the cone will have a slope of 45°, because light travels a distance of one light-second in vacuum during one second. Since special relativity requires the speed of light to be equal in every inertial frame, all observers must arrive at the same angle of 45° for their light cones. Commonly a Minkowski diagram is used to illustrate this property of Lorentz transformations. Elsewhere, an integral part of light cones, is the region of spacetime outside the light cone at a given event (a point in spacetime). Events that are elsewhere from each other are mutually unobservable, and cannot be causally connected. Light-cones in general relativity. In flat spacetime, the future light cone of an event is the boundary of its causal future and its past light cone is the boundary of its causal past. In a curved spacetime, assuming spacetime is globally hyperbolic, it is still true that the future light cone of an event includes the boundary of its causal future (and similarly for the past). However gravitational lensing can cause part of the light cone to fold in on itself, in such a way that part of the cone is strictly inside the causal future (or past), and not on the boundary. Light cones also cannot all be tilted so that they are 'parallel'; this reflects the fact that the spacetime is curved and is essentially different from Minkowski space. In vacuum regions (those points of spacetime free of matter), this inability to tilt all the light cones so that they are all parallel is reflected in the non-vanishing of the Weyl tensor [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@244efc35 [docID]310474 [hitPos]12 [correct]false [extraScores][F@cee73b7 , [answer]Special relativity is a physical theory that plays a fundamental role in the description of all physical phenomena, as long as gravitation is not significant. Many experiments played (and still play) an important role in its development and justification. The strength of the theory lies in its unique ability to correctly predict to high precision the outcome of an extremely diverse range of experiments. Repeats of many of those experiments are still being conducted with steadily increased precision, with modern experiments focusing on effects such as at the Planck scale and in the neutrino sector. Their results are consistent with the predictions of special relativity. Collections of various tests were given by Jakob Laub, Zhang, Mattingly, Clifford Will, and Roberts/Schleif. Special relativity is restricted to flat spacetime, "i.e.", to all phenomena without significant influence of gravitation. The latter lies in the domain of general relativity and the corresponding tests of general relativity must be considered. Experiments paving the way to relativity. The predominant theory of light in the 19th century was that of the luminiferous aether, a "stationary" medium in which light propagates in a manner analogous to the way sound propagates through air. By analogy, it follows that the speed of light is constant in all directions in the aether and is independent of the velocity of the source. Thus an observer moving relative to the aether must measure some sort of "aether wind" even as an observer moving relative to air measures an apparent wind. First-order experiments. Beginning with the work of François Arago (1810), a series of optical experiments had been conducted, which should have given a positive result for magnitudes to first order in "v/c" and which thus should have demonstrated the relative motion of the aether. Yet the results were negative. An explanation was provided by Augustin Fresnel (1818) with the introduction of an auxiliary hypothesis, the so called "dragging coefficient", that is, matter is dragging the aether to a small extent. This coefficient was directly demonstrated by the Fizeau experiment (1851). It was later shown that all first-order optical experiments must give a negative result due to this coefficient. In addition, also some electrostatic first order experiments were conducted, again having a negative results. In general, Hendrik Lorentz (1892, 1895) introduced several new auxiliary variables for moving observers, demonstrating why all first-order optical and electrostatic experiments have produced null results. For example, Lorentz proposed a location-variable by which electrostatic fields contract in the line of motion and another variable ("local time") by which the time coordinates for moving observers depend on their current location. Second-order experiments. The stationary aether theory, however, would give positive results when the experiments are precise enough to measure magnitudes of second order in "v/c". The first experiment of this kind was the Michelson–Morley experiment (1881, 1887) where two rays of light, traveling for some time in different directions were brought to interfere, so that different orientations relative to the aether wind should lead to a displacement of the interference fringes. But the result was negative again. The only way out of this dilemma was the proposal by George Francis FitzGerald (1889) and Lorentz (1892) that matter is contracted in the line of motion with respect to the aether (length contraction). That is, the older hypothesis of a contraction of electrostatic fields was extended to intermolecular forces. However, since there was no theoretical reason for that, the contraction hypothesis was considered ad hoc. Besides the optical Michelson–Morley experiment, its electrodynamic equivalent was also conducted, the Trouton–Noble experiment. By that it should be demonstrated that a moving condenser must be subjected to a torque. In addition, the Experiments of Rayleigh and Brace intended to measure some consequences of length contraction in the laboratory frame, for example the assumption that it would lead to birefringence. Though all of those experiments led to negative results. (The Trouton–Rankine experiment conducted in 1908 also gave a negative result when measuring the influence of length contraction on a coil.) To explain all experiments conducted before 1904, Lorentz was forced to again expand his theory by introducing the complete Lorentz transformation. Henri Poincaré declared in 1905 that the impossibility of demonstrating absolute motion (principle of relativity) is apparently a law of nature. Refutations of complete aether drag. The idea that the aether might be completely dragged within or in the vicinity of earth, by which the negative aether drift experiments could be explained, was refuted by a variety of experiments. Lodge expressed the paradoxical situation in which physicists found themselves as follows: "...at no practicable speed does ... matter any appreciable viscous grip upon the ether. Atoms "must" be able to throw it into vibration, if they are oscillating or revolving at sufficient speed; otherwise they would not emit light or any kind of radiation; but in no case do they appear to drag it along, or to meet with resistance in any uniform motion through it." Special relativity. Overview. Eventually, Albert Einstein (1905) drew the conclusion that established theories and facts known at that time only form a logical coherent system when the concepts of space and time are subjected to a fundamental revision. For instance: The result is special relativity theory, which is based on the constancy of the speed of light in all inertial frames of reference and the principle of relativity. Here, the Lorentz transformation is no longer a mere collection of auxiliary hypotheses but reflects a fundamental Lorentz symmetry and forms the basis of successful theories such as Quantum electrodynamics. Special relativity offers a large number of testable predictions, such as: Fundamental experiments. The effects of special relativity can phenomenologically be derived from the following three fundamental experiments: From these three experiments and by using the Poincaré-Einstein synchronization, the complete Lorentz transformation follows, with formula_1 being the Lorentz factor: Besides the derivation of the Lorentz transformation, the combination of these experiments is also important because they can be interpreted in different ways when viewed individually. For example, isotropy experiments such as Michelson-Morley can be seen as a simple consequence of the relativity principle, according to which any inertially moving observer can consider himself as at rest. Therefore, by itself, the MM experiment is compatible to Galilean-invariant theories like emission theory or the complete aether drag hypothesis, which also contain some sort of relativity principle. However, when other experiments that exclude the Galilean-invariant theories are considered ("i.e." the Ives–Stilwell experiment, various refutations of emission theories and refutations of complete aether dragging), Lorentz-invariant theories and thus special relativity are the only theories that remain viable. Constancy of the speed of light. Interferometers, resonators. Modern variants of Michelson-Morley and Kennedy–Thorndike experiments have been conducted in order to test the isotropy of the speed of light. Contrary to Michelson-Morley, the Kennedy-Thorndike experiments employ different arm lengths, and the evaluations last several months. In that way, the influence of different velocities during Earth's orbit around the sun can be observed. Laser, maser and optical resonators are used, reducing the possibility of any anisotropy of the speed of light to the 10−17 level. In addition to terrestrial tests, Lunar Laser Ranging Experiments have also been conducted as a variation of the Kennedy-Thorndike-experiment. Another type of isotropy experiments are the Moessbauer rotor experiments in the 1960s, by which the anisotropy of the Doppler effect on a rotating disc can be observed by using the Moessbauer effect (those experiments can also utilized to measure time dilation, see below). No dependence on source velocity or energy. Emission theories, according to which the speed of light depends on the velocity of the source, can conceivably explain the negative outcome of aether drift experiments. However, a series of experiments have definitely ruled out this class of model. For example, the Alväger–Experiment demonstrated that photons didn't acquire the speed of the high speed decaying mesons which were their source; the Sagnac experiment showed that light rays move independently of the velocity of the rotating apparatus; the de Sitter double star experiment showed that stellar orbits don't appear scrambled due to different propagation times of light. Observations of Gamma-ray bursts also demonstrated that the speed of light is independent of the frequency and energy of the light rays. One-way speed of light. A series of one-way measurements were undertaken, all of them confirming the isotropy of the speed of light. However, it should be noted that only the two-way speed of light (from A to B back to A) can unambiguously be measured, since the one-way speed depends on the definition of simultaneity and therefore on the method of synchronization. The Poincaré-Einstein synchronization convention makes the one-way speed equal to the two-way speed. However, there are many models having isotropic two-way speed of light, in which the one-way speed is anisotropic by choosing different synchronization schemes. They are experimentally equivalent to special relativity because all of these models include effects like time dilation of moving clocks, that compensate any measurable anisotropy. However, of all models having isotropic two-way speed, only special relativity is acceptable for the overwhelming majority of physicists since all other synchronizations are much more complicated, and those other models (such as Lorentz ether theory) are based on extreme and implausible assumptions concerning some dynamical effects, which are aimed at hiding the "preferred frame" from observation. Isotropy of mass, energy, and space. Clock-comparison experiments (because periodic processes and frequencies can be considered as clocks) such as the Hughes–Drever experiments provide stringent tests of Lorentz invariance. They are not restricted to the photon sector as Michelson-Morley but directly determine any anisotropy of mass, energy, or space by measuring the ground state of nuclei. Upper limit of such anisotropies of 10−33 GeV have been provided. Thus these experiments are among the most precise verifications of Lorentz invariance ever conducted. Time dilation and Length contraction. The transverse Doppler effect and consequently time dilation was directly observed for the first time in the Ives–Stilwell experiment (1938). In modern Ives-Stilwell experiments in heavy ion storage rings using saturated spectroscopy, the maximum measured deviation of time dilation from the relativistic prediction has been limited to ≤ 10−8. Other confirmations of time dilation include Mössbauer rotor experiments in which gamma rays were sent from the middle of a rotating disc to a receiver at the edge of the disc, so that the transverse Doppler effect can be evaluated by means of the Mössbauer effect. By measuring the lifetime of muons in the atmosphere and in particle accelerators, the time dilation of moving particles was also verified. On the other hand, the Hafele–Keating experiment confirmed the twin paradox, "i.e." that a clock moving from A to B back to A is retarded with respect to the initial clock. However, in this experiment the effects of general relativity also play an essential role. Direct confirmation of length contraction is hard to achieve in practice since the dimensions of the observed particles are vanishingly small. However, there are indirect confirmations; for example, the behavior of colliding heavy ions can only be explained if their increased density due to Lorentz contraction is considered. Contraction also leads to an increase of the intensity of the Coulomb field perpendicular to the direction of motion, whose effects already have been observed. Consequently, both time dilation and length contraction must be considered when conducting experiments in particle accelerators. Relativistic momentum and energy. Starting with 1901, a series of measurements was conducted aimed at demonstrating the velocity dependence of the mass of electrons. The results actually showed such a dependency but the precision necessary to distinguish between competing theories was disputed for a long time. Eventually, it was possible to definitely rule out all competing models except special relativity. Today, special relativity's predictions are routinely confirmed in particle accelerators such as the Relativistic Heavy Ion Collider. For example, the increase of relativistic momentum and energy is not only precisely measured but also necessary to understand the behavior of cyclotrons and synchrotrons etc., by which particles are accelerated near to the speed of light. Sagnac and Fizeau. Special relativity also predicts that two light rays traveling in opposite directions around a loop or closed path require different flight times to come back to the moving emitter/receiver (this is a consequence of the independence of the speed of light from the velocity of the source, see above). This effect was actually observed and is called the Sagnac effect. Currently, the consideration of this effect is necessary for many experimental setups and for the correct functioning of GPS. If such experiments are conducted in moving media, it is also necessary to consider Fresnel's dragging coefficient as demonstrated by the Fizeau experiment. Although this effect was initially understood as giving evidence of a nearly stationary aether or a partial aether drag it can easily be explained with special relativity by using the velocity composition law. Test theories. Several test theories have been developed to assess a possible positive outcome in Lorentz violation experiments by adding certain parameters to the standard equations. These include the Robertson-Mansouri-Sexl framework (RMS) and the Standard-Model Extension (SME). RMS has three testable parameters with respect to length contraction and time dilation. From that, any anisotropy of the speed of light can be assessed. On the other hand, SME includes many Lorentz violation parameters, not only for special relativity, but for the Standard model and General relativity as well; thus it has a much larger number of testable parameters. Other modern tests. Due to the developments concerning various models of Quantum gravity in recent years, deviations of Lorentz invariance (possibly following from those models) are again the target of experimentalists. Because "local Lorentz invariance" (LLI) also holds in freely falling frames, experiments concerning the weak Equivalence principle belong to this class of tests as well. The outcomes are analyzed by test theories (as mentioned above) like RMS or, more importantly, by SME [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@12925ecb [docID]1608886 [hitPos]12 [correct]false [extraScores][F@7eb2474b , [answer]The Planck voltage is the unit of voltage, denoted by VP, in the system of natural units known as Planck units. formula_1 1.04295 × 1027 V where formula_2 is the Planck energy formula_3 is the Planck charge formula_4 is the speed of light in a vacuum formula_5 is the gravitational constant formula_6 is the permittivity of free space [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@15ab6789 [docID]3858601 [hitPos]12 [correct]false [extraScores][F@1afcdc83 , [answer]A tachyon or tachyonic particle is a hypothetical particle that always moves faster than light. The word comes from the or "tachys", meaning "swift, quick, fast, rapid", and was coined by Gerald Feinberg. Most physicists think that faster-than-light particles cannot exist because they are not consistent with the known laws of physics. If such particles did exist, they could be used to build a tachyonic antitelephone and send signals faster than light, which (according to special relativity) would lead to violations of causality. Potentially consistent theories that allow faster-than-light particles include those that break Lorentz invariance, the symmetry underlying special relativity, so that the speed of light is not a barrier. In the 1967 paper that coined the term, Feinberg proposed that tachyonic particles could be quanta of a quantum field with negative squared mass. However, it was soon realized that excitations of such imaginary mass fields do "not" in fact propagate faster than light, and instead represent an instability known as tachyon condensation. Nevertheless, negative squared mass fields are commonly referred to as "tachyons", and in fact have come to play an important role in modern physics. Despite theoretical arguments against the existence of faster-than-light particles, experiments have been conducted to search for them. No compelling evidence for their existence has been found. Tachyons in relativistic theory. In special relativity, a faster-than-light particle would have space-like four-momentum, in contrast to ordinary particles that have time-like four-momentum. It would also have imaginary mass. Being constrained to the spacelike portion of the energy–momentum graph, it could not slow down to subluminal speeds. Mass. In a Lorentz invariant theory, the same formulas that apply to ordinary slower-than-light particles (sometimes called "bradyons" in discussions of tachyons) must also apply to tachyons. In particular the energy–momentum relation: (where p is the relativistic momentum of the bradyon and m is its rest mass) should still apply, along with the formula for the total energy of a particle: This equation shows that the total energy of a particle (bradyon or tachyon) contains a contribution from its rest mass (the "rest mass–energy") and a contribution from its motion, the kinetic energy. When "v" is larger than "c", the denominator in the equation for the energy is "imaginary", as the value under the radical is negative. Because the total energy must be real, the numerator must "also" be imaginary: i.e. the rest mass m must be imaginary, as a pure imaginary number divided by another pure imaginary number is a real number. Speed. One curious effect is that, unlike ordinary particles, the speed of a tachyon "increases" as its energy decreases. In particular, formula_3 approaches zero when formula_4 approaches infinity. (For ordinary bradyonic matter, "E" increases with increasing speed, becoming arbitrarily large as "v" approaches "c," the speed of light). Therefore, just as bradyons are forbidden to break the light-speed barrier, so too are tachyons forbidden from slowing down to below "c", because infinite energy is required to reach the barrier from either above or below. As noted by Einstein, Tolman, and others, special relativity implies that faster-than-light particles, if they existed, could be used to communicate backwards in time. Neutrinos. In 1985 Chodos et al. proposed that neutrinos can have a tachyonic nature. The possibility of standard model particles moving at superluminal speeds can be modeled using Lorentz invariance violating terms, for example in the Standard-Model Extension. In this framework, neutrinos experience Lorentz-violating oscillations and can travel faster than light at high energies. This proposal was strongly criticized. Cherenkov radiation. A tachyon with an electric charge would lose energy as Cherenkov radiation—just as ordinary charged particles do when they exceed the local speed of light in a medium. A charged tachyon traveling in a vacuum therefore undergoes a constant proper time acceleration and, by necessity, its worldline forms a hyperbola in space-time. However reducing a tachyon's energy "increases" its speed, so that the single hyperbola formed is of "two" oppositely charged tachyons with opposite momenta (same magnitude, opposite sign) which annihilate each other when they simultaneously reach infinite speed at the same place in space. (At infinite speed the two tachyons have no energy each and finite momentum of opposite direction, so no conservation laws are violated in their mutual annihilation. The time of annihilation is frame dependent.) Even an electrically neutral tachyon would be expected to lose energy via gravitational Cherenkov radiation, because it has a gravitational mass, and therefore increase in speed as it travels, as described above. If the tachyon interacts with any other particles, it can also radiate Cherenkov energy into those particles. Neutrinos interact with the other particles of the Standard Model, and Andrew Cohen and Sheldon Glashow recently used this to argue that the faster-than-light neutrino anomaly cannot be explained by making neutrinos propagate faster than light, and must instead be due to an error in the experiment. Causality. Causality is a fundamental principle of physics. If tachyons can transmit information faster than light, then according to relativity they violate causality, leading to logical paradoxes of the "kill your own grandfather" type. This is often illustrated with thought experiments such as the "tachyon telephone paradox" or "logically pernicious self-inhibitor." The problem can be understood in terms of the relativity of simultaneity in special relativity, which says that different inertial reference frames will disagree on whether two events at different locations happened "at the same time" or not, and they can also disagree on the order of the two events (technically, these disagreements occur when spacetime interval between the events is 'space-like', meaning that neither event lies in the future light cone of the other). If one of the two events represents the sending of a signal from one location and the second event represents the reception of the same signal at another location, then as long as the signal is moving at the speed of light or slower, the mathematics of simultaneity ensures that all reference frames agree that the transmission-event happened before the reception-event. However, in the case of a hypothetical signal moving faster than light, there would always be some frames in which the signal was received before it was sent, so that the signal could be said to have moved backwards in time. Because one of the two fundamental postulates of special relativity says that the laws of physics should work the same way in every inertial frame, if it is possible for signals to move backwards in time in any one frame, it must be possible in all frames. This means that if observer A sends a signal to observer B which moves faster than light in A's frame but backwards in time in B's frame, and then B sends a reply which moves faster than light in B's frame but backwards in time in A's frame, it could work out that A receives the reply before sending the original signal, challenging causality in "every" frame and opening the door to severe logical paradoxes. Mathematical details can be found in the tachyonic antitelephone article, and an illustration of such a scenario using spacetime diagrams can be found in "Baker, R. (2003)" Reinterpretation principle. The reinterpretation principle asserts that a tachyon sent "back" in time can always be "reinterpreted" as a tachyon traveling "forward" in time, because observers cannot distinguish between the emission and absorption of tachyons. The attempt to "detect" a tachyon "from" the future (and violate causality) would actually "create" the same tachyon and send it "forward" in time (which is causal). However, this principle is not widely accepted as resolving the paradoxes. Instead, what would be required to avoid paradoxes is that unlike any known particle, tachyons do not interact in any way and can never be detected or observed, because otherwise a tachyon beam could be modulated and used to create an anti-telephone or a "logically pernicious self-inhibitor". All forms of energy are believed to interact at least gravitationally, and many authors state that superluminal propagation in Lorentz invariant theories always leads to causal paradoxes. Fundamental models. In modern physics, all fundamental particles are regarded as excitations of quantum fields. There are several distinct ways in which tachyonic particles could be embedded into a field theory. Fields with imaginary mass. In the paper that coined the term "tachyon", Gerald Feinberg studied Lorentz invariant quantum fields with imaginary mass. Because the group velocity for such a field is superluminal, naively it appears that its excitations propagate faster than light. However, it was quickly understood that the superluminal group velocity does not correspond to the speed of propagation of any localized excitation (like a particle). Instead, the negative mass represents an instability to tachyon condensation, and all excitations of the field propagate subluminally and are consistent with causality. Despite having no faster-than-light propagation, such fields are referred to simply as "tachyons" in many sources. Tachyonic fields play an important role in modern physics. Perhaps the most famous is the Higgs boson of the Standard Model of particle physics, which—in its uncondensed phase—has an imaginary mass. In general, the phenomenon of spontaneous symmetry breaking, which is closely related to tachyon condensation, plays a very important role in many aspects of theoretical physics, including the Ginzburg–Landau and BCS theories of superconductivity. Another example of a tachyonic field is the tachyon of bosonic string theory. Lorentz violating theories. In theories that do not respect Lorentz invariance the speed of light is not (necessarily) a barrier, and particles can travel faster than the speed of light without infinite energy or causal paradoxes. A class of field theories of that type are the so-called Standard Model extensions. However, the experimental evidence for Lorentz invariance is extremely good, so such theories are very tightly constrained. Fields with non-canonical kinetic term. By modifying the kinetic energy of the field, it is possible to produce Lorentz invariant field theories with excitations that propagate superluminally. However, such theories in general do not have a well-defined Cauchy problem (for reasons related to the issues of causality discussed above), and are probably inconsistent quantum mechanically. History. As mentioned above, the term "tachyon" was coined by Gerald Feinberg in a 1967 paper titled "Possibility of Faster-Than-Light Particles". Feinberg studied the kinematics of such particles according to special relativity. In his paper he also introduced fields with imaginary mass (now also referred to as "tachyons") in an attempt to understand the microphysical origin such particles might have. The first hypothesis regarding faster-than-light particles is sometimes attributed to German physicist Arnold Sommerfeld in 1904, and more recent discussions happened in 1962 and 1969. In fiction. Tachyons have appeared in many works of fiction. They have been used as a standby mechanism upon which many science fiction authors rely to establish faster-than-light communication, with or without reference to causality issues. The word "tachyon" has become widely recognized to such an extent that it can impart a science-fictional connotation even if the subject in question has no particular relation to superluminal travel (a form of technobabble, akin to "positronic brain [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@58777339 [docID]31296 [hitPos]12 [correct]false [extraScores][F@4d6cef92 , [answer]Photon diffusion equation is a second order partial differential equation describing the time behavior of photon fluence rate distribution in a low-absorption high-scattering medium. Its mathematical form is as follows. formula_1 where formula_2 is photon fluence rate (W/cm2), formula_3 is absorption coefficient (cm−1), formula_4 is diffusion constant, formula_5 is the speed of light in the medium (m/s), and formula_6 is an isotropic source term (W/cm3). Its main difference with diffusion equation in physics is that photon diffusion equation has an absorption term in it. Application. Medical Imaging. diffuse optical tomography [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@15ab6789 [docID]16895165 [hitPos]13 [correct]false [extraScores][F@575c7dae , [answer]In astrophysics and physical cosmology, Olbers' paradox, named after the German astronomer Heinrich Wilhelm Olbers (1758–1840) and also called the "dark night sky paradox", is the argument that the darkness of the night sky conflicts with the assumption of an infinite and eternal static universe. The darkness of the night sky is one of the pieces of evidence for a non-static universe such as the Big Bang model. If the universe is static and populated by an infinite number of stars, any sight line from Earth must end at the (very bright) surface of a star, so the night sky should be completely bright. This contradicts the observed darkness of the night. History. Edward Robert Harrison's "Darkness at Night: A Riddle of the Universe" (1987) gives an account of the dark night sky paradox, seen as a problem in the history of science. According to Harrison, the first to conceive of anything like the paradox was Thomas Digges, who was also the first to expound the Copernican system in English and also postulated an infinite universe with infinitely many stars. Kepler also posed the problem in 1610, and the paradox took its mature form in the 18th century work of Halley and Cheseaux. The paradox is commonly attributed to the German amateur astronomer Heinrich Wilhelm Olbers, who described it in 1823, but Harrison shows convincingly that Olbers was far from the first to pose the problem, nor was his thinking about it particularly valuable. Harrison argues that the first to set out a satisfactory resolution of the paradox was Lord Kelvin, in a little known 1901 paper, and that Edgar Allan Poe's essay "" (1848) curiously anticipated some qualitative aspects of Kelvin's argument: The paradox. The paradox is that a static, infinitely old universe with an infinite number of stars distributed in an infinitely large space would be bright rather than dark. To show this, we divide the universe into a series of concentric shells, 1 light year thick . Thus, a certain number of stars will be in the shell 1,000,000,000 to 1,000,000,001 light years away. If the universe is homogeneous at a large scale, then there would be four times as many stars in a second shell between 2,000,000,000 to 2,000,000,001 light years away. However, the second shell is twice as far away, so each star in it would appear four times dimmer than the first shell. Thus the total light received from the second shell is the same as the total light received from the first shell. Thus each shell of a given thickness will produce the same net amount of light regardless of how far away it is. That is, the light of each shell adds to the total amount. Thus the more shells, the more light. And with infinitely many shells there would be a bright night sky. Dark clouds could obstruct the light. But in that case the clouds would heat up, until they were as hot as stars, and then radiate the same amount of light. Kepler saw this as an argument for a finite observable universe, or at least for a finite number of stars. In general relativity theory, it is still possible for the paradox to hold in a finite universe: though the sky would not be infinitely bright, every point in the sky would still be like the surface of a star. The mainstream explanation. Poet Edgar Allan Poe suggested that the finite size of the observable universe resolves the apparent paradox. More specifically, because the universe is finitely old and the speed of light is finite, only finitely many stars can be observed within a given volume of space visible from Earth. The density of stars within this finite volume is sufficiently low that any line of sight from Earth is unlikely to reach a star. However, the Big Bang theory introduces a new paradox: it states that the sky was much brighter in the past, especially at the end of the recombination era, when it first became transparent. All points of the local sky at that era were brighter than the surface of the sun, due to the high temperature of the universe in that era; and most light rays will terminate not in a star but in the relic of the Big Bang. This paradox is explained by the fact that the Big Bang theory also involves the expansion of space which can cause the energy of emitted light to be reduced via redshift. More specifically, the extreme levels of radiation from the Big Bang have been redshifted to microwave wavelengths (1100 times longer than its original wavelength) as a result of the cosmic expansion, and thus form the cosmic microwave background radiation. This explains the relatively low light densities present in most of our sky despite the assumed bright nature of the Big Bang. The redshift also affects light from distant stars and quasars, but the diminution is minor, since the most distant galaxies and quasars have redshifts of only around 5 to 8.6. Alternative explanations. Steady State. The redshift hypothesised in the Big Bang model would by itself explain the darkness of the night sky, even if the universe were infinitely old. The steady state cosmological model assumed that the universe is infinitely old and uniform in time as well as space. There is no Big Bang in this model, but there are stars and quasars at arbitrarily great distances. The light from these distant stars and quasars will be redshifted accordingly (by the Doppler effect and thermalisation), so that the total light flux from the sky remains finite. Thus the observed radiation density (the sky brightness of extragalactic background light) can be independent of finiteness of the Universe. Mathematically, the total electromagnetic energy density (radiation energy density) in thermodynamic equilibrium from Planck's law is e.g. for temperature 2.7 K it is 40 fJ/m3 ... 4.5×10−31 kg/m3 and for visible temperature 6000 K we get 1 J/m3 ... 1.1×10−17 kg/m3. But the total radiation emitted by a star (or other cosmic object) is at most equal to the total nuclear binding energy of isotopes in the star. For the density of the observable universe of about 4.6×10−28 kg/m3 and given the known abundance of the chemical elements, the corresponding maximal radiation energy density of 9.2×10−31 kg/m3, i.e. temperature 3.2 K. This is close to the summed energy density of the cosmic microwave background and the cosmic neutrino background. The Big Bang hypothesis, by contrast, predicts that the CBR should have the same energy density as the binding energy density of the primordial helium, which is much greater than the binding energy density of the non-primordial elements; so it gives almost the same result. But (neglecting quantum fluctuations in the early universe) the Big Bang would also predict a uniform distribution of CBR, while the steady-state model predicts nothing about its distribution. Nevertheless the isotropy is very probable in steady state as in the kinetic theory. Since the speed of light is a constant value, regardless of the shift towards infrared frequencies, the universe is still sharply constrained to finite sizes in space as well as in time. Some models of an infinite (Steady State theory or static universe) solution of the universe are still viable, and Olber's paradox cannot sharply distinguish between them from some variants of the Big Bang model. Finite age of stars. Stars have a finite age and a finite power, thereby implying that each star has a finite impact on a sky's light field density. But if the universe were infinitely old, there would be infinitely many other stars in the same angular direction, with an infinite total impact. Absorption. A commonly proposed alternative explanation is that the universe is not transparent, and the light from distant stars is blocked by intermediate dark stars or absorbed by dust or gas, so that there is a bound on the distance from which light can reach the observer. This would not resolve the paradox given the following argument: According to the laws of thermodynamics, the intermediate matter must eventually heat up (or cool down, if it was initially hotter) until it is in thermal equilibrium with the surrounding stars. Once this happens, the matter would then radiate the energy it receives from the stars at the same (average) temperature. So the sky would still appear uniformly bright. How bright would the sky be? Suppose that the universe were not expanding, and always had the same stellar density; then the temperature of the universe would continually increase as the stars put out more radiation. Eventually, it would reach 3000 K (corresponding to a typical photon energy of 0.3 eV and so a frequency of 7.5×1013 Hz), and the photons would begin to be absorbed by the hydrogen plasma filling most of the universe, rendering outer space opaque. This maximal radiation density corresponds to about eV/m3 = , which is much greater than the observed value of . So the sky is about fifty billion times darker than it would be if the universe were neither expanding nor too young to have reached equilibrium yet. Fractal star distribution. A different resolution, which does not rely on the Big Bang theory, was first proposed by Carl Charlier in 1908 and later rediscovered by Benoît Mandelbrot in 1974. They both postulated that if the stars in the universe were distributed in a hierarchical fractal cosmology (e.g., similar to Cantor dust)—the average density of any region diminishes as the region considered increases—it would not be necessary to rely on the Big Bang theory to explain Olbers' paradox. This model would not rule out a Big Bang but would allow for a dark sky even if the Big Bang had not occurred. Mathematically, the light received from stars as a function of star distance in a hypothetical fractal cosmos is: where: "r"0 = the distance of the nearest star. "r"0 > 0; "r" = the variable measuring distance from the Earth; "L"("r") = average luminosity per star at distance "r"; "N"("r") = number of stars at distance "r". The function of luminosity from a given distance "L"("r")"N"("r") determines whether the light received is finite or infinite. For any luminosity from a given distance "L"("r")"N"("r") proportional to "r""a", formula_3 is infinite for "a" ≥ −1 but finite for "a" < −1. So if "L"("r") is proportional to "r"−2, then for formula_3 to be finite, "N"("r") must be proportional to "r""b", where "b" < 1. For "b" = 1, the numbers of stars at a given radius is proportional to that radius. When integrated over the radius, this implies that for "b" = 1, the "total" number of stars is proportional to "r"2. This would correspond to a fractal dimension of 2. Thus the fractal dimension of the universe would need to be less than 2 for this explanation to work. This explanation is not widely accepted among cosmologists since the evidence suggests that the fractal dimension of the universe is at least 2. Moreover, the majority of cosmologists take the cosmological principle as a given, which assumes that matter at the scale of billions of light years is distributed isotropically. Contrasting this, fractal cosmology requires anisotropic matter distribution at the largest scales [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@58777339 [docID]22486 [hitPos]13 [correct]false [extraScores][F@1feb35a1 , [answer]In physics, special relativity (SR, also known as the special theory of relativity or STR) is the accepted physical theory regarding the relationship between space and time. It is based on two postulates: (1) that the laws of physics are invariant (i.e., identical) in all inertial systems (non-accelerating frames of reference); and (2) that the speed of light in a vacuum is the same for all observers, regardless of the motion of the light source. It was originally proposed in 1905 by Albert Einstein in the paper "On the Electrodynamics of Moving Bodies". The inconsistency of classical mechanics with Maxwell’s equations of electromagnetism led to the development of special relativity, which corrects classical mechanics to handle situations involving motions nearing the speed of light. As of today, special relativity is the most accurate model of motion at any speed. Even so, classical mechanics is still useful (due to its sheer simplicity and high accuracy) as an approximation at small velocities relative to the speed of light. Special relativity implies a wide range of consequences, which have been experimentally verified, including length contraction, time dilation, relativistic mass, mass–energy equivalence, a universal speed limit, and relativity of simultaneity. It has replaced the conventional notion of an absolute universal time with the notion of a time that is dependent on reference frame and spatial position. Rather than an invariant time interval between two events, there is an invariant spacetime interval. Combined with other laws of physics, the two postulates of special relativity predict the equivalence of mass and energy, as expressed in the mass–energy equivalence formula "E" = "mc"2, where "c" is the speed of light in vacuum. A defining feature of special relativity is the replacement of the Galilean transformations of classical mechanics with the Lorentz transformations. Time and space cannot be defined separately from one another. Rather space and time are interwoven into a single continuum known as spacetime. Events that occur at the same time for one observer could occur at different times for another. The theory is called "special" because it applied the principle of relativity only to the special case of inertial reference frames. Einstein later published a paper on general relativity in 1915 to apply the principle in the general case, that is, to any frame so as to handle general coordinate transformations, and gravitational effects. As Galilean relativity is now considered an approximation of special relativity valid for low speeds, special relativity is considered an approximation of the theory of general relativity valid for weak gravitational fields. The presence of gravity becomes undetectable at sufficiently small-scale, free-falling conditions. General relativity incorporates noneuclidean geometry, so that the gravitational effects are represented by the geometric curvature of spacetime. Contrarily, special relativity is restricted to flat spacetime. The geometry of spacetime in special relativity is called Minkowski space. A locally Lorentz invariant frame that abides by Special relativity can be defined at sufficiently small scales, even in curved spacetime. Galileo Galilei had already postulated that there is no absolute and well-defined state of rest (no privileged reference frames), a principle now called Galileo's principle of relativity. Einstein extended this principle so that it accounted for the constant speed of light, a phenomenon that had been recently observed in the Michelson–Morley experiment. He also postulated that it holds for all the laws of physics, including both the laws of mechanics and of electrodynamics. Postulates. Einstein discerned two fundamental propositions that seemed to be the most assured, regardless of the exact validity of the (then) known laws of either mechanics or electrodynamics. These propositions were the constancy of the speed of light and the independence of physical laws (especially the constancy of the speed of light) from the choice of inertial system. In his initial presentation of special relativity in 1905 he expressed these postulates as: The derivation of special relativity depends not only on these two explicit postulates, but also on several tacit assumptions (made in almost all theories of physics), including the isotropy and homogeneity of space and the independence of measuring rods and clocks from their past history. Following Einstein's original presentation of special relativity in 1905, many different sets of postulates have been proposed in various alternative derivations. However, the most common set of postulates remains those employed by Einstein in his original paper. A more mathematical statement of the Principle of Relativity made later by Einstein, which introduces the concept of simplicity not mentioned above is: Henri Poincaré provided the mathematical framework for relativity theory by proving that Lorentz transformations are a subset of his Poincaré group of symmetry transformations. Einstein later derived these transformations from his axioms. Many of Einstein's papers present derivations of the Lorentz transformation based upon these two principles. Einstein consistently based the derivation of Lorentz invariance (the essential core of special relativity) on just the two basic principles of relativity and light-speed invariance. He wrote: Thus many modern treatments of special relativity base it on the single postulate of universal Lorentz covariance, or, equivalently, on the single postulate of Minkowski spacetime. From the principle of relativity alone without assuming the constancy of the speed of light (i.e. using the isotropy of space and the symmetry implied by the principle of special relativity) one can show that the spacetime transformations between inertial frames are either Euclidean, Galilean, or Lorentzian. In the Lorentzian case, one can then obtain relativistic interval conservation and a certain finite limiting speed. Experiments suggest that this speed is the speed of light in vacuum. The constancy of the speed of light was motivated by Maxwell's theory of electromagnetism and the lack of evidence for the luminiferous ether. There is conflicting evidence on the extent to which Einstein was influenced by the null result of the Michelson–Morley experiment. In any case, the null result of the Michelson–Morley experiment helped the notion of the constancy of the speed of light gain widespread and rapid acceptance. Lack of an absolute reference frame. The principle of relativity, which states that there is no preferred inertial reference frame, dates back to Galileo, and was incorporated into Newtonian physics. However, in the late 19th century, the existence of electromagnetic waves led physicists to suggest that the universe was filled with a substance known as "aether", which would act as the medium through which these waves, or vibrations travelled. The aether was thought to constitute an absolute reference frame against which speeds could be measured, and could be considered fixed and motionless. Aether supposedly possessed some wonderful properties: it was sufficiently elastic to support electromagnetic waves, and those waves could interact with matter, yet it offered no resistance to bodies passing through it. The results of various experiments, including the Michelson–Morley experiment, indicated that the Earth was always 'stationary' relative to the aether – something that was difficult to explain, since the Earth is in orbit around the Sun. Einstein's solution was to discard the notion of an aether and the absolute state of rest. In relativity, any reference frame moving with uniform motion will observe the same laws of physics. In particular, the speed of light in vacuum is always measured to be "c", even when measured by multiple systems that are moving at different (but constant) velocities. Reference frames, coordinates and the Lorentz transformation. Relativity theory depends on "reference frames". The term reference frame as used here is an observational perspective in space which is not undergoing any change in motion (acceleration), from which a position can be measured along 3 spatial axes. In addition, a reference frame has the ability to determine measurements of the time of events using a 'clock' (any reference device with uniform periodicity). An event is an occurrence that can be assigned a single unique time and location in space relative to a reference frame: it is a "point" in spacetime. Since the speed of light is constant in relativity in each and every reference frame, pulses of light can be used to unambiguously measure distances and refer back the times that events occurred to the clock, even though light takes time to reach the clock after the event has transpired. For example, the explosion of a firecracker may be considered to be an "event". We can completely specify an event by its four spacetime coordinates: The time of occurrence and its 3-dimensional spatial location define a reference point. Let's call this reference frame "S". In relativity theory we often want to calculate the position of a point from a different reference point. Suppose we have a second reference frame "S"′, whose spatial axes and clock exactly coincide with that of "S" at time zero, but it is moving at a constant velocity "v" with respect to "S" along the "x"-axis. Since there is no absolute reference frame in relativity theory, a concept of 'moving' doesn't strictly exist, as everything is always moving with respect to some other reference frame. Instead, any two frames that move at the same speed in the same direction are said to be "comoving". Therefore "S" and "S"′ are not "comoving". Define the event to have spacetime coordinates in system "S" and in "S"′. Then the Lorentz transformation specifies that these coordinates are related in the following way: where is the Lorentz factor and "c" is the speed of light in vacuum, and the velocity "v" of "S"′ is parallel to the "x"-axis. The "y" and "z" coordinates are unaffected; only the "x" and "t" coordinates are transformed. These Lorentz transformations form a one-parameter group of linear mappings, that parameter being called rapidity. There is nothing special about the "x"-axis, the transformation can apply to the "y" or "z" axes, or indeed in any direction, which can be done by directions parallel to the motion (which are warped by the γ factor) and perpendicular; see main article for details. A quantity invariant under Lorentz transformations is known as a Lorentz scalar. Writing the Lorentz transformation and its inverse in terms of coordinate differences, where for instance one event has coordinates and , another event has coordinates and , and the differences are defined as we get These effects are not merely appearances; they are explicitly related to our way of measuring "time intervals" between events which occur at the same place in a given coordinate system (called "co-local" events). These time intervals will be "different" in another coordinate system moving with respect to the first, unless the events are also simultaneous. Similarly, these effects also relate to our measured distances between separated but simultaneous events in a given coordinate system of choice. If these events are not co-local, but are separated by distance (space), they will "not" occur at the same "spatial distance" from each other when seen from another moving coordinate system. However, the spacetime interval will be the same for all observers. The underlying reality remains the same. Only our perspective changes. Consequences derived from the Lorentz transformation. The consequences of special relativity can be derived from the Lorentz transformation equations. These transformations, and hence special relativity, lead to different physical predictions than those of Newtonian mechanics when relative velocities become comparable to the speed of light. The speed of light is so much larger than anything humans encounter that some of the effects predicted by relativity are initially counterintuitive. Relativity of simultaneity. Two events happening in two different locations that occur simultaneously in the reference frame of one inertial observer, may occur non-simultaneously in the reference frame of another inertial observer (lack of absolute simultaneity). From the first equation of the Lorentz transformation in terms of coordinate differences it is clear that two events that are simultaneous in frame "S" (satisfying ), are not necessarily simultaneous in another inertial frame "S"′ (satisfying ). Only if these events are co-local in frame "S" (satisfying ), will they be simultaneous in another frame "S"′. Time dilation. The time lapse between two events is not invariant from one observer to another, but is dependent on the relative speeds of the observers' reference frames (e.g., the twin paradox which concerns a twin who flies off in a spaceship traveling near the speed of light and returns to discover that his or her twin sibling has aged much more). Suppose a clock is at rest in the unprimed system S. Two different ticks of this clock are then characterized by . To find the relation between the times between these ticks as measured in both systems, the first equation can be used to find: This shows that the time (Δ"t"') between the two ticks as seen in the frame in which the clock is moving ("S"′), is "longer" than the time (Δ"t") between these ticks as measured in the rest frame of the clock ("S"). Time dilation explains a number of physical phenomena; for example, the decay rate of muons produced by cosmic rays impinging on the Earth's atmosphere. Length contraction. The dimensions (e.g., length) of an object as measured by one observer may be smaller than the results of measurements of the same object made by another observer (e.g., the ladder paradox involves a long ladder traveling near the speed of light and being contained within a smaller garage). Similarly, suppose a measuring rod is at rest and aligned along the x-axis in the unprimed system "S". In this system, the length of this rod is written as Δ"x". To measure the length of this rod in the system "S"′, in which the clock is moving, the distances "x"′ to the end points of the rod must be measured simultaneously in that system "S"′. In other words, the measurement is characterized by , which can be combined with the fourth equation to find the relation between the lengths Δ"x" and Δ"x"′: This shows that the length (Δ"x"′) of the rod as measured in the frame in which it is moving ("S"′), is "shorter" than its length (Δ"x") in its own rest frame ("S"). Composition of velocities. Velocities (speeds) do not simply add. If the observer in "S" measures an object moving along the "x" axis at velocity "u", then the observer in the "S"′ system, a frame of reference moving at velocity "v" in the "x" direction with respect to "S", will measure the object moving with velocity "u"′ where (from the Lorentz transformations above): The other frame "S" will measure: Notice that if the object were moving at the speed of light in the "S" system (i.e. "u" = "c"), then it would also be moving at the speed of light in the "S"′ system. Also, if both "u" and "v" are small with respect to the speed of light, we will recover the intuitive Galilean transformation of velocities The usual example given is that of a train (frame "S"′ above) traveling due east with a velocity "v" with respect to the tracks (frame "S"). A child inside the train throws a baseball due east with a velocity "u"′ with respect to the train. In classical physics, an observer at rest on the tracks will measure the velocity of the baseball (due east) as , while in special relativity this is no longer true; instead the velocity of the baseball (due east) is given by the second equation: . Again, there is nothing special about the "x" or east directions. This formalism applies to any direction by considering parallel and perpendicular motion to the direction of relative velocity "v", see main article for details. Einstein's addition of colinear velocities is consistent with the Fizeau experiment which determined the speed of light in a fluid moving parallel to the light, but no experiment has ever tested the formula for the general case of non-parallel velocities. Other consequences. Thomas rotation. The orientation of an object (i.e. the alignment of its axes with the observer's axes) may be different for different observers. Unlike other relativistic effects, this effect becomes quite significant at fairly low velocities as can be seen in the spin of moving particles. Equivalence of mass and energy. As an object's speed approaches the speed of light from an observer's point of view, its relativistic mass increases thereby making it more and more difficult to accelerate it from within the observer's frame of reference. The energy content of an object at rest with mass "m" equals "mc"2. Conservation of energy implies that, in any reaction, a decrease of the sum of the masses of particles must be accompanied by an increase in kinetic energies of the particles after the reaction. Similarly, the mass of an object can be increased by taking in kinetic energies. In addition to the papers referenced above—which give derivations of the Lorentz transformation and describe the foundations of special relativity—Einstein also wrote at least four papers giving heuristic arguments for the equivalence (and transmutability) of mass and energy, for . Mass–energy equivalence is a consequence of special relativity. The energy and momentum, which are separate in Newtonian mechanics, form a four-vector in relativity, and this relates the time component (the energy) to the space components (the momentum) in a nontrivial way. For an object at rest, the energy–momentum four-vector is : it has a time component which is the energy, and three space components which are zero. By changing frames with a Lorentz transformation in the x direction with a small value of the velocity v, the energy momentum four-vector becomes . The momentum is equal to the energy multiplied by the velocity divided by "c"2. As such, the Newtonian mass of an object, which is the ratio of the momentum to the velocity for slow velocities, is equal to "E"/"c"2. The energy and momentum are properties of matter and radiation, and it is impossible to deduce that they form a four-vector just from the two basic postulates of special relativity by themselves, because these don't talk about matter or radiation, they only talk about space and time. The derivation therefore requires some additional physical reasoning. In his 1905 paper, Einstein used the additional principles that Newtonian mechanics should hold for slow velocities, so that there is one energy scalar and one three-vector momentum at slow velocities, and that the conservation law for energy and momentum is exactly true in relativity. Furthermore, he assumed that the energy of light is transformed by the same Doppler-shift factor as its frequency, which he had previously shown to be true based on Maxwell's equations. The first of Einstein's papers on this subject was "Does the Inertia of a Body Depend upon its Energy Content?" in 1905. Although Einstein's argument in this paper is nearly universally accepted by physicists as correct, even self-evident, many authors over the years have suggested that it is wrong. Other authors suggest that the argument was merely inconclusive because it relied on some implicit assumptions. Einstein acknowledged the controversy over his derivation in his 1907 survey paper on special relativity. There he notes that it is problematic to rely on Maxwell's equations for the heuristic mass–energy argument. The argument in his 1905 paper can be carried out with the emission of any massless particles, but the Maxwell equations are implicitly used to make it obvious that the emission of light in particular can be achieved only by doing work. To emit electromagnetic waves, all you have to do is shake a charged particle, and this is clearly doing work, so that the emission is of energy. How far can one travel from the Earth? Since one can not travel faster than light, one might conclude that a human can never travel further from Earth than 40 light years if the traveler is active between the age of 20 and 60. One would easily think that a traveler would never be able to reach more than the very few solar systems which exist within the limit of 20–40 light years from the earth. But that would be a mistaken conclusion. Because of time dilation, a hypothetical spaceship can travel thousands of light years during the pilot's 40 active years. If a spaceship could be built that accelerates at a constant 1g, it will after a little less than a year be traveling at almost the speed of light as seen from Earth. Time dilation will increase his life span as seen from the reference system of the Earth, but his lifespan measured by a clock traveling with him will not thereby change. During his journey, people on Earth will experience more time than he does. A 5 year round trip for him will take 6½ Earth years and cover a distance of over 6 light-years. A 20 year round trip for him (5 years accelerating, 5 decelerating, twice each) will land him back on Earth having traveled for 335 Earth years and a distance of 331 light years. A full 40 year trip at 1 g will appear on Earth to last 58,000 years and cover a distance of 55,000 light years. A 40 year trip at 1.1 g will take 148,000 Earth years and cover about 140,000 light years. A one-way 28 year (14 years accelerating, 14 decelerating as measured with the cosmonaut's clock) trip at 1 g acceleration could reach 2,000,000 light-years to the Andromeda Galaxy. This same time dilation is why a muon traveling close to "c" is observed to travel much further than "c" times its half-life (when at rest). Causality and prohibition of motion faster than light. The interval AC in the diagram is 'space-like'; i.e., there is a frame of reference in which events A and C occur simultaneously, separated only in space. There are also frames in which A precedes C (as shown) and frames in which C precedes A. If it were possible for a cause-and-effect relationship to exist between events A and C, then paradoxes of causality would result. For example, if A was the cause, and C the effect, then there would be frames of reference in which the effect preceded the cause. Although this in itself won't give rise to a paradox, one can show that faster than light signals can be sent back into one's own past. A causal paradox can then be constructed by sending the signal if and only if no signal was received previously. Therefore, if causality is to be preserved, one of the consequences of special relativity is that no information signal or material object can travel faster than light in vacuum. However, some "things" can still move faster than light. For example, the location where the beam of a search light hits the bottom of a cloud can move faster than light when the search light is turned rapidly. Even without considerations of causality, there are other strong reasons why faster-than-light travel is forbidden by special relativity. For example, if a constant force is applied to an object for a limitless amount of time, then integrating gives a momentum that grows without bound, but this is simply because formula_13 approaches infinity as formula_14 approaches "c". To an observer who is not accelerating, it appears as though the object's inertia is increasing, so as to produce a smaller acceleration in response to the same force. This behavior is observed in particle accelerators, where each charged particle is accelerated by the electromagnetic force. Theoretical and experimental tunneling studies carried out by Günter Nimtz and Petrissa Eckle claimed that under special conditions signals may travel faster than light. It was measured that fiber digital signals were traveling up to 5 times c and a zero-time tunneling electron carried the information that the atom is ionized, with photons, phonons and electrons spending zero time in the tunneling barrier. According to Nimtz and Eckle, in this superluminal process only the Einstein causality and the special relativity but not the primitive causality are violated: Superluminal propagation does not result in any kind of time travel. Several scientists have stated not only that Nimtz' interpretations were erroneous, but also that the experiment actually provided a trivial experimental confirmation of the special relativity theory. Geometry of spacetime. Comparison between flat Euclidean space and Minkowski space. Special relativity uses a 'flat' 4-dimensional Minkowski space – an example of a spacetime. Minkowski spacetime appears to be very similar to the standard 3-dimensional Euclidean space, but there is a crucial difference with respect to time. In 3D space, the differential of distance (line element) "ds" is defined by where are the differentials of the three spatial dimensions. In Minkowski geometry, there is an extra dimension with coordinate "X"0 derived from time, such that the distance differential fulfills where are the differentials of the four spacetime dimensions. This suggests a deep theoretical insight: special relativity is simply a rotational symmetry of our spacetime, analogous to the rotational symmetry of Euclidean space (see image right). Just as Euclidean space uses a Euclidean metric, so spacetime uses a Minkowski metric. Basically, special relativity can be stated as the "invariance of any spacetime interval" (that is the 4D distance between any two events) when viewed from "any inertial reference frame". All equations and effects of special relativity can be derived from this rotational symmetry (the Poincaré group) of Minkowski spacetime. The actual form of "ds" above depends on the metric and on the choices for the "X"0 coordinate. To make the time coordinate look like the space coordinates, it can be treated as imaginary: (this is called a Wick rotation). According to Misner, Thorne and Wheeler (1971, §2.3), ultimately the deeper understanding of both special and general relativity will come from the study of the Minkowski metric (described below) and to take , rather than a "disguised" Euclidean metric using "ict" as the time coordinate. Some authors use , with factors of "c" elsewhere to compensate; for instance, spatial coordinates are divided by "c" or factors of "c"±2 are included in the metric tensor. These numerous conventions can be superseded by using natural units where . Then space and time have equivalent units, and no factors of "c" appear anywhere. 3D spacetime. If we reduce the spatial dimensions to 2, so that we can represent the physics in a 3D space we see that the null geodesics lie along a dual-cone (see image right) defined by the equation; or simply  which is the equation of a circle of radius "c dt". 4D spacetime. If we extend this to three spatial dimensions, the null geodesics are the 4-dimensional cone: so This null dual-cone represents the "line of sight" of a point in space. That is, when we look at the stars and say "The light from that star which I am receiving is X years old", we are looking down this line of sight: a null geodesic. We are looking at an event a distance formula_22 away and a time "d/c" in the past. For this reason the null dual cone is also known as the 'light cone'. (The point in the lower left of the picture below represents the star, the origin represents the observer, and the line represents the null geodesic "line of sight".) The cone in the −"t" region is the information that the point is 'receiving', while the cone in the +"t" section is the information that the point is 'sending'. The geometry of Minkowski space can be depicted using Minkowski diagrams, which are useful also in understanding many of the thought-experiments in special relativity. Note that, in 4d spacetime, the concept of the center of mass becomes more complicated, see center of mass (relativistic). Physics in spacetime. Transformations of physical quantities between reference frames. Above, the Lorentz transformation for the time coordinate and three space coordinates illustrates that they are intertwined. This is true more generally: certain pairs of "timelike" and "spacelike" quantities naturally combine on equal footing under the same Lorentz transformation. The Lorentz transformation in standard configuration above, i.e. for a boost in the "x" direction, can be recast into matrix form as follows: In Newtonian mechanics, quantities which have magnitude and direction are mathematically described as 3d vectors in Euclidean space, and in general they are parametrized by time. In special relativity, this notion is extended by adding the appropriate timelike quantity to a spacelike vector quantity, and we have 4d vectors, or "four vectors", in Minkowski spacetime. The components of vectors are written using tensor index notation, as this has numerous advantages. The notation makes it clear the equations are manifestly covariant under the Poincaré group, thus bypassing the tedious calculations to check this fact. In constructing such equations, we often find that equations previously thought to be unrelated are, in fact, closely connected being part of the same tensor equation. Recognizing other physical quantities as tensors simplifies their transformation laws. Throughout, upper indices (superscripts) are contravariant indices rather than exponents except when they indicate a square (this is should be clear from the context), and lower indices (subscripts) are covariant indices. For simplicity and consistency with the earlier equations, Cartesian coordinates will be used. The simplest example of a four-vector is the position of an event in spacetime, which constitutes a timelike component "ct" and spacelike component , in a contravariant position four vector with components: where we define so that the time coordinate has the same dimension of distance as the other spatial dimensions; so that space and time are treated equally. Now the transformation of the contravariant components of the position 4-vector can be compactly written as: where there is an implied summation on "ν" from 0 to 3, and formula_26 is a matrix. More generally, all contravariant components of a four-vector formula_27 transform from one frame to another frame by a Lorentz transformation: Examples of other 4-vectors include the four-velocity "U"μ, defined as the derivative of the position 4-vector with respect to proper time: where the Lorentz factor is: The relativistic energy formula_31 and relativistic momentum formula_32 of an object are respectively the timelike and spacelike components of a covariant four momentum vector: where "m" is the invariant mass. The four-acceleration is the proper time derivative of 4-velocity: The transformation rules for "three"-dimensional velocities and accelerations are very awkward; even above in standard configuration the velocity equations are quite complicated owing to their non-linearity. On the other hand, the transformation of "four"-velocity and "four"-acceleration are simpler by means of the Lorentz transformation matrix. The four-gradient of a scalar field φ transforms covariantly rather than contravariantly: that is: only in Cartesian coordinates. It's the covariant derivative which transforms in manifest covariance, in Cartesian coordinates this happens to reduce to the partial derivatives, but not in other coordinates. More generally, the "co"variant components of a 4-vector transform according to the "inverse" Lorentz transformation: where formula_38 is the reciprocal matrix of formula_26. The postulates of special relativity constrain the exact form the Lorentz transformation matrices take. More generally, most physical quantities are best described as (components of) tensors. So to transform from one frame to another, we use the well-known tensor transformation law where formula_41 is the reciprocal matrix of formula_42. All tensors transform by this rule. An example of a four dimensional second order antisymmetric tensor is the relativistic angular momentum, which has six components: three are the classical angular momentum, and the other three are related to the boost of the center of mass of the system. The derivative of the relativistic angular momentum with respect to proper time is the relativistic torque, also second order antisymmetric tensor. The electromagnetic field tensor is another second order antisymmetric tensor field, with six components: three for the electric field and another three for the magnetic field. There is also the stress–energy tensor for the electromagnetic field, namely the electromagnetic stress–energy tensor. Metric. The metric tensor allows one to define the inner product of two vectors, which in turn allows one to assign a magnitude to the vector. Given the four-dimensional nature of spacetime the Minkowski metric "η" has components (valid in any inertial reference frame) which can be arranged in a matrix: which is equal to its reciprocal, formula_44, in those frames. Throughout we use the signs as above, different authors use different conventions – see Minkowski metric alternative signs. The Poincaré group is the most general group of transformations which preserves the Minkowski metric: and this is the physical symmetry underlying special relativity. The metric can be used for raising and lowering indices on vectors and tensors. Invariants can be constructed using the metric, the inner product of a 4-vector "T" with another 4-vector "S" is: Invariant means that it takes the same value in all inertial frames, because it is a scalar (0 rank tensor), and so no Λ appears in its trivial transformation. The magnitude of the 4-vector "T" is the positive square root of the inner product with itself: One can extend this idea to tensors of higher order, for a second order tensor we can form the invariants: similarly for higher order tensors. Invariant expressions, particularly inner products of 4-vectors with themselves, provide equations that are useful for calculations, because one doesn't need to perform Lorentz transformations to determine the invariants. Relativistic kinematics and invariance. The coordinate differentials transform also contravariantly: so the squared length of the differential of the position four-vector "dXμ" constructed using is an invariant. Notice that when the line element "d"X2 is negative that is the differential of proper time, while when "d"X2 is positive, is differential of the proper distance. The 4-velocity "U"μ has an invariant form: which means all velocity four-vectors have a magnitude of "c". This is an expression of the fact that there is no such thing as being at coordinate rest in relativity: at the least, you are always moving forward through time. Differentiating the above equation by "τ" produces: So in special relativity, the acceleration four-vector and the velocity four-vector are orthogonal. Relativistic dynamics and invariance. The invariant magnitude of the momentum 4-vector generates the energy–momentum relation: We can work out what this invariant is by first arguing that, since it is a scalar, it doesn't matter which reference frame we calculate it, and then by transforming to a frame where the total momentum is zero. We see that the rest energy is an independent invariant. A rest energy can be calculated even for particles and systems in motion, by translating to a frame in which momentum is zero. The rest energy is related to the mass according to the celebrated equation discussed above: Note that the mass of systems measured in their center of momentum frame (where total momentum is zero) is given by the total energy of the system in this frame. It may not be equal to the sum of individual system masses measured in other frames. To use Newton's third law of motion, both forces must be defined as the rate of change of momentum with respect to the same time coordinate. That is, it requires the 3D force defined above. Unfortunately, there is no tensor in 4D which contains the components of the 3D force vector among its components. If a particle is not traveling at "c", one can transform the 3D force from the particle's co-moving reference frame into the observer's reference frame. This yields a 4-vector called the four-force. It is the rate of change of the above energy momentum four-vector with respect to proper time. The covariant version of the four-force is: In the rest frame of the object, the time component of the four force is zero unless the "invariant mass" of the object is changing (this requires a non-closed system in which energy/mass is being directly added or removed from the object) in which case it is the negative of that rate of change of mass, times "c". In general, though, the components of the four force are not equal to the components of the three-force, because the three force is defined by the rate of change of momentum with respect to coordinate time, i.e. "dp"/"dt" while the four force is defined by the rate of change of momentum with respect to proper time, i.e. "dp"/"d"τ. In a continuous medium, the 3D "density of force" combines with the "density of power" to form a covariant 4-vector. The spatial part is the result of dividing the force on a small cell (in 3-space) by the volume of that cell. The time component is −1/"c" times the power transferred to that cell divided by the volume of the cell. This will be used below in the section on electromagnetism. Relativity and unifying electromagnetism. Theoretical investigation in classical electromagnetism led to the discovery of wave propagation. Equations generalizing the electromagnetic effects found that finite propagation speed of the E and B fields required certain behaviors on charged particles. The general study of moving charges forms the Liénard–Wiechert potential, which is a step towards special relativity. The Lorentz transformation of the electric field of a moving charge into a non-moving observer's reference frame results in the appearance of a mathematical term commonly called the magnetic field. Conversely, the "magnetic" field generated by a moving charge disappears and becomes a purely "electrostatic" field in a comoving frame of reference. Maxwell's equations are thus simply an empirical fit to special relativistic effects in a classical model of the Universe. As electric and magnetic fields are reference frame dependent and thus intertwined, one speaks of "electromagnetic" fields. Special relativity provides the transformation rules for how an electromagnetic field in one inertial frame appears in another inertial frame. Maxwell's equations in the 3D form are already consistent with the physical content of special relativity, although they are easier to manipulate in a manifestly covariant form, i.e. in the language of tensor calculus. See main links for more detail. Status. Special relativity in its Minkowski spacetime is accurate only when the absolute value of the gravitational potential is much less than "c"2 in the region of interest. In a strong gravitational field, one must use general relativity. General relativity becomes special relativity at the limit of weak field. At very small scales, such as at the Planck length and below, quantum effects must be taken into consideration resulting in quantum gravity. However, at macroscopic scales and in the absence of strong gravitational fields, special relativity is experimentally tested to extremely high degree of accuracy (10−20) An overview can be found on this page</ref> and thus accepted by the physics community. Experimental results which appear to contradict it are not reproducible and are thus widely believed to be due to experimental errors. Special relativity is mathematically self-consistent, and it is an organic part of all modern physical theories, most notably quantum field theory, string theory, and general relativity (in the limiting case of negligible gravitational fields). Newtonian mechanics mathematically follows from special relativity at small velocities (compared to the speed of light) – thus Newtonian mechanics can be considered as a special relativity of slow moving bodies. See classical mechanics for a more detailed discussion. Several experiments predating Einstein's 1905 paper are now interpreted as evidence for relativity. Of these it is known Einstein was aware of the Fizeau experiment before 1905, and historians have concluded that Einstein was at least aware of the Michelson–Morley experiment as early as 1899 despite claims he made in his later years that it played no role in his development of the theory. Particle accelerators routinely accelerate and measure the properties of particles moving at near the speed of light, where their behavior is completely consistent with relativity theory and inconsistent with the earlier Newtonian mechanics. These machines would simply not work if they were not engineered according to relativistic principles. In addition, a considerable number of modern experiments have been conducted to test special relativity. Some examples: Theories of relativity and quantum mechanics. "Special" relativity can be combined with quantum mechanics to form relativistic quantum mechanics. It is an unsolved problem in physics how "general" relativity and quantum mechanics can be unified; quantum gravity and a "theory of everything", which require such a unification, are active and ongoing areas in theoretical research. The early Bohr–Sommerfeld atomic model explained the fine structure of alkali metal atoms using both special relativity and the preliminary knowledge on quantum mechanics of the time. In 1928, Paul Dirac constructed an influential relativistic wave equation, now known as the Dirac equation in his honour, that is fully compatible both with special relativity and with the final version of quantum theory existing after 1926. This equation explained not only the intrinsic angular momentum of the electrons called "spin", it also led to the prediction of the antiparticle of the electron (the positron), and fine structure could only be fully explained with special relativity. It was the first foundation of "relativistic quantum mechanics". In non-relativistic quantum mechanics, spin is phenomenological and cannot be explained. On the other hand, the existence of antiparticles leads to the conclusion that relativistic quantum mechanics is not enough for a more accurate and complete theory of particle interactions. Instead, a theory of particles interpreted as quantized fields, called "quantum field theory", becomes necessary; in which particles can be created and destroyed throughout space and time [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@12925ecb [docID]26962 [hitPos]14 [correct]false [extraScores][F@451c5c06 , [answer]The four-frequency of a photon is defined by where formula_2 is the photon's frequency and formula_3 is a unit vector in the direction of the photon's motion. The four-frequency is always a future-pointing and null vector. An observer moving with four-velocity formula_4 will observe a frequency Where formula_6 is the Minkowski inner-product (+---) Closely related to the four-frequency is the wave four-vector defined by where formula_8, formula_9 is the speed of light and formula_10 and formula_11 is the wavelength of the photon. The wave four-vector is more often used in practice than the four-frequency, but the two vectors are related (using formula_12) by [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@15ab6789 [docID]1753397 [hitPos]14 [correct]false [extraScores][F@3c95ea71 , [answer]The history of special relativity consists of many theoretical results and empirical findings obtained by Albert Michelson, Hendrik Lorentz, Henri Poincaré and others. It culminated in the theory of special relativity proposed by Albert Einstein and subsequent work of Max Planck, Hermann Minkowski and others. Introduction. Although Isaac Newton based his theory on absolute time and space, he also adhered to the principle of relativity of Galileo Galilei. This stated that all observers who move uniformly relative to each other are equal and no absolute state of motion can be attributed to any observer. During the 19th century the aether theory was widely accepted, mostly in the form given by James Clerk Maxwell. According to Maxwell "all" optical and electrical phenomena propagate in a medium. Thus it seemed possible to determine "absolute" motion relative to the aether and therefore to disprove Galileo's principle. The failure of any experiment to detect motion through the aether led Hendrik Lorentz in 1892 to develop a theory based on an immobile aether and the Lorentz transformation. Based on Lorentz's aether, Henri Poincaré in 1905 proposed the "relativity principle" as a general law of nature, including electrodynamics and gravitation. In the same year Albert Einstein published what is now called special relativity – he radically reinterpreted Lorentzian electrodynamics by changing the concepts of space and time and abolishing the aether. This paved the way for general relativity. Subsequent work of Hermann Minkowski laid the foundations of relativistic field theories. Aether and electrodynamics of moving bodies. Aether models and Maxwell's equations. Following the work of Thomas Young (1804) and Augustin-Jean Fresnel (1816), it was believed that light propagates as a transverse wave within an elastic medium called luminiferous aether. However, a distinction was made between optical and electrodynamical phenomena so it was necessary to create specific aether models for all phenomena. Attempts to unify those models or to create a complete mechanical description of them did not succeed, but after considerable work by many scientists, including Michael Faraday and Lord Kelvin, James Clerk Maxwell (1864) developed an accurate theory of electromagnetism by deriving a set of equations in electricity, magnetism and inductance, named Maxwell's equations. He first proposed that light was in fact undulations (Electromagnetic radiation) in the "same" aetherial medium that is the cause of electric and magnetic phenomena. However, Maxwell's theory was unsatisfactory regarding the optics of moving bodies, and while he was able to present a complete mathematical model, he was not able to provide a coherent mechanical description of the aether. After Heinrich Hertz in 1887 demonstrated the existence of electromagnetic waves, Maxwell's theory was widely accepted. In addition, Oliver Heaviside and Hertz further developed the theory and introduced modernized versions of Maxwell's equations. The "Maxwell-Hertz" or "Heaviside-Hertz" Equations subsequently formed an important basis for the further development of electrodynamics, and Heaviside's notation is still used today.--> Other important contributions to Maxwell's theory were made by George FitzGerald, Joseph John Thomson, John Henry Poynting, Hendrik Lorentz, and Joseph Larmor. Search for the aether. Regarding the relative motion and the mutual influence of matter and aether, there were two controversial theories.One of them was developed by Fresnel (and subsequently Lorentz). This model (Stationary Aether Theory) supposed that light propagates as a transverse wave and aether is partially dragged with a certain coefficient by matter. Based on this assumption, Fresnel was able to explain the aberration of light and many optical phenomena.The other hypothesis was proposed by George Gabriel Stokes, who stated in 1845 that the aether was "fully" dragged by matter (later this view was also shared by Hertz). In this model the aether might be (by analogy with pine pitch) rigid for fast objects and fluid for slower objects. Thus the Earth could move through it fairly freely, but it would be rigid enough to transport light. Fresnel's theory was preferred because his dragging coefficient was confirmed by the Fizeau experiment in 1851, who measured the speed of light in moving liquids. A possible solution to the problem was shown by Woldemar Voigt (1887), who investigated the Doppler effect for waves propagating in an incompressible elastic medium and deduced transformation relations that left the Wave equation in free space unchanged, and explained the negative result of the Michelson-Morley Experiment. The Voigt transformations include the Lorentz factor formula_1 for the y- and z-coordinates, and a new time variable formula_2 which later was called "local time". However, Voigt's work was completely ignored by his contemporaries. FitzGerald (1889) offered another explanation of the negative result of the Michelson-Morley experiment. Contrary to Voigt, he speculated that the intermolecular forces are possibly of electrical origin so that material bodies would contract in the line of motion (length contraction). This was in connection with the work of Heaviside (1887), who determined that the electrostatic fields in motion were deformed (Heaviside Ellipsoid), which leads to physically undetermined conditions at the speed of light. However, Fitzgerald's idea remained widely unknown and was not discussed before Oliver Lodge published a summary of the idea in 1892. Also Lorentz (1892b) proposed length contraction independently from Fitzgerald in order to explain the Michelson-Morley experiment. For plausibility reasons, Lorentz referred to the analogy of the contraction of electrostatic fields. However, even Lorentz admitted that that was not a necessary reason and length-contraction consequently remained an ad hoc hypothesis. Lorentz's theory of electrons. A very similar model was created by Joseph Larmor (1897, 1900). Larmor was the first to put Lorentz's 1895-transformation into a form algebraically equivalent to the modern Lorentz transformations, however, he stated that his transformations preserved the form of Maxwell's equations only to second order of formula_3. Lorentz later noted that these transformations did in fact preserve the form of Maxwell's equations to all orders of formula_3. Larmor noticed on that occasion, that not only can length-contraction be derived from it, but he also calculated some sort of time dilation for electron orbits. Larmor specified his considerations in 1900 and 1904. Independently of Larmor, also Lorentz (1899) extended his transformation for second order terms and noted a (mathematical) Time Dilation effect as well. However, besides Lorentz and Larmor also other physicists tried to develop a consistent model of electrodynamics. For example, Emil Cohn (1900, 1901) created an alternative Electrodynamics in which he, as one of the first, discarded the existence of the aether (at least in the previous form) and would use, like Ernst Mach, the fixed stars as a reference frame instead. Due to inconsistencies within his theory, like different light speeds in different directions, it was superseded by Lorentz's and Einstein's. Electromagnetic mass. During his development of Maxwell's Theory, J. J. Thomson (1881) recognized that charged bodies are harder to set in motion than uncharged bodies. He also noticed that the mass of a body "in motion" is increased by a constant quantity. Electrostatic fields behave as if they add an "electromagnetic mass" to the mechanical mass of the bodies. I.e., according to Thomson, electromagnetic energy corresponds to a certain mass. This was interpreted as some form of self-inductance of the electromagnetic field. Thomson's work was continued and perfected by FitzGerald, Heaviside (1888), and George Frederick Charles Searle (1896, 1897). For the electromagnetic mass they gave — in modern notation — the formula formula_7, where formula_8 is the electromagnetic mass and formula_9 is the electromagnetic energy. Heaviside and Searle also recognized that the increase of the mass of a body is not constant and varies with its velocity. Consequently, Searle noted the impossibility of superluminal velocities, because infinite energy would be needed to exceed the speed of light. Also for Lorentz (1899), the integration of the speed-dependence of masses recognized by Thomson was especially important. He noticed that the mass not only varied due to speed, but is also dependent on the direction, and he introduced what Abraham later called "longitudinal" and "transverse" mass. (The transversal mass corresponds to what later was called relativistic mass.) Wilhelm Wien (1900) assumed (following the works of Thomson, Heaviside, and Searle) that the "entire" mass is of electromagnetic origin, which was formulated in the context that all forces of nature are electromagnetic ones (the "Electromagnetic World View"). Wien stated that, if it is assumed that gravitation is an electromagnetic effect too, then there has to be a proportionality between electromagnetic energy, inertial mass and gravitational mass. In the same paper Henri Poincaré (1900b) found another way of combining the concepts of mass and energy. He recognized that electromagnetic energy behaves like a fictitious fluid with mass density of formula_10 (or formula_11) and defined a fictitious electromagnetic momentum as well. However, he arrived at a radiation paradox which was fully explained by Einstein in 1905. Walter Kaufmann (1901–1903) was the first to confirm the velocity dependence of electromagnetic mass by analyzing the ratio formula_12 (where formula_13 is the charge and formula_8 the mass) of cathode rays. He found that the value of formula_12 decreased with the speed, showing that, assuming the charge constant, the mass of the electron increased with the speed. He also believed that those experiments confirmed the assumption of Wien, that there is no "real" mechanical mass, but only the "apparent" electromagnetic mass, or in other words, the mass of all bodies is of electromagnetic origin. Max Abraham (1902–1904), who was a supporter of the electromagnetic world view, quickly offered an explanation for Kaufmann's experiments by deriving expressions for the electromagnetic mass. Together with this concept, Abraham introduced (like Poincaré in 1900) the notion of "Electromagnetic Momentum" which is proportional to formula_16. But unlike the fictitious quantities introduced by Poincaré, he considered it as a "real" physical entity. Abraham also noted (like Lorentz in 1899) that this mass also depends on the direction and coined the names "Longitudinal" and "Transverse" Mass. In contrast to Lorentz, he didn't incorporate the Contraction Hypothesis into his theory, and therefore his mass terms differed from those of Lorentz. Based on the preceding work on electromagnetic mass, Friedrich Hasenöhrl suggested that part of the mass of a body (which he called apparent mass) can be thought of as radiation bouncing around a cavity. The "apparent mass" of radiation depends on the temperature (because every heated body emits radiation) and is proportional to its energy. Hasenöhrl stated that this energy-apparent-mass relation only holds as long as the body radiates, i.e., if the temperature of a body is greater than 0 K. At first he gave the expression formula_17 for the apparent mass; however, Abraham and Hasenöhrl himself in 1905 changed the result to formula_7, the same value as for the electromagnetic mass for a body at rest. Absolute space and time. Some scientists started to criticize Newton's definitions of absolute space and time. Ernst Mach (1883) argued that absolute time and space are meaningless and only relative motion is a useful concept. He also said that even accelerated motion such as rotation could be related to the fixed stars without using Newton's absolute space. And Carl Neumann (1870) introduced a "Body alpha", which represents some sort of rigid and fixed body for defining inertial motion. Based on the definition of Neumann, Heinrich Streintz (1883) argued that if gyroscopes don't measure any signs of rotation, then one can speak of inertial motion which is related to a "Fundamental body" and a "Fundamental Coordinate System". Eventually, Ludwig Lange (1885) was the first to coin the expression inertial frame of reference and "inertial time scale" as operational replacements for absolute space and time, by defining ""a reference frame in which a mass point thrown from the same point in three different (non-co-planar) directions follows rectilinear paths each time it is thrown is called a inertial frame"". And in 1902, Henri Poincaré published the philosophical and popular-science book "Science and Hypothesis", which included: philosophical assessments on the relativity of space, time, and simultaneity; the opinion that a violation of the Relativity Principle can never be detected; the possible non-existence of the aether but also some arguments supporting the aether; many remarks on non-Euclidean geometry. There were also some attempts to use time as a fourth dimension. This was done as early as 1754 by Jean le Rond d'Alembert in the Encyclopédie, and by some authors in the 19th century like H. G. Wells in his novel The Time Machine (1895). In 1901 a philosophical model was developed by Menyhért Palágyi, in which space and time were only two sides of some sort of "spacetime". He used time as an imaginary fourth dimension, which he gave the form formula_19 (where formula_20, i.e. imaginary number). However, Palagyi's time coordinate is not connected to the speed of light. He also rejected any connection with the existing constructions of "n"-dimensional spaces and non-Euclidean geometry, so his philosophical model bears only little resemblance with spacetime physics, as it was later developed by Minkowski. Light constancy and the principle of relative motion. In the second half of the 19th century there were many attempts to develop a worldwide clock network synchronized by electrical signals. On that occasion, the finite propagation speed of light had to be considered as well. So Henri Poincaré (1898) in his paper drew some important consequences of this process and explained that astronomers, in determining the speed of light, simply assume that light has a constant speed and that this speed is the same in all directions. Without this postulate it would be impossible to infer the speed of light from astronomical observations, as Ole Rømer did based on observations of the moons of Jupiter. Poincaré also noted that the propagation speed of light can be (and in practice often is) used to define simultaneity between spatially separate events. He concluded by saying that ""The simultaneity of two events, or the order of their succession, the equality of two durations, are to be so defined that the enunciation of the natural laws may be as simple as possible. In other words, all these rules, all these definitions are only the fruit of an unconscious opportunism."" In some other papers, Poincaré (1895, 1900b) argued that experiments like that of Michelson-Morley show the impossibility of detecting the absolute motion of matter, i.e., the relative motion of matter in relation to the aether. He called this the "principle of relative motion". In the same year he interpreted Lorentz's local time as the result of a synchronization procedure based on light signals. He assumed that 2 observers A and B, which are moving in the aether, synchronize their clocks by optical signals. Since they believe themselves to be at rest, they must consider only the transmission time of the signals and then cross-reference their observations to examine whether their clocks are synchronous. However, from the point of view of an observer at rest in the aether, the clocks are not synchronous and indicate the local time formula_21. But because the moving observers do not know anything about their movement, they do not recognize this. So, contrary to Lorentz, Poincaré-defined local time can be measured and indicated by clocks. Therefore, in his recommendation of Lorentz for the Nobel Prize in 1902, Poincaré argued that Lorentz has convincingly explained the negative outcome of the aether drift experiments by inventing the "diminished time", i.e. that two events at different places could appear as simultaneous, although they are not simultaneous in reality. Like Poincaré, Alfred Bucherer (1903) believed in the validity of the relativity principle within the domain of electrodynamics, but contrary to Poincaré, Bucherer even assumed that this implies the nonexistence of the aether. However, the theory that was created by him later in 1906 was incorrect and not self-consistent, and the Lorentz transformation was absent within his theory as well. Lorentz's 1904 model. In his paper , Lorentz (1904) was following the suggestion of Poincaré and attempted to create a formulation of Electrodynamics, which explains the failure of all known aether drift experiments, i.e. the validity of the relativity principle. He tried to prove the applicability of the Lorentz transformation for all orders, although he didn't succeed completely. Like Wien and Abraham, he argued that there exists only electromagnetic mass, not mechanical mass, and derived the correct expression for longitudinal and transverse mass, which were in agreement with Kaufmann's experiments (even though those experiments were not precise enough to distinguish between the theories of Lorentz and Abraham). And using the electromagnetic momentum, he could explain the negative result of the Trouton-Noble experiment, in which a charged parallel-plate capacitor moving through the aether should orient itself perpendicular to the motion. Also the Experiments of Rayleigh and Brace could be explained. Another important step was the postulate that the Lorentz Transformation has to be valid for non-electrical forces as well. At the same time, when Lorentz worked out his theory, Wien (1903) recognized an important consequence of the velocity dependence of mass. He argued that superluminal velocities were impossible, because that would require an infinite amount of energy — the same was already noted by Thomson (1893) and Searle (1897). And in June 1904, after he had read Lorentz's 1904 paper, he noticed the same in relation to length contraction, because at superluminal velocities the factor formula_22 becomes imaginary. Lorentz's theory was criticized by Abraham, who demonstrated that on one side the theory obeys the relativity principle, and on the other side the electromagnetic origin of all forces is assumed. Abraham showed, that both assumptions were incompatible, because in Lorentz's theory of the contracted electrons, non-electric forces were needed in order to guarantee the stability of matter. However, in Abraham's theory of the rigid electron, no such forces were needed. Thus the question arose whether the Electromagnetic conception of the world (compatible with Abraham's theory) or the Relativity Principle (compatible with Lorentz's Theory) was correct. In a September 1904 lecture in St. Louis named , Poincaré drew some consequences from Lorentz's theory and defined (in modification of Galileo's Relativity Principle and Lorentz's Theorem of Corresponding States) the following principle: ""The Principle of Relativity, according to which the laws of physical phenomena must be the same for a stationary observer as for one carried along in a uniform motion of translation, so that we have no means, and can have none, of determining whether or not we are being carried along in such a motion."" He also specified his clock synchronization method and explained the possibility of a "new method" or "new mechanics", in which no velocity can surpass that of light for "all" observers. However, he critically noted that the Relativity Principle, Newton's action and reaction, the Conservation of Mass, and the Conservation of Energy are not fully established and are even threatened by some experiments. Also Emil Cohn (1904) continued to develop his alternative model (as described above), and while comparing his theory with that of Lorentz, he discovered some important physical interpretations of the Lorentz transformations. He illustrated (like Joseph Larmor in the same year) this transformation by using rods and clocks: If they are at rest in the aether, they indicate the true length and time, and if they are moving, they indicate contracted and dilated values. Like Poincaré, Cohn defined local time as the time, which is based on the assumption of isotropic propagation of light. Contrary to Lorentz and Poincaré it was noticed by Cohn, that within Lorentz's theory the separation of "real" and "apparent" coordinates is artificial, because no experiment can distinguish between them. Yet according to Cohn's own theory, the Lorentz transformed quantities would only be valid for optical phenomena, while mechanical clocks would indicate the "real" time. Eventually Poincaré (independently of Einstein) finished a substantially extended work of his June paper (the so-called "Palermo paper", received July 23, printed December 14, published January 1906 ). He spoke literally of "the postulate of relativity". He showed that the transformations are a consequence of the Principle of Least Action and developed the properties of the Poincaré stresses. He demonstrated in more detail the group characteristics of the transformation, which he called the Lorentz group, and he showed that the combination formula_23 is invariant. While elaborating his gravitational theory, he said the Lorentz transformation is merely a rotation in four-dimensional space about the origin, by introducing formula_24 as a fourth imaginary coordinate (contrary to Palagyi, he included the speed of light), and he already used four-vectors. He wrote that the discovery of magneto-cathode rays by Paul Ulrich Villard (1904) seemed to threaten the entire theory of Lorentz, but this problem was quickly solved. However, although in his philosophical writings Poincaré rejected the ideas of absolute space and time, in his physical papers he continued to refer to an (undetectable) aether. He also continued (1900b, 1904, 1906, 1908b) to describe coordinates and phenomena as local/apparent (for moving observers) and true/real (for observers at rest in the aether). So, with a few exceptions, most historians of science argue that Poincaré did not invent what is now called special relativity, although it is admitted that Poincaré anticipated much of Einstein's methods and terminology. Special relativity. Einstein 1905. Electrodynamics of moving bodies. On September 26, 1905 (received June 30), Albert Einstein published his annus mirabilis paper on what is now called "special relativity". Einstein's paper includes a fundamental new definition of space and time (all time and space coordinates in all reference frames are equal, so there is no "true" or "apparent" time) and the abolition of the aether. He identified two fundamental principles, the Principle of Relativity and the "Principle of the Constancy of Light", which served as the axiomatic basis of his theory. To better understand Einstein's step, a summary of the situation before 1905, as it was described above, shall be given (it must be remarked that Einstein was familiar with the 1895 theory of Lorentz, and "Science and Hypothesis" by Poincaré, but not their papers of 1904-1905): with the following consequences for the speed of light and the theories known at that time: To make the preceding theories tenable the introduction of ad hoc hypotheses would be required. Yet in science the assumption of a conspiracy of effects which prevent the discovery of other effects is considered to be very improbable, and it would violate Occam's razor as well. So Einstein refused to invent auxiliary hypotheses and drew the direct conclusions from the facts stated above: That the relativity principle is correct and the speed of light is constant in all inertial reference frames. Because of his axiomatic method, Einstein was able to derive "all results" of his predecessors – and in addition the formulas for the relativistic Doppler effect and relativistic aberration – on a few pages, while his predecessors needed years of long, complicated work to arrive at the same mathematical formalism. Lorentz and Poincaré had also adopted these same principles, as necessary to achieve their final results, but didn't recognize that they were also sufficient and hence they obviated all the other assumptions (especially the stationary aether) underlying Lorentz's initial derivations. Another reason for Einstein's rejection of the aether was probably his work on quantum physics. Einstein found out that light can also be described as a particle, so the aether as the medium for electromagnetic "waves" (which was highly important for Lorentz and Poincaré) had no place in his theoretical concepts anymore. It's notable that Einstein's paper contains no direct references to other papers. However, many historians of science like Holton, Miller, Stachel, have tried to find out possible influences on Einstein. He stated that his thinking was influenced by the empiricist philosophers David Hume and Ernst Mach. Regarding the Relativity Principle, the moving magnet and conductor problem (possibly after reading a book of August Föppl) and the various negative aether drift experiments were important for him to accept that principle — but he denied any significant influence of the "most important" experiment: the Michelson-Morley experiment. Other possible sources are Poincaré's "Science and Hypothesis", where he described the Principle of Relativity and which was read by Einstein in 1904, and the writings of Max Abraham, from whom he borrowed the terms "Maxwell-Hertz equations" and "longitudinal and transverse mass". Regarding his views on Electrodynamics and the Principle of the Constancy of Light, Einstein stated that Lorentz's theory of 1895 (or the Maxwell-Lorentz electrodynamics) and also the Fizeau experiment had considerable influence on his thinking. He said in 1909 and 1912 that he borrowed that principle from Lorentz's stationary aether (which implies validity of Maxwell's equations and the constancy of light in the aether frame), but he recognized that this principle together with the principle of relativity makes the aether useless. As he wrote in 1907 and in later papers, the apparent contradiction between those principles can be solved if it is realized that Lorentz's local time is not an auxiliary quantity, but can simply be defined as "time" and is connected with signal velocity. Before Einstein, also Poincaré developed a similar physical interpretation of local time and noticed the connection to signal velocity, but contrary to Einstein he continued to argue that clocks in the aether show the true time, and moving clocks show the apparent time. Eventually, in 1953 Einstein described the advances of his theory (although Poincaré already stated in 1905 that Lorentz invariance is a general condition for any physical theory): Mass-energy equivalence. Already in §10 of his paper on electrodynamics, Einstein used the formula for the kinetic energy of an electron. In elaboration of this he published a paper (received September 27, November 1905), in which Einstein showed that when a material body lost energy (either radiation or heat) of amount "E", its mass decreased by the amount "E"/"c"2. This led to the famous mass–energy equivalence formula: "E" = "mc"2. Einstein considered the equivalency equation to be of paramount importance because it showed that a massive particle possesses an energy, the "rest energy", distinct from its classical kinetic and potential energies. As it was shown above, many authors before Einstein arrived at similar formulas (including a 4/3-factor) for the relation of mass to energy. However, their work was focused on electromagnetic energy which (as we know today) only represents a small part of the entire energy within matter. So it was Einstein who was the first a) to ascribe this relation to all forms of energy, and b) to understand the connection of Mass-energy equivalence with the relativity principle. Early reception. First assessments. Walter Kaufmann (1905, 1906) was probably the first who referred to Einstein's work. He compared the theories of Lorentz and Einstein and, although he said Einstein's method is to be preferred, he argued that both theories are observationally equivalent. Therefore he spoke of the relativity principle as the "Lorentz-Einsteinian" basic assumption. Shortly afterwards, Max Planck (1906a) was the first who publicly defended the theory and interested his students, Max von Laue and Kurd von Mosengeil, in this formulation. He described Einstein's theory as a "generalization" of Lorentz's theory and, to this "Lorentz-Einstein-Theory", he gave the name "relative theory"; while Alfred Bucherer changed Planck's notation into the now common "theory of relativity". On the other hand, Einstein himself and many others continued to refer simply to the new method as the "relativity principle". And in an important overview article on the relativity principle (1908a), Einstein described SR as a "union of Lorentz's theory and the relativity principle", including the fundamental assumption that Lorentz's local time can be described as real time. (Yet, Poincaré's contributions were rarely mentioned in the first years after 1905.) All of those expressions, (Lorentz-Einstein theory, relativity principle, relativity theory) were used by different physicists alternately in the next years. Kaufmann-Bucherer experiments. Kaufmann (1905, 1906) announced the results of his new experiments on the charge to mass ratio, i.e. the velocity dependence of mass. They represented, in his opinion, a clear refutation of the relativity principle and the Lorentz-Einstein-Theory, and a confirmation of Abraham's theory. For some years Kaufmann's experiments represented a weighty objection against the relativity principle, although it was criticized by Planck and Adolf Bestelmeyer (1906). Following Kaufmann other physicists, like Alfred Bucherer (1908) and Günther Neumann (1914), also examined the velocity-dependence of mass and this time it was thought that the "Lorentz-Einstein theory" and the relativity principle were confirmed, and Abraham's theory disproved. However, it was later pointed out that the Kaufmann–Bucherer–Neumann experiments only showed a qualitative mass increase of moving electrons, but they were not precise enough to distinguish between the models of Lorentz-Einstein and Abraham. So it continued until 1940, when experiments of this kind were repeated with sufficient accuracy for confirming the Lorentz-Einstein formula. However, this problem occurred only with this kind of experiment. The investigations of the fine structure of the hydrogen lines already in 1917 provided a clear confirmation of the Lorentz-Einstein formula and the refutation of Abraham's theory. Relativistic momentum and mass. Planck (1906a) defined the relativistic momentum and gave the correct values for the longitudinal and transverse mass by correcting a slight mistake of the expression given by Einstein in 1905. Planck's expressions were in principle equivalent to those used by Lorentz in 1899. Based on the work of Planck, the concept of relativistic mass was developed by Gilbert Newton Lewis and Richard C. Tolman (1908, 1909) by defining mass as the ratio of momentum to velocity. So the older definition of longitudinal and transverse mass, in which mass was defined as the ratio of force to acceleration, became superfluous. Finally, Tolman (1912) interpreted relativistic mass simply as "the" mass of the body. However, many modern textbooks on relativity don't use the concept of relativistic mass anymore, and mass is considered as an invariant quantity. Mass and energy. Einstein (1906) showed that the inertia of energy (mass-energy-equivalence) is a necessary and sufficient condition for the conservation of the center of mass theorem. On that occasion, he noted that the formal mathematical content of Poincaré paper on the center of mass (1900b) and his own paper were mainly the same, although the physical interpretation was different in light of relativity. Kurd von Mosengeil (1906) by extending Hasenöhrl's calculation of black-body-radiation in a cavity, derived the same expression for the additional mass of a body due to electromagnetic radiation as Hasenöhrl. Hasenöhrl's idea was that the mass of bodies included a contribution from the electromagnetic field, he imagined a body as a cavity containing light. His relationship between mass and energy, like all other pre-Einstein ones, contained incorrect numerical prefactors (see Electromagnetic mass). Eventually Planck (1907) derived the mass-energy-equivalence in general within the framework of special relativity, including the binding forces within matter. He acknowledged the priority of Einstein's 1905 work on formula_26, but Planck judged his own approach as more general than Einstein's. Experiments by Fizeau and Sagnac. As it was explained above, already in 1895 Lorentz succeeded in deriving Fresnel's dragging coefficient (to first order of v/c) and the Fizeau experiment by using the electromagnetic theory and the concept of local time. After first attempts by Jakob Laub (1907) to create a relativistic "optics of moving bodies", it was Max von Laue (1907) who derived the coefficient for terms of all orders by using the colinear case of the relativistic velocity addition law. In addition, Laue's calculation was much simpler than the complicated methods used by Lorentz. In 1911 Laue also discussed a situation where on a platform a beam of light is split and the two beams are made to follow a trajectory in opposite directions. On return to the point of entry the light is allowed to exit the platform in such a way that an interference pattern is obtained. Laue calculated a displacement of the interference pattern if the platform is in rotation – because the speed of light is independent of the velocity of the source, so one beam has covered less distance than the other beam. An experiment of this kind was performed by Georges Sagnac in 1913, who actually measured a displacement of the interference pattern (Sagnac effect). While Sagnac himself concluded that his theory confirmed the theory of an aether at rest, Laue's earlier calculation showed that it is compatible with special relativity as well because in "both" theories the speed of light is independent of the velocity of the source. This effect can be understood as the electromagnetic counterpart of the mechanics of rotation, for example in analogy to a Foucault pendulum in 1909–11, Franz Harress (1912) performed an experiment which can be considered as a synthesis of the experiments of Fizeau and Sagnac. He tried to measure the dragging coefficient within glass. Contrary to Fizeau he used a rotating device so he found the same effect as Sagnac. While Harress himself misunderstood the meaning of the result, it was shown by Laue that the theoretical explanation of Harress' experiment is in accordance with the Sagnac effect. Eventually, the Michelson–Gale–Pearson experiment (1925, a variation of the Sagnac experiment) indicated the angular velocity of the Earth itself in accordance with special relativity and a resting aether. Relativity of simultaneity. The first derivations of relativity of simultaneity by synchronization with light signals were also simplified. Daniel Frost Comstock (1910) placed an observer in the middle between two clocks A and B. From this observer a signal is sent to both clocks, and in the frame in which A and B are at rest, they synchronously start to run. But from the perspective of a system in which A and B are moving, clock B is first set in motion, and then comes clock A – so the clocks are not synchronized. Also Einstein (1917) created a model with an observer in the middle between A and B. However, in his description two signals are sent "from" A and B to the observer. From the perspective of the frame, in which A and B are at rest, the signals are sent at the same time and the observer ""is hastening towards the beam of light coming from B, whilst he is riding on ahead of the beam of light coming from A. Hence the observer will see the beam of light emitted from B earlier than he will see that emitted from A. Observers who take the railway train as their reference-body must therefore come to the conclusion that the lightning flash B took place earlier than the lightning flash A."" Spacetime physics. Minkowski's spacetime. Poincaré's attempt of a four-dimensional reformulation of the new mechanics was not continued by himself, so it was Hermann Minkowski (1907), who worked out the consequences of that notion (other contributions were made by Roberto Marcolongo (1906) and Richard Hargreaves (1908)). This was based on the work of many mathematicians of the 19th century like Arthur Cayley, Felix Klein, or William Kingdon Clifford, who contributed to group theory, invariant theory and projective geometry. Using similar methods, Minkowski succeeded in formulating a geometrical interpretation of the Lorentz transformation. He completed, for example, the concept of four vectors; he created the Minkowski diagram for the depiction of space-time; he was the first to use expressions like world line, proper time, Lorentz invariance/covariance, etc.; and most notably he presented a four-dimensional formulation of electrodynamics. Similar to Poincaré he tried to formulate a Lorentz-invariant law of gravity, but that work was subsequently superseded by Einstein's elaborations on gravitation. In 1907 Minkowski named four predecessors who contributed to the formulation of the relativity principle: Lorentz, Einstein, Poincaré and Planck. And in his famous lecture Space and Time (1908) he mentioned Voigt, Lorentz and Einstein. Minkowski himself considered Einstein's theory as a generalization of Lorentz's and credited Einstein for completely stating the relativity of time, but he criticized his predecessors for not fully developing the relativity of space. However, modern historians of science argue that Minkowski's claim for priority was unjustified, because Minkowski (like Wien or Abraham) adhered to the electromagnetic world-picture and apparently didn't fully understand the difference between Lorentz's electron theory and Einstein's kinematics. In 1908, Einstein and Laub rejected the four-dimensional electrodynamics of Minkowski as too complicated and published a "more elementary", non-four-dimensional derivation of the basic-equations for moving bodies. But it was Minkowski's formalism which a) showed that special relativity is a complete and consistent theory, and b) served as a basis for further development of relativity. Eventually, Einstein (1912) agreed on the importance of Minkowski's spacetime formalism and used it for his work on the foundations of general relativity. Today special relativity is seen as an application of linear algebra, but at the time special relativity was being developed the field of linear algebra was still in its infancy. There were no textbooks on linear algebra as modern vector space and transformation theory, and the matrix notation of Arthur Cayley (that unifies the subject) had not yet come into widespread use. In retrospect, we can see that the Lorentz transformations are simply hyperbolic rotations, as explicitly noted by Minkowski. Vector notation and closed systems. Minkowski's space-time formalism was quickly accepted and further developed. For example, Arnold Sommerfeld (1910) replaced Minkowski's matrix notation by an elegant vector notation and coined the terms "four vector" and "six vector". He also introduced a trigonometric formulation of the relativistic velocity addition rule, which according to Sommerfeld, removes much of the strangeness of that concept. Other important contributions were made by Laue (1911, 1913), who used the spacetime formalism to create a relativistic theory of deformable bodies and an elementary particle theory. He extended Minkowski's expressions for electromagnetic processes to all possible forces and thereby clarified the concept of mass-energy-equivalence. Laue also showed that non-electrical forces are needed to ensure the proper Lorentz transformation properties, and for the stability of matter – he could show that the "Poincaré stresses" (as mentioned above) are a natural consequence of relativity theory so that the electron be a closed system. Lorentz transformation without second postulate. There were some attempts to derive the Lorentz transformation without the postulate of the constancy of the speed of light. Vladimir Ignatowski (1910) for example used for this purpose a) the principle of relativity, b) and homogeneity and isotropy of space c) the requirement of reciprocity. Philipp Frank and Hermann Rothe (1911) argued that this derivation is incomplete and needs additional assumptions. Their own calculation was based on the assumptions that a) the Lorentz transformation forms a homogeneous linear group, b) when changing frames, only the sign of the relative speed changes, c) length contraction solely depends on the relative speed. However, according to Pauli and Miller such models were insufficient to identify the invariant speed in their transformation with the speed of light — for example, Ignatowski was forced to recourse to electrodynamics to include the speed of light. So Pauli and others argued that both postulates are needed to derive the Lorentz transformation. However, until today, others continued the attempts to derive special relativity without the light postulate. Non-euclidean formulations without imaginary time coordinate. It was noted by Minkowski (1907) that his space-time formalism represents a "four-dimensional non-euclidean manifold", but in order to emphasize the formal similarity to the more familiar Euclidean geometry, Minkowski noted that the time coordinate could be treated as imaginary. This was just a way of representing a non-Euclidean metric while emphasizing the formal similarity to a Euclidean metric. However, many subsequent writers have dispensed with the imaginary time coordinate, and simply written the metric in explicitly non-Euclidean form (i.e., with a negative signature), since it makes no difference to the content or results of the equations. It merely affects (slightly) their appearance. Sommerfeld (1910) gave a trigonometric formulation of velocities, and Vladimir Varićak (1912) emphasized the similarity of this formulation to (Bolyai-Lobachevskian) hyperbolic geometry and tried to reformulate relativity using that non-euclidean geometry. Alfred Robb (1911) introduced the concept of Rapidity as a hyperbolic angle to characterize frame velocity. Edwin Bidwell Wilson and Gilbert N. Lewis (1912) introduced a vector notation for spacetime. Émile Borel (1913) derived the kinematic basis of Thomas precession. Different authors have used the phrase hyperbolic plane to refer both to (Bolyai-Lobachevskian) hyperbolic geometry and Minkowski geometry but these are two different geometries. Space-time is described by Minkowski space, but the velocity space is described by hyperbolic geometry. In particular the hyperboloid model was identified with velocities by Minkowski (1908). Today one still finds texts on special relativity that make use of an imaginary time coordinate, but most have adopted real-valued coordinates and a metric with negative signature. (The implications of the two different formalisms in the context of general relativity - as in the recent work of Hawking - are beyond the scope of this article.) Time dilation and twin paradox. Einstein (1907a) proposed a method for detecting the transverse Doppler effect as a direct consequence of time dilation. And in fact, that effect was measured in 1938 by Herbert E. Ives and G. R. Stilwell (Ives–Stilwell experiment). And Lewis and Tolman (1909) described the reciprocity of time dilation by using two light clocks A and B, traveling with a certain relative velocity to each other. The clocks consist of two plane mirrors parallel to one another and to the line of motion. Between the mirrors a light signal is bouncing, and for the observer resting in the same reference frame as A, the period of clock A is the distance between the mirrors divided by the speed of light. But if the observer looks at clock B, he sees that within that clock the signal traces out a longer, angled path, thus clock B is slower than A. However, for the observer moving alongside with B the situation is completely in reverse: Clock B is faster and A is slower. Also Lorentz (1910–1912) discussed the reciprocity of time dilation and analyzed a clock "paradox", which apparently occurs as a consequence of the reciprocity of time dilation. Lorentz showed that there is no paradox if one considers that in one system only one clock is used, while in the other system two clocks are necessary. So the relativity of simultaneity has to be considered as well. Acceleration. Einstein (1908) tried – as a preliminary in the framework of special relativity – also to include accelerated frames within the relativity principle. In the course of this attempt he recognized that for any single moment of acceleration of a body one can define an inertial reference frame in which the accelerated body is temporarily at rest. It follows that in accelerated frames defined in this way, the application of the constancy of the speed of light to define simultaneity is restricted to small localities. However, the equivalence principle that was used by Einstein in the course of that investigation, which expresses the equality of inertial and gravitational mass and the equivalence of accelerated frames and homogeneous gravitational fields, transcended the limits of special relativity and resulted in the formulation of general relativity. Nearly simultaneously with Einstein, also Minkowski (1908) considered the special case of uniform accelerations within the framework of his space-time formalism. He recognized that the world-line of such an accelerated body corresponds to a hyperbola. This notion was further developed by Born (1909) and Sommerfeld (1910), with Born introducing the expression "hyperbolic motion". He noted that uniform acceleration can be used as an approximation for any form of acceleration within special relativity. In addition, Harry Bateman and Ebenezer Cunningham (1910) showed that Maxwell's equations are invariant under a much wider group of transformation than the Lorentz-group, i.e., the so-called "conformal transformations". Under those transformations the equations preserve their form for some types of accelerated motions. A general covariant formulation of electrodynamics in Minkowski space was eventually given by Friedrich Kottler (1912), whereby his formulation is also valid for general relativity. Concerning the further development of the description of accelerated motion in special relativity, the works by Langevin and others for rotating frames (Born coordinates), and by Wolfgang Rindler and others for uniform accelerated frames (Rindler coordinates) must be mentioned. Rigid bodies and Ehrenfest paradox. Einstein (1907b) discussed the question of whether, in rigid bodies, as well as in all other cases, the velocity of information can exceed the speed of light, and explained that information could be transmitted under these circumstances into the past, thus causality would be violated. Since this contravenes radically against every experience, superluminal velocities are thought impossible. He added that a dynamics of the rigid body must be created in the framework of SR. Eventually, Max Born (1909) in the course of his above mentioned work concerning accelerated motion, tried to include the concept of rigid bodies into SR. However, Paul Ehrenfest (1909) showed that Born's concept lead the so-called Ehrenfest paradox, in which, due to length contraction, the circumference of a rotating disk is shortened while the radius stays the same. This question was also considered by Gustav Herglotz (1910), Fritz Noether (1910), and von Laue (1911). It was recognized by Laue that the classic concept is not applicable in SR since a "rigid" body possesses infinitely many Degrees of freedom. Yet, while Born's definition was not applicable on rigid bodies, it was very useful in describing rigid "motions" of bodies. In connection to the Ehrenfest paradox, it was also discussed (by Vladimir Varićak and others) whether length contraction is "real" or "apparent", and whether there is a difference between the dynamic contraction of Lorentz and the kinematic contraction of Einstein. However, it was rather a dispute over words because, as Einstein said, the kinematic length contraction is "apparent" for an co-moving observer, but for an observer at rest it is "real" and the consequences are measurable. Acceptance of special relativity. Eventually, around 1911 most mathematicians and theoretical physicists accepted the results of special relativity. For example, already Planck (1909) compared the implications of the modern relativity principle — especially Einstein's relativity of time — with the revolution by the Copernican system. As a result, the fundamental difference between the dynamic approach of Lorentz and the kinematic one of Einstein was pointed out, and the term "Lorentz-Einstein-Theory" wasn't used anymore. Only a few theoretical physicists like Lorentz, Poincaré, Abraham or Langevin, still believed in the existence of an aether in any form. Another important reason for accepting special relativity was the extension of Minkowski's space-time formalism around 1910–1913. So in 1912 Wilhelm Wien recommended both Lorentz and Einstein for the Nobel Prize in Physics – even though this prize was never awarded for special relativity. After formulating GR, Einstein in 1915, for the first time, used the expression "special theory of relativity" to distinguish between the theories. Relativistic theories. Gravitation. The first attempt to formulate a relativistic theory of gravitation was undertaken by Poincaré (1905). He tried to modify Newton's law of gravitation so that it assumes a Lorentz-covariant form. He noted that there were many possibilities for a relativistic law, and he discussed two of them. It was shown by Poincaré that the argument of Pierre-Simon Laplace, who argued that the speed of gravity is many times faster than the speed of light, is not valid within a relativistic theory. That is, in a relativistic theory of gravitation, planetary orbits are stable even when the speed of gravity is equal to that of light. Similar models as that of Poincaré were discussed by Minkowski (1907b) and Sommerfeld (1910). However, it was shown by Abraham (1912) that those models belong to the class of "vector theories" of gravitation. The fundamental defect of those theories is that they implicitly contain a negative value for the gravitational energy in the vicinity of matter, which would violate the energy principle. As an alternative, Abraham (1912) and Gustav Mie (1913) proposed different "scalar theories" of gravitation. While Mie never formulated his theory in a consistent way, Abraham completely gave up the concept of Lorentz-covariance (even locally), and therefore it was irreconcilable with relativity. In addition, all of those models violated the equivalence principle, and Einstein argued that it is impossible to formulate a theory which is both Lorentz-covariant and satisfies the equivalence principle. However, Gunnar Nordström (1912, 1913) was able to create a model which fulfilled both conditions. This was achieved by making both the gravitational and the inertial mass dependent on the gravitational potential. Nordström's theory of gravitation was remarkable because it was shown by Einstein and Adriaan Fokker (1914), that in this model gravitation can be completely described in terms of space-time curvature. Although Nordström's theory is without contradiction, from Einstein's point of view a fundamental problem persisted: It doesn't fulfill the important condition of general covariance, as in this theory preferred frames of reference can still be formulated. So contrary to those "scalar theories", Einstein (1911–1915) developed a "tensor theory" (i.e. general relativity), which fulfills both the equivalence principle and general covariance. As a consequence, the notion of a complete "special relativistic" theory of gravitation had to be given up, as in general relativity the constancy of light speed (and Lorentz covariance) is only locally valid. The decision between those models was brought about by Einstein, when he was able to exactly derive the perihelion precession of Mercury, while the other theories gave erroneous results. In addition, Einstein's theory was the only theory which gave the correct value for the deflection of light near the sun. Quantum field theory. The need to put together relativity and quantum mechanics was one of the major motivations in the development of quantum field theory. Pascual Jordan and Wolfgang Pauli showed in 1928 that quantum fields could be made to be relativistic, and Paul Dirac produced the Dirac equation for electrons, and in so doing predicted the existence of antimatter. Many other domains have since been reformulated with relativistic treatments: relativistic thermodynamics, relativistic statistical mechanics, relativistic hydrodynamics, relativistic quantum chemistry, relativistic heat conduction, etc. Experimental evidence. Important early experiments confirming special relativity as mentioned above were the Fizeau experiment, the Michelson–Morley experiment, the Kaufmann–Bucherer–Neumann experiments, the Trouton–Noble experiment, the experiments of Rayleigh and Brace, and the Trouton–Rankine experiment. In the 1920s, a series of Michelson-Morley type experiments were conducted, confirming relativity to even higher precision than the original experiment. Another type of interferometer experiment was the Kennedy–Thorndike experiment in 1932, by which the independence of the speed of light on the apparatus' velocity was confirmed. Also time dilation was directly measured in the Ives–Stilwell experiment in 1938 and by measuring the decay rates of moving particles in 1940. All of those experiments have been repeated several times with increased precision. In addition, that the speed of light is unreachable for massive bodies was measured in many tests of relativistic energy and momentum. Therefore, knowledge of those relativistic effects is required in the construction of particle accelerators. Many other tests of special relativity have been conducted, testing possible violations of Lorentz invariance in some variants of quantum gravity. However, no sign of anisotropy of the speed of light has been found even at the 10−17 level, and some experiments even ruled out Lorentz violations at the 10−40 level, see Modern searches for Lorentz violation. Priority. Some claim that Poincaré (and Lorentz), not Einstein, are the true founders of special relativity. For more see the article on relativity priority dispute. Criticisms. Some criticized Special Relativity for various reasons, such as lack of empirical evidence, internal inconsistencies, rejection of mathematical physics "per se", or philosophical reasons. Although there still are critics of relativity outside the scientific mainstream, the overwhelming majority of scientists agree that Special Relativity has been verified in many different ways and there are no inconsistencies within the theory [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@58777339 [docID]1790788 [hitPos]14 [correct]false [extraScores][F@251d5e09 , [answer]The aberration of light (also referred to as astronomical aberration or stellar aberration) is an astronomical phenomenon which produces an apparent motion of celestial objects about their locations dependent on the velocity of the observer. Aberration causes objects to appear to be angled or tilted towards the direction of motion of the observer compared to when the observer is stationary. The change in angle is typically very small, on the order of "v/c" where "c" is the speed of light and "v" the velocity of the observer. In the case of "stellar" or "annual" aberration, the apparent position of a star to an observer on Earth varies periodically over the course of a year as the Earth's velocity changes as it revolves around the Sun, by a maximum angle of approximately 20 arcseconds in right ascension or declination. Aberration is historically significant because of its role in the development of the theories of light, electromagnetism and, ultimately, the theory of Special Relativity. It was first observed in the late 1600s by astronomers searching for stellar parallax in order to confirm the heliocentric model of the solar system, much to their surprise. In 1729, James Bradley provided a classical explanation for it in terms of the finite speed of light relative to the motion of the Earth in its orbit around the Sun, which he used to make one of the earliest measurements of the speed of light. However, Bradley's theory was incompatible with 19th century theories of light, and aberration became a major motivation for the aether drag theories of Augustin Fresnel (in 1818) and G. G. Stokes (in 1845), and for Hendrick Lorentz' aether theory of electromagnetism in 1892. The aberration of light, together with Lorentz' elaboration of Maxwell's electrodynamics, the moving magnet and conductor problem, the negative aether drift experiments, as well as the Fizeau experiment, led Albert Einstein to develop the theory of Special Relativity in 1905, which provided a conclusive explanation for the aberration phenomenon. The term 'aberration' has historically been used to refer to a number of related phenomena concerning the propagation of light in moving bodies Aberration should not be confused with stellar parallax. The latter is caused by a change in the position of the observer looking at a relatively nearby object (theoretically, at any object outside the Solar System); the former is related to light-time correction and relativistic beaming, although it is often considered separately from these effects. The term "aberration" may also be used to refer to unrelated phenomena in optical systems — optical aberration. Aberration should also be distinguished from light-time correction, which is due to the motion of the observed object, like a planet, through space during the time taken by its light to reach an observer on Earth. Light-time correction depends upon the velocity and distance "of the emitting object" during the time it takes for its light to travel to Earth. Light-time correction does not depend on the motion of the Earth – it only depends on Earth's "position" at the instant when the light is observed. Aberration is usually larger than a planet's light-time correction except when the planet is near quadrature (90° from the Sun), where aberration drops to zero because then the Earth is directly approaching or receding from the planet. At opposition to or conjunction with the Sun, aberration is 20.5″ while light-time correction varies from 4″ for Mercury to 0.37″ for Neptune (the Sun's light-time correction is less than 0.03″). Explanation. Aberration may be explained as the difference in angle of a beam of light in different inertial frames of reference. A common analogy is to the apparent direction of falling rain: If rain is falling vertically in the frame of reference of a person standing still, then to a person moving forwards the rain will appear to arrive at an angle, requiring the moving observer to tilt their umbrella forwards. The faster the observer moves, the more tilt is needed. The net effect is that light rays striking the moving observer from the sides in a stationary frame will come angled from ahead in the moving observer's frame. This effect is sometimes called the "searchlight" or "headlight" effect. In the case of annual aberration of starlight, the direction of incoming starlight as seen in the Earth's moving frame is tilted relative to the angle observed in the Sun's frame. Since the direction of motion of the Earth changes during its orbit, the direction of this tilting changes during the course of the year, and causes the apparent position of the star to differ from its true position as measured in the inertial frame of the Sun. While classical reasoning gives intuition for aberration, it leads to a number of physical paradoxes observable even at the classical level (see history). The theory of Special Relativity is required to correctly account for aberration. The relativistic explanation is very similar to the classical one however, and in both theories aberration may be understood as a case of velocity addition. Classical Explanation. In the Sun's frame, consider a beam of light with velocity equal to the speed of light c, with x and y velocity components formula_1 and formula_2, at an angle formula_3. If the Earth is moving at velocity formula_4 in the x direction relative to the Sun, then by velocity addition the x component of the beam's velocity in the Earth's frame of reference is formula_5, and the y velocity is unchanged, formula_6. (Note that you need the velocity of the sun with respect to the earth which is the negative of the velocity of the earth with respect to the sun. Also note that we are only using vectors here without indication of direction.) Thus the angle of the light in the Earth's frame in terms of the angle in the Sun's frame is In the case of formula_8, this result reduces to formula_9. Relativistic Explanation. The reasoning in the relativistic case is the same except that the relativistic velocity addition formulae must be used, which can be derived from Lorentz transformations between different frames of reference. These formulae are where formula_12, giving the components of the light beam in the Earth's frame in terms of the components in the Sun's frame. The angle of the beam in the Earth's frame is thus In the case of formula_8, this result reduces to formula_15, and in the limit formula_16 this may be approximated by formula_17. This relativistic derivation keeps the speed of light formula_18 constant in all frames of reference, unlike the classical derivation above. Relationship to Light-Time Correction and Relativistic Beaming. Aberration is related to two other phenomena, Light-time correction, which is due to the motion of an observed object during the time taken by its light to reach an observer, and relativistic beaming, which is an angling of the light emitted by a moving light source. It can be considered equivalent to them but in a different inertial frame of reference. In aberration, the observer is considered to be moving relative to a (for the sake of simplicity) stationary light source, while in light-time correction and relativistic beaming the light source is considered to be moving relative to a stationary observer. Consider the case of an observer and a light source moving relative to each other at constant velocity, with a light beam moving from the source to the observer. At the moment of emission, the beam in the observer's rest frame is tilted compared to the one in the source's rest frame, as understood through relativistic beaming. During the time it takes the light beam to reach the observer the light source moves in the observer's frame, and the 'true position' of the light source is displaced relative to the apparent position the observer sees, as explained by light-time correction. Finally, the beam in the observer's frame at the moment of observation is tilted compared to the beam in source's frame, which can be understood as an aberrational effect. Thus, a person in the light source's frame would describe the apparent tilting of the beam in terms of aberration, while a person in the observer's frame would describe it as a light-time effect. The relationship between these phenomena is only valid if the observer and source's frames are inertial frames. In practice, because the Earth is not an inertial rest frame but experiences centripetal acceleration towards the Sun, many aberrational effects such as annual aberration on Earth cannot be considered light-time corrections. However, if the time between emission and detection of the light is short compared to the orbital period of the Earth, the Earth may be approximated as an inertial frame and aberrational effects are equivalent to light-time corrections. Types of aberration. There are a number of types of aberration, caused by the differing components of the Earth's motion: Annual aberration. Annual aberration is caused by the motion of an observer on the Earth revolving around the Sun. The velocity formula_4 of the Earth (in the Sun's rest frame) varies periodically over the course of a year as the Earth traverses its orbit and consequently the aberration also varies periodically, typically causing stars to appear to move in small ellipses. Approximating the Earth's orbit as circular, the maximum displacement of a star due to annual aberration is known as the "constant of aberration", conventionally represented by formula_20. It may be calculated using the relation formula_21 substituting the Earth's average speed in the Sun's frame for formula_4 and the speed of light formula_23. Its accepted value is 20″.49552  arcseconds (at J2000). Assuming a circular orbit, annual aberration causes stars exactly on the ecliptic (the plane of the Earth's orbit) to appear to move back and forth along a straight line, varying by formula_20 on either side of their position in the Sun's frame. A star that is precisely at one of the ecliptic poles (at 90 degrees from the ecliptic plane) will appear to move in a circle of radius formula_20 about its true position, and stars at intermediate ecliptic latitudes will appear to move along a small ellipse. For illustration, consider a star at the northern ecliptic pole viewed by an observer on the 'top' of the earth (towards the ecliptic pole), at a point on the arctic circle. At the time of the March equinox, the Earth's orbit carries the observer in a southwards direction, and the star's apparent declination is therefore displaced to the south by an angle of formula_20. At the September equinox, the star's position is displaced to the north by an equal and opposite amount. At the June and December solstices, the displacement in declination is zero. Conversely, the amount of displacement in right ascension is zero at either equinox and maximum at the solstices. In practice the Earth's orbit is slightly elliptic rather than circular and its speed changes somewhat over the course of its orbit, which means the description above is only approximate. Aberration is more accurately calculated using the Earth's instantaneous velocity relative to the center of mass of the Solar System. Note that the displacement due to aberration is orthogonal to any displacement due to parallax. If parallax were detectable, the maximum displacement to the south would occur in December, and the maximum displacement to the north in June. It is this apparently anomalous motion that so mystified early astronomers. Solar annual aberration. A special case of annual aberration is the nearly constant deflection of the Sun from its position in the Sun's rest frame by formula_20 towards the "west" (as viewed from Earth), opposite to the apparent motion of the Sun along the ecliptic (which is from west to east, as seen from Earth). The deflection thus makes the Sun appear to be behind (or retarded) from its rest-frame position on the ecliptic by a position or angle formula_20. This deflection may equivalently be described as a light-time effect due to motion of the Earth during the 8.3 minutes that it takes light to travel from the Sun to Earth. This is possible since the transit time of sunlight is short relative to the orbital period of the Earth, so the Earth's frame may be approximated as inertial. In the Earth's frame, the Sun moves by a distance formula_29 in the time it takes light to reach Earth, formula_30 for the orbit of radius formula_31. This gives an angular correction formula_32 which can be solved to give formula_33, the same as the aberrational correction. Planetary aberration. Planetary aberration is the combination of the aberration of light (due to Earth's velocity) and light-time correction (due to the object's motion and distance), as calculated in the rest frame of the Solar System. Both are determined at the instant when the moving object's light reaches the moving observer on Earth. It is so called because it is usually applied to planets and other objects in the solar system whose motion and distance are accurately known. Diurnal aberration. Diurnal aberration is caused by the velocity of the observer on the surface of the rotating Earth. It is therefore dependent not only on the time of the observation, but also the latitude and longitude of the observer. Its effect is much smaller than that of annual aberration, and is only 0′′.32 in the case of an observer at the equator, where the rotational velocity is greatest. Secular aberration. The Sun and Solar System are revolving around the center of the Galaxy. Aberration due to this motion is known as secular aberration and affects the apparent positions of distant stars and extragalactic objects. However, since the galactic year is about 230 million years the aberration varies very slowly the change in aberration is extremely difficult to observe. Therefore secular aberration is usually ignored when considering the positions of stars. In other words, star maps show the observed apparent positions of the stars, not their calculated true positions after accounting for secular aberration. For stars significantly less than 230 million light years away, the Solar System may be approximated as an inertial frame and so the effect of secular aberration is equivalent to a light-time correction. This includes stars in the Milky Way, since the Milky Way is about 100,000 light years in diameter. For these stars the true position of the star is then easily computed from the product of its proper motion (in arcseconds per year) and its distance (in light years). Secular aberration is typically a small number of arcminutes, for example the stationary star Groombridge 1830 is displaced by approximately 3 arcminutes. due to secular aberration. This is roughly 8 times the effect of annual aberration, as one would expect since the velocity of the Solar System relative to the Milky Way is about 8 times the velocity of the Earth relative to the Sun. Discovery and First Observations. The discovery of the aberration of light was totally unexpected, and it was only by extraordinary perseverance and perspicacity that Bradley was able to explain it in 1727. Its origin is based on attempts made to discover whether the stars possessed appreciable parallaxes. The Copernican theory of the solar system – that the Earth revolved annually about the Sun – had received confirmation by the observations of Galileo and Tycho Brahe and the mathematical investigations of Kepler and Newton. Search for stellar parallax. As early as 1573, Thomas Digges had suggested that parallactic shifting of the stars should occur according to the heliocentric model of the Solar System, and consequently if such stellar parallaxes could be observed they would help confirm the heliocentric theory. Many observers claimed to have determined such parallaxes, but Tycho Brahe and Giovanni Battista Riccioli concluded that they existed only in the minds of the observers, and were due to instrumental and personal errors. In 1680 Jean Picard, in his "Voyage d’Uranibourg," stated, as a result of ten years' observations, that Polaris, or the Pole Star, exhibited variations in its position amounting to 40″ annually. Some astronomers endeavoured to explain this by parallax, but these attempts were futile, for the motion was at variance with that which parallax would produce. John Flamsteed, from measurements made in 1689 and succeeding years with his mural quadrant, similarly concluded that the declination of the Pole Star was 40″ less in July than in September. Robert Hooke, in 1674, published his observations of γ Draconis, a star of magnitude 2m which passes practically overhead at the latitude of London, and whose observations are therefore free from the complex corrections due to astronomical refraction, and concluded that this star was 23″ more northerly in July than in October. James Bradley's Observations. When James Bradley and Samuel Molyneux entered this sphere of astronomical research in 1725, there consequently prevailed much uncertainty whether stellar parallaxes had been observed or not; and it was with the intention of definitely answering this question that these astronomers erected a large telescope at the house of the latter at Kew. They determined to reinvestigate the motion of γ Draconis; the telescope, constructed by George Graham (1675–1751), a celebrated instrument-maker, was affixed to a vertical chimney stack, in such manner as to permit a small oscillation of the eyepiece, the amount of which (i.e. the deviation from the vertical) was regulated and measured by the introduction of a screw and a plumb line. The instrument was set up in November 1725, and observations on γ Draconis were made starting in December. The star was observed to move 40″ southwards between September and March, reversing its course from March to September. These results were unexpected and inexplicable by existing theories. Early Hypotheses. This motion was evidently not due to parallax nor was it due to observational errors. Bradley and Molyneux discussed several hypotheses in the hope of finding the solution. Bradley first hypothesized that the apparent motion could be due to oscillations in the orientation of the Earth's axis relative to the celestial sphere – a phenomenon known as nutation. This could be tested using the fact the apparent position of stars on the opposite side of the celestial sphere would be affected by an equal and opposite amount. Bradley tested this using a star with a right ascension nearly exactly opposite to that of γ Draconis. This star was seen to possess an apparent motion which could be consistent with nutation, but since its declination varied only one half as much as in the case of γ Draconis, it was obvious that nutation did not supply the requisite solution. Although nutation could not explain the observed stellar motion, Bradely later went on to discover that the Earth does indeed nutate. Bradley also investigated the possibility that the motion was due to an irregular distribution of the Earth's atmosphere, thus involving abnormal variations in the refractive index, but again obtained negative results. On August 19, 1727, Bradley then embarked upon a further series of observations using a telescope of his own erected at the Rectory, Wanstead. This instrument had the advantage of a larger field of view and he was able to obtain precise positions of a large number of stars over the course of about two years. This established the existence of the phenomenon of aberration beyond all doubt, and also allowed Bradley to formulate a set of rules that would allow the calculation of the effect on any given star at a specified date. Development of the theory of aberration. Bradley eventually developed the explanation of aberration in about September 1728 and his theory was presented to the Royal Society in mid January the next year. Based on his early calculations, Bradley was able to estimate the constant of aberration at 20", and with this able to estimate the speed of light at per second.One well-known story was that he saw the change of direction of a wind vane on a boat on the Thames, caused not by an alteration of the wind itself, but by a change of course of the boat relative to the wind direction. However, there is no record of this incident in Bradley's own account of the discovery, and it may therefore be apocryphal. The discovery and elucidation of aberration is now regarded as a classic case of the application of scientific method, in which observations are made to test a theory, but unexpected results are sometimes obtained that in turn lead to new discoveries. It is also worth noting that part of the original motivation of the search for stellar parallax was to test the Copernican theory that the Earth revolves around the Sun, but of course the existence of aberration also establishes the truth of that theory. Historical Theories of Aberration. The phenomenon of aberration became a driving force for many physical theories during the 200 years between its observation and the conclusive explanation by Albert Einstein. The first classical explanation was provided in 1729 by James Bradley as described above, who attributed it to the finite speed of light and the motion of Earth in its orbit around the Sun. However, this explanation proved inaccurate once the wave nature of light was better understood, and correcting it became a major goal of the 19th century theories of luminiferous aether. Augustin-Jean Fresnel proposed a correction due to the motion of a medium (the aether) through which light propagated, known as "partial aether drag". He proposed that objects partially drag the aether along with them as they move, and this became the accepted explanation for aberration for some time. George Stokes proposed a similar theory, explaining that aberration occurs due to the flow of aether induced by the motion of the Earth. Accumulated evidence against these explanations combined with new understanding of the electromagnetic nature of light led Hendrik Lorentz to develop an electron theory which featured an immobile aether, and he explained that objects contract in length as they move through the aether. Motivated by these previous theories Albert Einstein then developed the theory of Special Relativity in 1905 which provides the modern account of aberration. Bradley's Classical Explanation. Bradley conceived of an explanation in terms of a corpuscular theory of light in which light is made of particles unaffected by gravity. His classical explanation appeals to the motion of the earth relative to a beam of light-particles moving at a finite velocity, and is developed in the Sun's frame of reference, unlike the classical derivation given above. Consider the case where a distant star is motionless relative to the Sun, and the star is extremely far away, so that parallax may be ignored. In the rest frame of the Sun, this means light from the star travels in parallel paths to the Earth observer, and arrives at the same angle regardless of where the Earth is in its orbit. Suppose the star is observed on Earth with a telescope, idealized as a narrow tube. The light enters the tube from the star at angle formula_34 and travels at speed formula_23 taking a time formula_36 to reach the bottom of the tube, where it is detected. Suppose observations are made from Earth, which is moving with a speed formula_4. During the transit of the light, the tube moves a distance formula_38. Consequently, for the particles of light to reach the bottom of the tube, the tube must be inclined at an angle formula_39 different from formula_34, resulting in an "apparent" position of the star at angle formula_39. As the Earth proceeds in its orbit it changes direction, so formula_39 changes with the time of year the observation is made. The apparent angle and true angle are related using trigonometry as: In the case of formula_8, this gives formula_45. While this is different from the more accurate relativistic result described above, in the limit of small angle and low velocity they are approximately the same, within the error of the measurements of Bradley's day. These results allowed Bradley to make one of the earliest measurements of the speed of light. Luminiferous Aether. In the early 19th century the wave theory of light was being rediscovered, and in 1804 Thomas Young adapted Bradley's explanation for corpuscular light to wavelike light traveling through a medium known as the luminiferous aether. His reasoning was the same as Bradley's, but it required that this medium be immobile in the Sun's reference frame and must pass through the earth unaffected, otherwise the medium (and therefore the light) would move along with the earth and no aberration would be observed. </ref> He wrote: However, it soon became clear Young's theory could not account for aberration when materials with a non-vacuum index of refraction were present. An important example is of a telescope filled with water. The velocity of the light in such a telescope will be slower than in vacuum, and is given by formula_46 rather than formula_23 where formula_48 is the index of refraction of the water. Thus, by Bradley and Young's reasoning the aberration angle is given by which predicts a medium-dependent angle of aberration. When refraction at the telescope's objective is taken into account this result deviates even more from the vacuum result. In 1810 François Arago performed a similar experiment and found that the aberration was unaffected by the medium in the telescope, providing solid evidence against Young's theory. This experiment was subsequently verified by many others in the following decades, most accurately by Airy in 1871, with the same result. Aether Drag Models. Fresnel's Aether Drag. In 1818 Augustin Fresnel developed a modified explanation to account for the water telescope and for other aberration phenomena. He explained that the aether is generally at rest in the Sun's frame of reference, but objects partially drag the aether along with them as they move. That is, the aether in an object of index of refraction formula_48 moving at velocity formula_4 is partially dragged with a velocity formula_52 bringing the light along with it. This factor is known as "Fresnel's dragging coefficient". This dragging effect, along with refraction at the telescope's objective, compensates for the slower speed of light in the water telescope in Bradley's explanation. With this modification Fresnel obtained Bradley's vacuum result even for non-vacuum telescopes, and was also able to predict many other phenomena related to the propagation of light in moving bodies. Fresnel's dragging coefficient became the dominant explanation of aberration for the next decades. Stokes' Aether Drag. However, the fact that light is polarized (discovered by Fresnel himself) led scientists such as Cauchy and Green to believe that the aether was a totally immobile elastic solid as opposed to Fresnel's fluid aether. There was thus renewed need for an explanation of aberration consistent both with Fresnel's predictions (and Arago's observations) as well as polarization. In 1845 Stokes proposed a 'putty-like' aether which acts as a liquid on large scales but as a solid on small scales, thus supporting both the transverse vibrations required for polarized light and the aether flow required to explain aberration. Making only the assumptions that the fluid is irrotational and that the boundary conditions of the flow are such that the aether has zero velocity far from the Earth, but moves at the Earth's velocity at its surface and within it, he was able to completely account for aberration. The velocity of the aether outside of the Earth would decrease as a function of distance from the Earth so light rays from stars would be progressively dragged as they approached the surface of the Earth. The Earth's motion would be unaffected by the aether due to D'Alembert's paradox. Both Fresnel and Stokes' theories were popular. However, the question of aberration was put aside during much of the second half of the 19th century as focus of inquiry turned to the electromagnetic properties of aether. Lorentz' Length Contraction. In the 1880s once electromagnetism was better understood, interest turned again to the problem of aberration. By this time flaws were known to both Fresnel's and Stokes' theories. Fresnel's theory required that the relative velocity of aether and matter to be different for light of different colors, and it was shown that the boundary conditions Stokes had assumed in his theory were inconsistent with his assumption of irrotational flow. At the same time, the modern theories of electromagnetic aether could not account for aberration at all. Many scientists such as Maxwell, Heaviside and Hertz unsuccessfully attempted to solve these problems by incorporating either Fresnel or Stokes' theories into Maxwell's new electromagnetic laws. Hendrik Lorentz spent considerable effort along these lines. After working on this problem for a decade, the issues with Stokes' theory caused him to abandon it and to follow Fresnel's suggestion of a (mostly) stationary aether (1892, 1895). However, in Lorentz's model the aether was "completely" immobile, like the electromagnetic aethers of Cauchy, Green and Maxwell and unlike Fresnel's aether. He obtained Fresnel's dragging coefficient from modifications of Maxwell's electromagnetic theory, including a modification of the time coordinates in moving frames ("local time"). In order to explain the Michelson-Morely experiment (1887), which apparently contradicted both Fresnel's and Lorentz's immobile aether theories, and apparently confirmed Stokes' complete aether drag, Lorentz theorized (1892) that objects undergo "length contraction" by a factor of formula_53 in the direction of their motion through the aether. In this way, aberration (and all related optical phenomena) can be accounted for in the context of an immobile aether. Lorentz' theory became the basis for much research in the next decade, and beyond. Its predictions for aberration are identical to those of the relativistic theory. Special relativity. Lorentz' theory matched experiment well, but it was complicated and made many unsubstantiated physical assumptions about the microscopic nature of electromagnetic media. In his 1905 theory of Special Relativity, Albert Einstein reinterpreted the results of Lorentz' theory in a much simpler and more natural conceptual framework which disposed of the idea of an aether. His derivation is given above, and is now the accepted explanation. Robert S. Shankland reported some conversations with Einstein, in which Einstein emphasized the importance of aberration: Other important motivations for Einstein's development of relativity were the moving magnet and conductor problem and (indirectly) the negative aether drift experiments, already mentioned by him in the introduction of his first relativity paper. Einstein wrote in a note in 1952: While Einstein's result is the same as Bradley's original equation except for an extra factor of formula_54, it should be emphasized that Bradley's result does not merely give the classical limit of the relativistic case, in the sense that it gives incorrect predictions even at low relative velocities. Bradley's explanation cannot account for situations such as the water telescope, nor for many other optical effects (such as interference) that might occur within the telescope. This is because in the Earth's frame it predicts that the direction of propagation of the light beam in the telescope is not normal to the wavefronts of the beam, in contradiction with Maxwell's theory of electromagnetism. It also does not preserve the speed of light c between frames. However, Bradley did correctly infer that the effect was due to relative velocities. External links. . Endnotes to that edition [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@244efc35 [docID]2703 [hitPos]15 [correct]false [extraScores][F@60fee615 , [answer]Interstellar space travel is manned or unmanned travel between stars. The concept of interstellar travel via starships is a staple of science fiction. Interstellar travel is conceptually much more difficult than interplanetary travel. The distance between the planets in the Solar System is typically measured in standard astronomical units, while the distance between the stars is hundreds of thousands of AU and often expressed in light years. Intergalactic travel, or travel between different galaxies, would be even more difficult. A variety of concepts have been discussed in the literature, since the first astronautical pioneers, such as Konstantin Tsiolkovsky, Robert Esnault-Pelterie and Robert Hutchings Goddard. Given sufficient travel time and engineering work, both unmanned and sleeper ship interstellar travel requires no break-through physics to be achieved, but considerable technological and economic challenges need to be met. NASA, ESA and other space agencies have been engaging in research into these topics for decades, and have accumulated a number of theoretical approaches. Difficulties. The main challenge facing interstellar travel is the immense distances between the stars. This means both great speed and a long travel time are required. The time required by propulsion methods based on currently known physical principles would require years to millennia. Hence an interstellar ship would face manifold hazards found in interplanetary travel, including vacuum, radiation, weightlessness, and micrometeoroids. Even the minimum multi-year travel times to the nearest stars are beyond current manned space mission design experience. The fundamental limits of spacetime present another challenge. The distances between stars isn't a problem in and of itself. Virtually all the material that would pose a problem is in our solar system along the disk that contains the planets, asteroid belt, Oort cloud, comets, free asteroids, macro and micro-meteroids, etc. So any device or projectile must be sent in a direction opposite of all of this material. The larger the object humans send, the greater the chances of it hitting something or vice versa. One option is to project something very small where the chance of it striking or being struck by something is virtually non-existent in the vacuum of interplanetary and interstellar space. Required energy. A significant factor contributing to the difficulty is the energy which must be supplied to obtain a reasonable travel time. A lower bound for the required energy is the kinetic energy K = ½ mv2 where m is the final mass. If deceleration on arrival is desired and cannot be achieved by any means other than the engines of the ship, then the required energy at least doubles, because the energy needed to halt the ship equals the energy needed to accelerate it to travel speed. The velocity for a manned round trip of a few decades to even the nearest star is several thousand times greater than those of present space vehicles. This means that due to the v2 term in the kinetic energy formula, millions of times as much energy is required. Accelerating one ton to one-tenth of the speed of light requires at least 450 PJ or 4.5  J or 125 billion kWh, without factoring in efficiency of the propulsion mechanism. This energy has to be either generated on-board from stored fuel, harvested from the interstellar medium, or projected over immense distances. The energy requirements make interstellar travel very difficult. It has been reported that at the 2008 Joint Propulsion Conference, multiple experts opined that it was improbable that humans would ever explore beyond the Solar System. Brice N. Cassenti, an associate professor with the Department of Engineering and Science at Rensselaer Polytechnic Institute, stated that at least the total energy output of the entire world a given year would be required to send a probe to the nearest star. Interstellar medium. A major issue with traveling at extremely high speeds is that interstellar dust and gas may cause considerable damage to the craft, due to the high relative speeds and large kinetic energies involved. Various shielding methods to mitigate this problem have been proposed. Larger objects (such as macroscopic dust grains) are far less common, but would be much more destructive. The risks of impacting such objects, and methods of mitigating these risks, have been discussed in the literature, but many unknowns remain. Travel time. It has been argued that an interstellar mission which cannot be completed within 50 years should not be started at all. Instead, assuming that a civilization is still on an increasing curve of propulsion system velocity, not yet having reached the limit, the resources should be invested in designing a better propulsion system. This is because a slow spacecraft would probably be passed by another mission sent later with more advanced propulsion (Incessant Obsolescence Postulate). On the other hand, Andrew Kennedy has shown that if one calculates the journey time to a given destination as the rate of travel speed derived from growth (even exponential growth) increases, there is a clear minimum in the total time to that destination from now (see wait calculation). Voyages undertaken before the minimum will be overtaken by those who leave at the minimum, while those who leave after the minimum will never overtake those who left at the minimum. One argument against the stance of delaying a start until reaching fast propulsion system velocity is that the various other non-technical problems that are specific to long-distance travel at considerably higher speed (such as interstellar particle impact, possible dramatic shortening of average human life span during extended space residence, etc.) may remain obstacles that take much longer time to resolve than the propulsion issue alone, assuming that they can even be solved eventually at all. A case can therefore be made for starting a mission without delay, based on the concept of an achievable and dedicated but relatively slow interstellar mission using the current technological state-of-the-art and at relatively low cost, rather than banking on being able to solve all problems associated with a faster mission without having a reliable time frame for achievability of such. Intergalactic travel involves distances about a million-fold greater than interstellar distances, making it radically more difficult than even interstellar travel. Interstellar distances. Astronomical distances are often measured in the time it would take a beam of light to travel between two points ("see light-year"). Light in a vacuum travels approximately 300,000 kilometers per second or 186,000 miles per second. The distance from Earth to the Moon is 1.3 light-seconds. With current spacecraft propulsion technologies, a craft can cover the distance from the Earth to the Moon in around eight hours (New Horizons). That means light travels approximately thirty thousand times faster than current spacecraft propulsion technologies. The distance from Earth to other planets in the Solar System ranges from three light-minutes to about four light-hours. Depending on the planet and its alignment to Earth, for a typical unmanned spacecraft these trips will take from a few months to a little over a decade. The nearest known star to the Sun is Proxima Centauri, which is 4.23 light-years away. However, there may be undiscovered brown dwarf systems that are closer. The fastest outward-bound spacecraft yet sent, Voyager 1, has covered 1/600th of a light-year in 30 years and is currently moving at 1/18,000th the speed of light. At this rate, a journey to Proxima Centauri would take 80,000 years. Of course, this mission was not specifically intended to travel fast to the stars, and current technology could do much better. The travel time could be reduced to a millennium using solar sails, or to a century or less using nuclear pulse propulsion. A better understanding of the vastness of the interstellar distance to one of the closest stars to the sun, Alpha Centauri A (a Sun-like star), can be obtained by scaling down the Earth-Sun distance (~150,000,000 km) to one meter. On this scale the distance to Alpha Centauri A would still be 271 kilometers or about 169 miles. However, more speculative approaches to interstellar travel offer the possibility of circumventing these difficulties. Special relativity offers the possibility of shortening the travel time: if a starship with sufficiently advanced engines could reach velocities approaching the speed of light, relativistic time dilation would make the voyage much shorter for the traveler. However, it would still take many years of elapsed time as viewed by the people remaining on Earth, and upon returning to Earth, the travelers would find that far more time had elapsed on Earth than had for them. (For more on this effect, see twin paradox.) General relativity offers the theoretical possibility that faster-than-light travel may be possible without violating fundamental laws of physics, for example, through wormholes, although it is still debated whether this is possible, in part, because of causality concerns. Proposed mechanisms for faster-than-light travel within the theory of general relativity require the existence of exotic matter. Communications. The round-trip delay time is the minimum time between an observation by the probe and the moment the probe can receive instructions from Earth reacting to the observation. Given that information can travel no faster than the speed of light, this is for the Voyager 1 about 17 hours, near Proxima Centauri it would be 8 years. Faster reaction would have to be programmed to be carried out automatically. Of course, in the case of a manned flight the crew can respond immediately to their observations. However, the round-trip delay time makes them not only extremely distant from, but, in terms of communication, also extremely isolated from Earth (analogous to how past long distance explorers were similarly isolated before the invention of the electrical telegraph). Interstellar communication is still problematic — even if a probe could reach the nearest star, its ability to communicate back to Earth would be difficult given the extreme distance. See Interstellar communication. Prime targets for interstellar travel. There are 59 known stellar systems within 20 light years from the Sun, containing 81 visible stars. The following could be considered prime targets for interstellar missions: Existing and near-term astronomical technology is capable of finding planetary systems around these objects, increasing their potential for exploration. Manned missions. The mass of any craft capable of carrying humans would inevitably be substantially larger than that necessary for an unmanned interstellar probe. For instance, the first space probe, Sputnik 1, had a payload of 83.6 kg, while spacecraft to carry a living passenger (Laika the dog), Sputnik 2, had a payload six times that at 508.3 kg. This underestimates the difference in the case of interstellar missions, given the vastly greater travel times involved and the resulting necessity of a closed-cycle life support system. As technology continues to advance, combined with the aggregate risks and support requirements of manned interstellar travel, the first interstellar missions are unlikely to carry earthly life forms. A manned craft will require more time to reach its top speed as humans have limited tolerance to acceleration. Time dilation. Assuming one can not travel faster than light, one might conclude that a human can never make a round-trip further from the Earth than 40 light years if the traveler is active between the ages of 20 and 60. So a traveler would never be able to reach more than the very few star systems which exist within the limit of 10–20 light years from the Earth. But that would be a mistaken conclusion because it fails to take into account time dilation. Informally explained, clocks aboard ship run slower than Earth clocks, so if the ship engines are powerful enough the ship can reach mostly anywhere in the galaxy and go back to Earth within 40 years ship-time. The problem is that there is a difference between the time elapsed in the astronaut's ship and the time elapsed on Earth. An example will make this clearer. Suppose a spaceship travels to a star 32 light years away. First it accelerates at a constant 1.03g (i.e., 10.1 m/s2) for 1.32 years (ship time). Then it stops the engines and coasts for the next 17.3 years (ship time) at a constant speed. Then it decelerates again for 1.32 ship-years so as to come at a stop at the destination. The astronaut takes a look around and comes back to Earth the same way. After the full round-trip, the clocks on board the ship show that 40 years have passed, but according to Earth calendar the ship comes back 76 years after launch. So, the overall average speed is 0.84 lightyears per earth year, or 1.6 lightyears per ship year. This is possible because at a speed of 0.87 c, time on board the ship seems to run slower. Every two Earth years, ship clocks advance 1 year. From the viewpoint of the astronaut, onboard clocks seem to be running normally. The star ahead seems to be approaching at a speed of 0.87 lightyears per ship year. As all the universe looks contracted along the direction of travel to half the size it had when the ship was at rest, the distance between that star and the Sun seems to be 16 light years as measured by the astronaut, so it's no wonder that the trip at 0.87 ly per shipyear takes 20 ship years. At higher speeds, the time onboard will run even slower, so the astronaut could travel to the center of the Milky Way (30 kly from Earth) and back in 40 years ship-time. But the speed according to Earth clocks will always be less than 1 lightyear per Earth year, so, when back home, the astronaut will find that 60 thousand years will have passed on Earth. Constant acceleration. Regardless of how it is achieved, if a propulsion system can produce 1 g of acceleration continuously from departure to destination, then this will be the fastest method of travel. If the propulsion system drives the ship faster and faster for the first half of the journey, then turns around and brakes the craft so that it arrives at the destination at a standstill, this is a constant acceleration journey. This would also have the advantage of producing constant gravity. From the planetary observer perspective the ship will appear to steadily accelerate but more slowly as it approaches the speed of light. The ship will be close to the speed of light after about a year of accelerating and remain at that speed until it brakes for the end of the journey. From the ship perspective there will be no top limit on speed – the ship keeps going faster and faster the whole first half. This happens because the ship's time sense slows down – relative to the planetary observer – the more it approaches the speed of light. The result is an impressively fast journey if you are in the ship. Here is a table of journey times, in years, for various constant accelerations. Note again, that times observed from the planetary frame of reference (which applies to both departure and destination points) are very different from those observed in the space craft, and that from the ship frame of reference there is no limit on the top speed. Proposed methods. Slow manned missions. Potential slow manned interstellar travel missions, based on current and near-future propulsion technologies are associated with trip times, starting from about one hundred years to thousands of years. The duration of a slow interstellar journey presents a major obstacle and existing concepts deal with this problem in different ways. They can be distinguished by the "state" in which humans are transported on-board of the spacecraft. Generation ships. A generation ship (or world ship) is a type of interstellar ark in which the crew which arrives at the destination is descended from those who started the journey. Generation ships are not currently feasible because of the difficulty of constructing a ship of the enormous required scale and the great biological and sociological problems that life aboard such a ship raises. Suspended animation. Scientists and writers have postulated various techniques for suspended animation. These include human hibernation and cryonic preservation. While neither is currently practical, they offer the possibility of sleeper ships in which the passengers lie inert for the long years of the voyage. Extended human lifespan. A variant on this possibility is based on the development of substantial human life extension, such as the "Strategies for Engineered Negligible Senescence" proposed by Dr. Aubrey de Grey. If a ship crew had lifespans of some thousands of years, or had artificial bodies, they could traverse interstellar distances without the need to replace the crew in generations. The psychological effects of such an extended period of travel would potentially still pose a problem. Frozen embryos. A robotic space mission carrying some number of frozen early stage human embryos is another theoretical possibility. This method of space colonization requires, among other things, the development of a method to replicate conditions in a uterus, the prior detection of a habitable terrestrial planet, and advances in the field of fully autonomous mobile robots and educational robots which would replace human parents. Island hopping through interstellar space. Interstellar space is not completely empty; it contains trillions of icy bodies ranging from small asteroids (Oort cloud) to possible rogue planets. There may be ways to take advantage of these resources for a good part of an interstellar trip, slowly hopping from body to body or setting up waystations along the way. Faster manned missions. If a spaceship could average 10 percent of light speed (and decelerate at the destination, for manned missions), this would be enough to reach Proxima Centauri in forty years. Several propulsion concepts are proposed that might be eventually developed to accomplish this (see section below on propulsion methods), but none of them are ready for near-term (few decades) development at acceptable cost. Unmanned. Nanoprobes. Near-lightspeed nanospacecraft might be possible within the near future built on existing microchip technology with a newly developed nanoscale thruster. Researchers at the University of Michigan are developing thrusters that use nanoparticles as propellant. Their technology is called “nanoparticle field extraction thruster”, or nanoFET. These devices act like small particle accelerators shooting conductive nanoparticles out into space. Given the light weight of these probes, it would take much less energy to accelerate them. With on board solar cells they could continually accelerate using solar power. One can envision a day when a fleet of millions or even billions of these particles swarm to distant stars at nearly the speed of light, while relaying signals back to earth through a vast interstellar communication network. Ships with an external energy source. There are projects ships supplying energy from external sources such as using laser. Thus reduced mass of the ship due to the lack of energy system on board the ship that makes it easier and allows us to develop more speed. Geoffrey A. Landis proposed for interstellar travel future-technology project automatic space station with supplying the energy from an external source (laser of base station) and Ion thruster. Interstellar vehicles using electric propulsion, such as an ion rocket or plasma rocket, can also be powered via a laser beamed from a stationary power-supply. Propulsion. Rocket concepts. All rocket concepts are limited by the rocket equation, which sets the characteristic velocity available as a function of exhaust velocity and mass ratio, the ratio of initial ("M"0, including fuel) to final ("M"1, fuel depleted) mass. Very high specific power, the ratio of jet-power to total vehicle mass, is required to reach interstellar targets within sub-century time-frames. Some heat transfer is inevitable and a tremendous heating load must be adequately handled. Thus, for interstellar rocket concepts of all technologies, a key engineering problem (seldom explicitly discussed) is limiting the heat transfer from the exhaust stream back into the vehicle. Nuclear fission powered. Fission-electric. Nuclear-electric or plasma engines, operating for long periods at low thrust and powered by fission reactors, have the potential to reach speeds much greater than chemically powered vehicles or nuclear-thermal rockets. Such vehicles probably have the potential to power Solar System exploration with reasonable trip times within the current century. Because of their low-thrust propulsion, they would be limited to off-planet, deep-space operation. Electrically powered spacecraft propulsion powered by a portable power-source, say a nuclear reactor, producing only small cravings, a lot of weight needed to convert nuclear energy into electrical equipment and as a consequence low accelerations, would take centuries to reach for example 15% of the velocity of light, thus unsuitable for interstellar flight in during a one human lifetime. Fission-fragment. Fission-fragment rockets use nuclear fission to create high-speed jets of fission fragments, which are ejected at speeds of up to 12,000 km/s. With fission, the energy output is approximately 0.1% of the total mass-energy of the reactor fuel and limits the effective exhaust velocity to about 5% of the velocity of light. For maximum velocity, the reaction mass should optimally consist of fission products, the "ash" of the primary energy source, in order that no extra reaction mass need be book-kept in the mass ratio. This is known as a fission-fragment rocket. thermal-propulsion engines such as NERVA produce sufficient thrust, but can only achieve relatively low-velocity exhaust jets, so to accelerate to the desired speed would require an enormous amount of fuel. Nuclear pulse. Based on work in the late 1950s to the early 1960s, it has been technically possible to build spaceships with nuclear pulse propulsion engines, i.e. driven by a series of nuclear explosions. This propulsion system contains the prospect of very high specific impulse (space travel's equivalent of fuel economy) and high specific power. Project Orion team member, Freeman Dyson, proposed in 1968 an interstellar spacecraft using nuclear pulse propulsion which used pure deuterium fusion detonations with a very high fuel-burnup fraction. He computed an exhaust velocity of 15,000 km/s and a 100,000 tonne space-vehicle able to achieve a 20,000 km/s delta-vee allowing a flight-time to Alpha Centauri of 130 years. Later studies indicate that the top cruise velocity that can theoretically be achieved by a Teller-Ulam thermonuclear unit powered Orion starship, assuming no fuel is saved for slowing back down, is about 8% to 10% of the speed of light (0.08-0.1c). An atomic (fission) Orion can achieve perhaps 3%-5% of the speed of light. A nuclear pulse drive starship powered by Fusion-antimatter catalyzed nuclear pulse propulsion units would be similarly in the 10% range and pure Matter-antimatter annihilation rockets would be theoretically capable of obtaining a velocity between 50% to 80% of the speed of light. In each case saving fuel for slowing down halves the max. speed. The concept of using a magnetic sail to decelerate the spacecraft as it approaches its destination has been discussed as an alternative to using propellant, this would allow the ship to travel near the maximum theoretical velocity. Alternative designs utilizing similar principles include Project Longshot, Project Daedalus, and Mini-Mag Orion. The principle of external nuclear pulse propulsion to maximize survivable power has remained common among serious concepts for interstellar flight without external power beaming and for very high-performance interplanetary flight. In the 1970s the Nuclear Pulse Propulsion concept further was refined by Project Daedalus by use of externally triggered inertial confinement fusion, in this case producing fusion explosions via compressing fusion fuel pellets with high-powered electron beams. Since then lasers, ion beams, neutral particle beams and hyper-kinetic projectiles have been suggested to produce nuclear pulses for propulsion purposes. A current impediment to the development of "any" nuclear explosive powered spacecraft is the 1963 Partial Test Ban Treaty which includes a prohibition on the detonation of any nuclear devices (even non-weapon based) in outer space. This treaty would therefore need to be re-negotiated, although a project on the scale of an interstellar mission using currently foreseeable technology would probably require international co-operation on at least the scale of the International Space Station. Nuclear fusion rockets. Fusion rocket starships, powered by nuclear fusion reactions, should conceivably be able to reach speeds of the order of 10% of that of light, based on energy considerations alone. In theory, a large number of stages could push a vehicle arbitrarily close to the speed of light. These would "burn" such light element fuels as deuterium, tritium, 3He, 11B, and 7Li. Because fusion yields about 0.3–0.9% of the mass of the nuclear fuel as released energy, it is energetically more favorable than fission, which releases <0.1% of the fuel's mass-energy. The maximum exhaust velocities potentially energetically available are correspondingly higher than for fission, typically 4–10% of c. However, the most easily achievable fusion reactions release a large fraction of their energy as high-energy neutrons, which are a significant source of energy loss. Thus, while these concepts seem to offer the best (nearest-term) prospects for travel to the nearest stars within a (long) human lifetime, they still involve massive technological and engineering difficulties, which may turn out to be intractable for decades or centuries. Early studies include Project Daedalus, performed by the British Interplanetary Society in 1973–1978, and Project Longshot, a student project sponsored by NASA and the US Naval Academy, completed in 1988. Another fairly detailed vehicle system, "Discovery II", designed and optimized for crewed Solar System exploration, based on the D3He reaction but using hydrogen as reaction mass, has been described by a team from NASA's Glenn Research Center. It achieves characteristic velocities of >300 km/s with an acceleration of ~1.7•10−3 "g", with a ship initial mass of ~1700 metric tons, and payload fraction above 10%. While these are still far short of the requirements for interstellar travel on human timescales, the study seems to represent a reasonable benchmark towards what may be approachable within several decades, which is not impossibly beyond the current state-of-the-art. Based on the concept's 2.2% burnup fraction it could achieve a pure fusion product exhaust velocity of ~3,000 km/s. Antimatter rockets. An antimatter rocket would have a far higher energy density and specific impulse than any other proposed class of rocket. If energy resources and efficient production methods are found to make antimatter in the quantities required and store it safely, it would be theoretically possible to reach speeds approaching that of light. Then relativistic time dilation would become more noticeable, thus making time pass at a slower rate for the travelers as perceived by an outside observer, reducing the trip time experienced by human travelers. Supposing the production and storage of antimatter should become practical, two further problems would present and need to be solved. First, in the annihilation of antimatter, much of the energy is lost in very penetrating high-energy gamma radiation, and especially also in neutrinos, so that substantially less than "mc"2 would actually be available if the antimatter were simply allowed to annihilate into radiations thermally. Even so, the energy available for propulsion would probably be substantially higher than the ~1% of "mc"2 yield of nuclear fusion, the next-best rival candidate. Second, once again heat transfer from exhaust to vehicle seems likely to deposit enormous wasted energy into the ship, considering the large fraction of the energy that goes into penetrating gamma rays. Even assuming biological shielding were provided to protect the passengers, some of the energy would inevitably heat the vehicle, and may thereby prove limiting. This requires consideration for serious proposals if useful accelerations are to be achieved, as the energies involved (e.g., for 0.1"g" ship acceleration, approaching 0.3 trillion watts per ton of ship mass) are very large. Recently Friedwardt Winterberg has suggested a means of converting an imploding matter-antimatter plasma into a highly collimated beam of gamma-rays - effectively a gamma-ray laser - which would very efficiently transfer thrust to the space vehicle's structure via a variant of the Mössbauer effect. Such a system, if antimatter production can be made efficient, would then be a very effective photon rocket, as originally envisaged by Eugen Sanger. Non-rocket concepts. A problem with all traditional rocket propulsion methods is that the spacecraft would need to carry its fuel with it, thus making it very massive, in accordance with the rocket equation. Some concepts attempt to escape from this problem: Interstellar ramjets. In 1960, Robert W. Bussard proposed the Bussard ramjet, a fusion rocket in which a huge scoop would collect the diffuse hydrogen in interstellar space, "burn" it on the fly using a proton–proton fusion reaction, and expel it out of the back. Though later calculations with more accurate estimates suggest that the thrust generated would be less than the drag caused by any conceivable scoop design, the idea is attractive because, as the fuel would be collected "en route" (commensurate with the concept of "energy harvesting"), the craft could theoretically accelerate to near the speed of light. Beamed propulsion. A light sail or magnetic sail powered by a massive laser or particle accelerator in the home star system could potentially reach even greater speeds than rocket- or pulse propulsion methods, because it would not need to carry its own reaction mass and therefore would only need to accelerate the craft's payload. Robert L. Forward proposed a means for decelerating an interstellar light sail in the destination star system without requiring a laser array to be present in that system. In this scheme, a smaller secondary sail is deployed to the rear of the spacecraft, while the large primary sail is detached from the craft to keep moving forward on its own. Light is reflected from the large primary sail to the secondary sail, which is used to decelerate the secondary sail and the spacecraft payload. A magnetic sail could also decelerate at its destination without depending on carried fuel or a driving beam in the destination system, by interacting with the plasma found in the solar wind of the destination star and the interstellar medium. Unlike Forward's light sail scheme, this would not require the action of the particle beam used for launching the craft. Alternatively, a magnetic sail could be pushed by a particle beam or a plasma beam to reach high velocity, as proposed by Landis and Winglee. Beamed propulsion seems to be the best interstellar travel technique presently available, since it uses known physics and known technology that is being developed for other purposes, and would be considerably cheaper than nuclear pulse propulsion. The following table lists some example concepts using beamed laser propulsion as proposed by the physicist Robert L. Forward: Pre-accelerated fuel. Achieving start-stop interstellar trip times of less than a human lifetime require mass-ratios of between 1,000 and 1,000,000, even for the nearer stars. This could be achieved by multi-staged vehicles on a vast scale. Alternatively large linear accelerators could propel fuel to fission propelled space-vehicles, avoiding the limitations of the Rocket equation. Speculative methods. Hawking radiation rockets. In a black hole starship, a parabolic reflector would reflect Hawking radiation from an artificial black hole. In 2009, Louis Crane and Shawn Westmoreland of Kansas State University published a paper investigating the feasibility of this idea. Their conclusion was that it was on the edge of possibility, but that quantum gravity effects that are presently unknown may make it easier or make it impossible. Magnetic monopole rockets. If some of the Grand unification models are correct, e.g. 't Hooft–Polyakov, it would be possible to construct a photonic engine that uses no antimatter thanks to the magnetic monopole which hypothetically can catalyze decay of a proton to a positron and π0-meson: π0 decays rapidly to two photons, and the positron annihilates with an electron to give two more photons. As a result, a hydrogen atom turns into four photons and only the problem of a mirror remains unresolved. A magnetic monopole engine could also work on a once-through scheme such as the Bussard ramjet (see below). At the same time, most of the modern Grand unification theories such as M-theory predict no magnetic monopoles, which casts doubt on this attractive idea. By transmission. If physical entities could be transmitted as information and reconstructed at a destination, travel at nearly the speed of light would be possible, which for the "travelers" would be instantaneous. However, sending an atom-by-atom description of (say) a human body would be a daunting task. Extracting and sending only a computer brain simulation is a significant part of that problem. "Journey" time would be the light-travel time plus the time needed to encode, send and reconstruct the whole transmission. Faster-than-light travel. Scientists and authors have postulated a number of ways by which it might be possible to surpass the speed of light. Even the most serious-minded of these are speculative. According to Einstein's equation of general relativity, spacetime is curved: General relativity may permit the travel of an object faster than light in curved spacetime. One could imagine exploiting the curvature to take a "shortcut" from one point to another. This is one form of the warp drive concept. In physics, the Alcubierre drive is based on an argument that the curvature could take the form of a wave in which a spaceship might be carried in a "bubble". Space would be collapsing at one end of the bubble and expanding at the other end. The motion of the wave would carry a spaceship from one space point to another in less time than light would take through unwarped space. Nevertheless, the spaceship would not be moving faster than light within the bubble. This concept would require the spaceship to incorporate a region of exotic matter, or "negative mass". Wormholes are conjectural distortions in spacetime that theorists postulate could connect two arbitrary points in the universe, across an Einstein–Rosen Bridge. It is not known whether wormholes are possible in practice. Although there are solutions to the Einstein equation of general relativity which allow for wormholes, all of the currently known solutions involve some assumption, for example the existence of negative mass, which may be unphysical. However, Cramer "et al." argue that such wormholes might have been created in the early universe, stabilized by cosmic string. The general theory of wormholes is discussed by Visser in the book "Lorentzian Wormholes". Designs and studies. Project Hyperion. Project Hyperion, one of the projects of Icarus Interstellar. Enzmann starship. The Enzmann starship, as detailed by G. Harry Stine in the October 1973 issue of "Analog", was a design for a future starship, based on the ideas of Dr. Robert Duncan-Enzmann. The spacecraft itself as proposed used a 12,000,000 ton ball of frozen deuterium to power 12–24 thermonuclear pulse propulsion units. Twice as long as the Empire State Building and assembled in-orbit, the spacecraft was part of a larger project preceded by interstellar probes and telescopic observation of target star systems. NASA research. NASA has been researching interstellar travel since its formation, translating important foreign language papers and conducting early studies on applying fusion propulsion, in the 1960s, and laser propulsion, in the 1970s, to interstellar travel. The NASA Breakthrough Propulsion Physics Program (terminated in FY 2003 after 6-year, $1.2 million study, as "No breakthroughs appear imminent.") identified some breakthroughs which are needed for interstellar travel to be possible. Geoffrey A. Landis of NASA's Glenn Research Center states that a laser-powered interstellar sail ship could possibly be launched within 50 years, using new methods of space travel. "I think that ultimately we're going to do it, it's just a question of when and who," Landis said in an interview. Rockets are too slow to send humans on interstellar missions. Instead, he envisions interstellar craft with extensive sails, propelled by laser light to about one-tenth the speed of light. It would take such a ship about 43 years to reach Alpha Centauri, if it passed through the system. Slowing down to stop at Alpha Centauri could increase the trip to 100 years, while a journey without slowing down raises the issue of making sufficiently accurate and useful observations and measurements during a fly-by. Hundred-Year Starship study. The 100 Year Starship (100YSS) is the name of the overall effort that will, over the next century, work toward achieving interstellar travel. The effort will also go by the moniker 100YSS. The 100 Year Starship study is the name of a one year project to assess the attributes of and lay the groundwork for an organization that can carry forward the 100 Year Starship vision. Dr. Harold ("Sonny") White from NASA's Johnson Space Center is a member of Icarus Interstellar, the nonprofit foundation whose mission is to realize interstellar flight before the year 2100. At the 2012 meeting of 100YSS, he reported using a laser to try to warp spacetime by 1 part in 10 million with the aim of helping to make interstellar travel possible [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@61f25b53 [docID]14843 [hitPos]15 [correct]false [extraScores][F@1b5fbab7 , [answer]John W. Moffat (born 1932) is a Professor Emeritus in physics at the University of Toronto. He is also an adjunct Professor in physics at the University of Waterloo and a resident affiliate member of the Perimeter Institute for Theoretical Physics. Moffat is best known for his work on gravity and cosmology, culminating in his nonsymmetric gravitational theory and scalar–tensor–vector gravity (now called MOG), and summarized in his 2008 book for general readers, "Reinventing Gravity". His theory explains galactic rotation curves without invoking dark matter. He proposes a variable speed of light approach to cosmological problems, which posits that "G"/"c" is constant through time, but "G" and "c" separately have not been. Moreover, the speed of light "c" may have been much higher during early moments of the Big Bang. His recent work on inhomogeneous cosmological models purports to explain certain anomalous effects in the CMB data, and to account for the recently discovered acceleration of the expansion of the universe. Moffat has proposed a new nonlocal variant of quantum field theory, that is finite at all orders and hence dispenses with renormalization. It also generates mass without a Higgs mechanism. An unusual start to a physics career. Moffat began life as a struggling artist, but gave up after living for a time in Paris with no income. Upon returning to Copenhagen, Denmark, he became interested in the cosmos and began teaching himself mathematics and physics. He made such quick progress that within a year he began working on problems of general relativity and unified field theory. "When I was about 20, I wrote a letter to Albert Einstein telling him that I was working on one of his theories. In 1953 Einstein sent me a reply, from Princeton, New Jersey, but it was written in German. So I ran down to my barber shop (in Copenhagen) to have my barber translate it for me. Through that summer and fall, we exchanged about a half dozen letters. The local press picked up on these stories which then caught the attention of physicist Niels Bohr and others. Suddenly doors of opportunity were swinging open for me". (Perimeter Institute for Theoretical Physics, 2005) In 1958, he was awarded a Ph.D. without a first degree at Trinity College, Cambridge. (He was supervised by Fred Hoyle and Abdus Salam.) "Dear Professor . . . I would be eternally indebted if you could find time to read my work," he began. <br><br> “Most honorable Mr. Moffat: Our situation is the following. We are standing in front of a closed box which we cannot open, and we try hard to discuss what is inside and what is not,” Einstein replied. During a career that spans over five decades, Moffat worked on a variety of subjects in Theoretical Physics. These include particle physics, quantum field theory, quantum gravity and cosmology. Variable Speed of Light: Theory and Controversy. In 1992, John Moffat proposed that the speed of light was much larger in the early universe, in which the speed of light was 1030 times faster than its current value. He published his "variable speed of light" (VSL) theory in two places—on the Los Alamos National Laboratory's (LANL) online archive, Nov. 16, 1992, and in a 1993 edition of "International Journal of Modern Physics D". The scientific community mostly ignored VSL theory until in 2001, University of New South Wales astronomer John Webb and peers detected experimental evidence from telescopic observations that the cosmological fine structure constant -- which contains the speed of light—may have been different than its present value in the very early Universe. The observations supported Moffat's VSL theory—and started a race for primacy that began in 1998. That year, five years after Moffat had published his VSL papers, João Magueijo of Imperial College in London, and collaborators Andrew Albrecht of the University of California at Davis and John D. Barrow of Cambridge University, published a strikingly similar idea in the more prestigious journal, "Physical Review D", which had rejected Moffat's paper years earlier. Moffat considered legal action to prevent Magueijo, "et al." from publishing the theory without himself being credited. Informed of the omission, Magueijo credited Moffat with an entire chapter in Magueijo's 2002 book, "Faster Than the Speed of Light: The story of a scientific speculation." The controversy reignited, however, when during a worldwide publicity tour for Magueijo's book, the author neither credited Moffat nor corrected numerous erroneous press accounts—in such magazines as "Discover" "Publisher's Weekly" "Seed Magazine" and the "Christian Science Monitor". In efforts to portray Magueijo as a "brash, young scientific upstart," dozens of publications attributed VSL theory entirely to Magueijo and his co-authors, leaving Moffat—in his late sixties by this time—out. Moffat expressed displeasure about the re-emergent omissions, urging reporters to check their facts, but to no avail. Stories emerged about the book tour media omissions in March and July 2003, written by a science journalist, Michael Martin, who had earlier attributed VSL theory to Moffat in a 2001 UPI article about Webb's astronomical discoveries. "Discover Magazine" writer Tim Folger acknowledged the omissions in his story and apologized. In response to a reader letter from Henry van Driel of the University of Toronto Department of Physics, Folger wrote, "Professor van Driel is absolutely right—John Moffat did develop a varying speed of light theory several years before João Magueijo, and I regret not including that information in my story." Months later, as other reports picked up on the reignited dispute, Magueijo reiterated Moffat's primacy in VSL theory. In September 2004, Discover Magazine's Tim Folger followed through on a promise he had made during the controversy to "write a story about John Moffat.". The two physicists eventually settled their differences and became friends, publishing a joint paper in 2007 in the journal "General Relativity and Gravitation". French astrophysicist Jean-Pierre Petit, a senior researcher at the National Center for Scientific Research, published an earlier 1988 theory involving variable speed of light in the journal "Modern Physics Letters A". Modified Gravity Theory. Continuing Einstein's search for a unified field theory, Moffat proposed a Nonsymmetric Gravitational Theory that, like Einstein's unified field, incorporated a symmetric field (gravity) and an antisymmetric field. Unlike Einstein, however, Moffat made no attempt to identify the latter with electromagnetism, instead proposing that the antisymmetric component is another manifestation of gravity. As investigation progressed, the theory evolved in a variety of ways; most notably, Moffat postulated that the antisymmetric field may be massive. The current version of his modified gravity (MOG) theory, which grew out of this investigation, modifies Einstein's gravity with the addition of a vector field, while also promoting the constants of the theory to scalar fields. The combined effect of these fields modifies the strength of gravity at large distances when large masses are involved, successfully accounting for a range of astronomical and cosmological observations. The resulting theory describes well, without invoking dark matter, the rotation curves of galaxies and the mass profiles of X-ray galaxy clusters. Non-local Quantum Field Theory. In 1990, Moffat proposed a finite, non-local quantum field theory. The theory was developed extensively by Evens, Moffat, Kleppe and Woodard in 1991. In subsequent work, Moffat proposed this theory as an alternative to the standard electroweak unification of electromagnetism and the weak nuclear interactions. Moffat's theory is a quantum field theory with a non-local term in the field Lagrangian. Despite the non-local term the theory does not violate causality. The theory is finite to all orders, requiring no renormalization, and it provides a mechanism to give mass to elementary particles without having to postulate the Higgs boson. External links. University of Toronto press releases re Moffat [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@12925ecb [docID]1863306 [hitPos]15 [correct]false [extraScores][F@25e7a367 , [answer]Electron scattering occurs when electrons are deviated from their original trajectory. This is due to the electrostatic forces within matter interaction or, if an external magnetic field is present, the electron may be deflected by the Lorentz force. This scattering typically happens with solids such as metals, semiconductors and insulators; and is a limiting factor in integrated circuits and transistors. The application of electron scattering is such that it can be used as a high resolution microscope for hadronic systems, that allows the measurement of the distribution of charges for nucleons and nuclear structure. The scattering of electrons has allowed us to understand that protons and neutrons are made up of the smaller elementary subatomic particles called quarks. Electrons may be scattered through a solid in several ways: The likelihood of an electron scattering and the proliferance of the scattering is a probability function of the specimen thickness to the mean free path. History. The principle of the electron was first theorised in the period of 1838-1851 by a natural philosopher by the name of Richard Laming who speculated the existence of sub-atomic, unit charged particles; he also pictured the atom as being an 'electrosphere' of concentric shells of electrical particles surrounding a material core. <br> It is generally accepted that J J Thompson first discovered the electron in 1897, although other notable members in the development in charged particle theory are George Johnstone Stoney (who coined the term "electron"), Emil Wiechert (who was first to publish his independent discovery of the electron), Walter Kaufmann, Pieter Zeeman and Hendrik Lorentz. <br> Compton scattering was first observed at Washington University in 1923 by Arthur Holly Compton who earned the 1927 Nobel Prize in Physics for the discovery; his graduate student Y. H. Woo who further verified the results is also of mention. Compton scattering is usually cited in reference to the interaction involving the electrons of an atom, however nuclear Compton scattering does exist. <br> The first electron diffraction experiment was conducted in 1927 by Clinton Davisson and Lester Germer using what would come to be a prototype for modern LEED system. The experiment was able to demonstrate the wave-like properties of electrons, thus confirming the de Broglie hypothesis that matter particles have a wave-like nature. However, after this the interest in LEED diminished in favour of High-energy electron diffraction until the early 1960s when an interest in LEED was revived; of notable mention during this period is H. E. Farnsworth who continued to develop LEED techniques. <br> High energy electron-electron colliding beam history begins in 1956 when K. O'Neill of Princeton University became interested in high energy collisions, and introduced the idea of accelerator(s) injecting into storage ring(s). While the idea of beam-beam collisions had been around since approximately the 1920s, it was not until 1953 that a German patent for colliding beam apparatus was obtained by Rolf Wideroe. Phenomena. Electrons can be scattered by other charged particles through the electrostatic Coulomb forces. Furthermore, if a magnetic field is present, a traveling electron will be deflected by the Lorentz force. An extremely accurate description of all electron scattering, including quantum and relativistic aspects, is given by the theory of quantum electrodynamics. Lorentz force. The Lorentz force, named after Dutch physicist Hendrik Lorentz, for a charged particle "q" is given (in SI units) by the equation: where "qE" describes the electric force due to a present electric field,E, acting on "q". <br>Which can also be written as: where "ϕ" is the electric potential, and A is the magnetic vector potential. It was Oliver Heaviside who is attributed in 1885 and 1889 to first deriving the correct expression for the Lorentz force of "qv x B". Hendrik Lorentz derived and refined the concept in 1892 and gave it his name, incorporating forces due to electric fields. or in the relativistic case using Lorentz contraction where "γ" is: this equation of motion was first verified in 1897 in J.J. Thomson's experiment investigating cathode rays which confirmed, through bending of the rays in a magnetic field, that these rays were a stream of charged particles now known as electrons. Variations on this basic formula describe the magnetic force on a current-carrying wire (sometimes called Laplace force), the electromotive force in a wire loop moving through a magnetic field (an aspect of Faraday's law of induction), and the force on a particle which might be traveling near the speed of light (relativistic form of the Lorentz force). Electrostatic Coulomb force. [[Image:Coulombslaw.svg|thumb|right|upright=1.5|A graphical representation of Coulomb's law.|In the image, the vector F1 is the force experienced by "q1", and the vector F2 is the force experienced by "q2". When "q1q2 > 0" the forces are repulsive (as in the image) and when "q1q2 < 0" the forces are attractive (opposite to the image). The magnitude of the forces will always be equal. In this case: formula_6 <br>(a unit vector pointing from "q2" to "q1"). <br>The vector form of the equation above calculates the force F1 applied on "q1" by "q2". If r12 is used instead, then the effect on "q2" can be found. It can be also calculated using Newton's third law: F2 = -F1.]] Electrostatic Coulomb force also known as Coulomb interaction and electrostatic force, named for Charles-Augustin de Coulomb who published the result in 1785, describes the attraction or repulsion of particles due to their electric charge. Coulomb's law states that: The magnitude of the electrostatic force is proportional to the scalar multiple of the charge magnitudes, and inversely proportional to the square of the distance (i.e. Inverse square law), and is given by: or in vector notation: where "εr" is the relative permittivity or dielectric constant of the space the force acts through, and is dimensionless. Collisions. If two particles interact with one another in a collision process there are four results possible after the interaction: Elastic. Elastic scattering is when the collisions between target and incident particles have total conservation of kinetic energy. This implies that there is no breaking up of the particles or energy loss through vibrations, that is to say that the internal states of each of the particles remains unchanged. Due to the fact that there is no breaking present, elastic collisions can be modeled as occurring between point-like particles, a principle that is very useful for an elementary particle such as the electron. Inelastic. Inelastic scattering is when the collisions do "not" conserve kinetic energy, and as such the internal states of one or both of the particles has changed. This is due to energy being converted into vibrations which can be interpreted as heat, waves (sound), or vibrations between constituent particles of either collision party. Particles "may" also split apart, further energy can be converted into breaking the chemical bonds between components. Furthermore, momentum is conserved in both elastic and inelastic scattering. The other two results are reactions (when the structure of the interacting particles is changed producing two or more (generally complex particles)), and that new particles that are not constituent elementary particles of the interacting particles are created. Types of scattering. Compton scattering. Compton scattering, so named for Arthur Holly Compton who first observed the effect in 1922 and which earned him the 1927 Nobel Prize in Physics; is the inelastic scattering of a high-energy photon by a free charged particle. This was demonstrated in 1923 by firing radiation of a given wavelength (X-rays in the given case) sent through a foil (carbon target) was scattered in a manner inconsistent with classical radiation theory, published a paper in the "Physical Review" explaining the phenomenon: "A quantum theory of the scattering of X-rays by light elements". The Compton effect can be understood as high-energy photons scattering in-elastically off individual electrons, when the incoming photon gives part of its energy to the electron, then the scattered photon has lower energy and lower frequency and longer wavelength according to the Planck relation: which gives the energy "E" of the photon in terms of frequency "f" or "ν", and Planck's constant "h" ( = ). The wavelength change in such scattering depends only upon the angle of scattering for a given target particle. This was an important discovery during the 1920s when the particle (photon) nature of light suggested by the Photoelectric effect was still being debated, the Compton experiment gave clear and independent evidence of particle-like behavior. The formula describing the Compton shift in the wavelength due to scattering is given by: where "λf" is the final wavelength of the photon after scattering, "λi" is the initial wavelength of the photon before scattering, "h" is Plank's constant, "me" is the rest mass of the electron, "c" is the speed of light and "θ" is the scattering angle of the photon. The coefficient of "(1 - cosθ)" is known as the "Compton wavelength", but is in fact a proportionality constant for the wavelength shift. The collision causes the photon wavelength to increase by somewhere between 0 (for a scattering angle of 0°) and twice the Compton wavelength (for a scattering angle of 180°). Thomson scattering is the classical elastic quantitative interpretation of the scattering process, and this can be seen to happen with lower, mid-energy, photons. The classical theory of an electromagnetic wave scattered by charged particles, cannot explain low intensity shifts in wavelength. Inverse Compton scattering takes place when the electron is moving, and has sufficient kinetic energy compared to the photon. In this case net energy may be transferred from the electron to the photon. The inverse Compton effect is seen in astrophysics when a low energy photon (e.g. of the cosmic microwave background) bounces off a high energy (relativistic) electron. Such electrons are produced in supernovae and active galactic nuclei. Synchrotron emission. If a charged particle such as an electron is accelerated, this can be acceleration in a straight line or motion in a curved path, electromagnetic radiation is emitted by the particle. Within electron storage rings and circular particle accelerators known as synchrotrons, electrons are bent in a circular path and emit X-rays typically. This radially emitted (formula_11) electromagnetic radiation when charged particles are accelerated is called synchrotron radiation. It is produced in synchrotrons using bending magnets, undulators and/or wigglers. The first observation came at the General Electric Research Laboratory in Schenectady, New York, on April 24, 1947 in the synchrotron built by a team of Herb Pollack to test the idea of phase-stability principle for RF accelerators. When the technician was asked to look around the shielding with a large mirror to check for sparking in the tube, he saw a bright arc of light coming from the electron beam. Robert Langmuir is credited as recognizing it as synchrotron radiation or, as he called it, "Schwinger radiation" after Julian Schwinger. Classically, the radiated power "P" from an accelerated electron is: this comes from the Larmor formula; where "K" is an electric permittivity constant, "e" is electron charge, "c" is the speed of light, and "a" is the acceleration. Within a circular orbit such as a storage ring, the non-relativistic case is simply the centripetal acceleration. However within a storage ring the acceleration is highly relitivistic, and can be obtained as follows: where "v" is the circular velocity, "r" is the radius of the circular accelerator, "m" is the rest mass of the charged particle, "p" is the momentum, "τ" is the Proper time (t/γ), and "γ" is the Lorentz factor. Radiated power then becomes: For highly relativistic particles, such that velocity becomes nearly constant, the γ4 term becomes the dominate variable in determining loss rate. This means that the loss scales as the fourth power of the particle energy γmc2; and the inverse dependence of synchrotron radiation loss on radius argues for building the accelerator as large as possible. Facilities. SLAC. Stanford Linear Accelerator Center is located near Stanford university, California. Construction began on the 2 mile long linear accelerator in 1962 and was completed in 1967, and in 1968 the first experimental evidence of quarks was discovered resulting in the 1990 Nobel Prize in Physics, shared by SLAC's Richard Taylor and Jerome I. Friedman and Henry Kendall of MIT. The accelerator came with a 20GeV capacity for the electron acceleration, and while similar to Rutherford's scattering experiment, that experiment operated with alpha particles at only 7MeV. In the SLAC case the incident particle was an electron and the target a proton, and due to the short wavelength of the electron (due to its high energy and momentum) it was able to probe into the proton. The Stanford Positron Electron Asymmetric Ring (SPEAR) addition to the SLAC made further such discoveries possible, leading to the discovery in 1974 of the J/psi particle, which consists of a paired charm quark and anti-charm quark, and another Nobel Prize in Physics in 1976. This was followed up with Martin Perl's announcement of the discovery of the tau lepton, for which he shared the 1995 Nobel Prize in Physics. The SLAC aims to be a premier accelerator laboratory, to pursue strategic programs in particle physics, particle astrophysics and cosmology, as well as the applications in discovering new drugs for healing, new materials for electronics and new ways to produce clean energy and clean up the environment. Under the directorship of Chi-­Chang Kao the SLAC's fifth director (as of November 2012), a noted X-ray scientist who came to SLAC in 2010 to serve as associate laboratory director for the Stanford Synchrotron Radiation Lightsource. SSRL - Stanford Synchrotron Radiation Lightsource. Other scientific programs run at SLAC include: RIKEN RI Beam Factory. RIKEN was founded in 1917 as a private research foundation in Tokyo, and is Japan's largest comprehensive research institution. Having grown rapidly in size and scope, it is today renowned for high-quality research in a diverse range of scientific disciplines, and encompasses a network of world-class research centers and institutes across Japan. The RIKEN RI Beam Factory, otherwise known as the RIKEN Nishina Centre (for Accelerator-Based Science), is a cyclotron-based research facility which began operating in 2007; 70 years after the first in Japanese cyclotron, from Dr. Yoshio Nishina whose name is given to the facility. As of 2006, the facility has a world-class heavy-ion accelerator complex. This consists of a K540-MeV ring cyclotron (RRC) and two different injectors: a variable-frequency heavy-ion linac (RILAC) and a K70-MeV AVF cyclotron (AVF). It has a projectile-fragment separator (RIPS) which provides RI (Radioactive Isotope) beams of less than 60 amu, the world's most intense light-atomic-mass RI beams. Overseen by the Nishina Centre, the RI Beam Factory is utilized by users worldwide promoting research in nuclear, particle and hadron physics. This promotion of accelerator applications research is an important mission of the Nishina Centre, and implements the use of both domestic and oversea accelerator facilities. SCRIT. The SCRIT (Self-Confining Radioactive isotope Ion Target) facility, is currently under construction at the RIKEN RI beam factory (RIBF) in Japan. The project aims to investigate short-lived nuclei through the use of an elastic electron scattering test of charge density distribution, with initial testing done with stable nuclei. With the first electron scattering off unstable Sn isotopes to take place in 2014. The investigation of short-lived radioactive nuclei (RI) by means of electron scattering has never been performed because of an inability to make these nuclei a target, now the with the advent of a novel self-confining RI technique at the world’s ﬁrst facility dedicated to the study of the structure of short-lived nuclei by electron scattering this research becomes possible. The principle of the technique is based around the ion trapping phenomenon which is observed at electron storage ring facilities, which has an adverse effect on the performance of electron storage rings. The novel idea to be employed at SCRIT is to "use" the ion trapping to allow short-lived RI's to be made a target, as trapped ions on the electron beam, for the scattering experiments. This idea was first given a proof-of-principle study using the electron storage ring of Kyoto University, KSR; this was done using a stable nucleus of 133Cs as a target in an experiment of 120MeV electron beam energy, 75mA typical stored beam current and a 100 seconds beam lifetime. The results of this study were favorable with elastically scattered electrons from the trapped Cs being clearly visible [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@15ab6789 [docID]3201543 [hitPos]15 [correct]false [extraScores][F@7d916b55 , [answer]The characteristic rotational temperature (θR or θrot) is commonly used in statistical thermodynamics, to simplify the expression of the rotational partition function and the rotational contribution to molecular thermodynamic properties. It has units of temperature and is defined as formula_1, where B is the rotational constant, and formula_2 is a molecular moment of inertia. Also h is the Planck constant, c is the speed of light, ħ = h/2π is the reduced Planck constant and "kB" is the Boltzmann constant. The physical meaning of θR is as an estimate of the temperature at which thermal energy (of the order of kBT) is comparable to the spacing between rotational energy levels (of the order of hcB). At about this temperature the population of excited rotational levels becomes important. Some typical values are 88 K for H2, 15.2 K for HCl and 0.561 K for CO2 [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@15ab6789 [docID]4252560 [hitPos]16 [correct]false [extraScores][F@26a3fcd5 , [answer]In physics, the world line of an object is the unique path of that object as it travels through 4-dimensional spacetime. The concept of "world line" is distinguished from the concept of "orbit" or "trajectory" (such as an "orbit in space" or a "trajectory" of a truck on a road map) by the "time" dimension, and typically encompasses a large area of spacetime wherein perceptually straight paths are recalculated to show their (relatively) more absolute position states — to reveal the nature of special relativity or gravitational interactions. The idea of world lines originates in physics and was pioneered by Hermann Minkowski. The term is now most often used in relativity theories (i.e., special relativity and general relativity). However, world lines are a general way of representing the course of events. The use of it is not bound to any specific theory. Thus in general usage, a world line is the sequential path of personal human events (with "time" and "place" as dimensions) that marks the history of a person — perhaps starting at the time and place of one's birth until one's death. The log book of a ship is a description of the ship's world line, as long as it contains a time tag attached to every position. The world line allows one to calculate the speed of the ship, given a measure of distance (a so-called metric) appropriate for the curved surface of the Earth. Usage in physics. In physics, a world line of an object (approximated as a point in space, e.g., a particle or observer) is the sequence of spacetime events corresponding to the history of the object. A world line is a special type of curve in spacetime. Below an equivalent definition will be explained: A world line is a time-like curve in spacetime. Each point of a world line is an event that can be labeled with the time and the spatial position of the object at that time. For example, the "orbit" of the Earth in space is approximately a circle, a three-dimensional (closed) curve in space: the Earth returns every year to the same point in space. However, it arrives there at a different (later) time. The "world line" of the Earth is helical in spacetime (a curve in a four-dimensional space) and does not return to the same point. Spacetime is the collection of points called events, together with a continuous and smooth coordinate system identifying the events. Each event can be labeled by four numbers: a time coordinate and three space coordinates; thus spacetime is a four-dimensional space. The mathematical term for spacetime is a four-dimensional manifold. The concept may be applied as well to a higher-dimensional space. For easy visualizations of four dimensions, two space coordinates are often suppressed. The event is then represented by a point in a Minkowski diagram, which is a plane usually plotted with the time coordinate, say formula_1, upwards and the space coordinate, say formula_2 horizontally. As expressed by F.R. Harvey A world line traces out the path of a single point in spacetime. A world sheet is the analogous two-dimensional surface traced out by a one-dimensional line (like a string) traveling through spacetime. The world sheet of an open string (with loose ends) is a strip; that of a closed string (a loop) is a volume. Once the object is not approximated as a mere point but has extended volume, it traces out not a "world line" but rather a world tube. World lines as a tool to describe events. A one-dimensional "line" or "curve" can be represented by the coordinates as a function of one parameter. Each value of the parameter corresponds to a point in spacetime and varying the parameter traces out a line. So in mathematical terms a curve is defined by four coordinate functions formula_3 (where formula_4 usually denotes the time coordinate) depending on one parameter formula_5. A coordinate grid in spacetime is the set of curves one obtains if three out of four coordinate functions are set to a constant. Sometimes, the term world line is loosely used for "any" curve in spacetime. This terminology causes confusions. More properly, a world line is a curve in spacetime which traces out the "(time) history" of a particle, observer or small object. One usually takes the proper time of an object or an observer as the curve parameter formula_5 along the world line. Trivial examples of spacetime curves. A curve that consists of a horizontal line segment (a line at constant coordinate time), may represent a rod in spacetime and would not be a world line in the proper sense. The parameter traces the length of the rod. A line at constant space coordinate (a vertical line in the convention adopted above) may represent a particle at rest (or a stationary observer). A tilted line represents a particle with a constant coordinate speed (constant change in space coordinate with increasing time coordinate). The more the line is tilted from the vertical, the larger the speed. Two world lines that start out separately and then intersect, signify a "collision" or "encounter." Two world lines starting at the same event in spacetime, each following its own path afterwards, may represent the decay of a particle into two others or the emission of one particle by another. World lines of a particle and an observer may be interconnected with the world line of a photon (the path of light) and form a diagram which depicts the emission of a photon by a particle which is subsequently observed by the observer (or absorbed by another particle). Tangent vector to a world line, four-velocity. The four coordinate functions formula_3 defining a world line, are real functions of a real variable formula_5 and can simply be differentiated in the usual calculus. Without the existence of a metric (this is important to realize) one can speak of the difference between a point formula_9 on the curve at the parameter value formula_10 and a point on the curve a little (parameter formula_11) farther away. In the limit formula_12, this difference divided by formula_13 defines a vector, the tangent vector of the world line at the point formula_9. It is a four-dimensional vector, defined in the point formula_9. It is associated with the normal 3-dimensional velocity of the object (but it is not the same) and therefore called four-velocity formula_16, or in components: where the derivatives are taken at the point formula_9, so at formula_19. All curves through point p have a tangent vector, not only world lines. The sum of two vectors is again a tangent vector to some other curve and the same holds for multiplying by a scalar. Therefore all tangent vectors in a point p span a linear space, called the tangent space at point p. For example, taking a 2-dimensional space, like the (curved) surface of the Earth, its tangent space at a specific point would be the flat approximation of the curved space. World lines in special relativity. So far a world line (and the concept of tangent vectors) has been described without a means of quantifying the interval between events. The basic mathematics is as follows: The theory of special relativity puts some constraints on possible world lines. In special relativity the description of spacetime is limited to "special" coordinate systems that do not accelerate (and so do not rotate either), called inertial coordinate systems. In such coordinate systems, the speed of light is a constant. The structure of spacetime is determined by a bilinear form η which gives a real number for each pair of events. The bilinear form is sometimes called a "spacetime metric", but since distinct events sometimes result in a zero value, unlike metrics in metric spaces of mathematics, the bilinear form is "not" a mathematical metric on spacetime. World lines of particles/objects at constant speed are called geodesics. In special relativity these are straight lines in Minkowski space. Often the time units are chosen such that the speed of light is represented by lines at a fixed angle, usually at 45 degrees, forming a cone with the vertical (time) axis. In general, curves in spacetime can be of three types: At a given event on a world line, spacetime (Minkowski space) is divided into three parts. Simultaneous hyperplane. Since a world line formula_20 determines a velocity 4-vector formula_21 that is time-like, the Minkowski form formula_22 determines a linear function formula_23 by formula_24 Let "N" be the null space of this linear functional. Then "N" is called the simultaneous hyperplane with respect to "v". The relativity of simultaneity is a statement that "N" depends on "v". Indeed, "N" is the orthogonal complement of "v" with respect to η. When two world lines "u" and "w" are related by formula_25 then they share the same simultaneous hyperplane. This hyperplane exists mathematically, but physical relations in relativity involve the movement of information by light. For instance, the traditional electro-static force described by Coulomb's law may be pictured in a simultaneous hyperplane, but relativistic relations of charge and force involve retarded potentials. World lines in general relativity. The use of world lines in general relativity is basically the same as in special relativity, with the difference that spacetime can be curved. A metric exists and its dynamics are determined by the Einstein field equations and are dependent on the mass distribution in spacetime. Again the metric defines lightlike (null), spacelike and timelike curves. Also, in general relativity, world lines are timelike curves in spacetime, where timelike curves fall within the lightcone. However, a lightcone is not necessarily inclined at 45 degrees to the time axis. However, this is an artifact of the chosen coordinate system, and reflects the coordinate freedom (diffeomorphism invariance) of general relativity. Any timelike curve admits a comoving observer whose "time axis" corresponds to that curve, and, since no observer is privileged, we can always find a local coordinate system in which lightcones are inclined at 45 degrees to the time axis. See also for example Eddington-Finkelstein coordinates. World lines of free-falling particles or objects (such as planets around the Sun or an astronaut in space) are called geodesics. World lines in literature. A popular description of human world lines was given by J. C. Fields at the University of Toronto in the early days of relativity. As described by Toronto lawyer Norman Robertson: Because they oversimplify world lines, which traverse four-dimensional spacetime, into one-dimensional timelines, almost all purported science-fiction stories about time travel are actually wishful fantasy stories. Some device or superpowered person is generally portrayed as departing from one point in time, and with little or no subjective lag, arriving at some other point in time — but at the same literally geographic point in space, typically inside a workshop or near some historic site. However, in reality the planet, its solar system, and its galaxy would all be at vastly different spatial positions on arrival. Thus, the time travel mechanism would also have to provide instantaneous teleportation, with infinitely accurate and simultaneous adjustment of final 3D location, linear momentum, and angular momentum. World lines appeared in Jeffrey Rowland's webcomic "Wigu Adventures" as part of the "Magical Adventures in Space" side story line, in which Topato Potato and Sheriff Pony accidentally delete a world line relating to the initial creation of Earth from asteroids, causing the Earth to never have existed. According to this webcomic, calculating the exact coordinates of a world line is "embarrassingly simple", and the deletion of the world line specified is executed by making a call and entering the coordinates of the world line, and pressing 3. Author Oliver Franklin published a science fiction work in 2008 entitled "World Lines" in which he related a simplified explanation of the hypothesis for laymen. In the short story "Life-Line", author Robert A. Heinlein describes the world line of a person: Heinlein's "Methuselah's Children" uses the term, as does James Blish's "The Quincunx of Time" (expanded from "Beep"). A visual novel named Steins;Gate, produced by 5pb., tells a story based on shifting of world line. Its series of works under the name "hypothetical science ADV" also utilized the concept [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@58777339 [docID]165487 [hitPos]16 [correct]false [extraScores][F@24130bc5 , [answer]The Fizeau experiment was carried out by Hippolyte Fizeau in 1851 to measure the relative speeds of light in moving water. Fizeau used a special interferometer arrangement to measure the effect of movement of a medium upon the speed of light. According to the theories prevailing at the time, light traveling through a moving medium would be dragged along by the medium, so that the measured speed of the light would be a simple sum of its speed "through" the medium plus the speed "of" the medium. Fizeau indeed detected a dragging effect, but the magnitude of the effect that he observed was far lower than expected. His results seemingly supported the partial aether-drag hypothesis of Fresnel, a situation that was disconcerting to most physicists. Over half a century passed before a satisfactory explanation of Fizeau's unexpected measurement was developed with the advent of Albert Einstein's theory of special relativity. Einstein later pointed out the importance of the experiment for special relativity. Although it is referred to as "the" Fizeau experiment, Fizeau was an active experimenter who carried out a wide variety of different experiments involving measuring the speed of light in different situations. Experimental setup. A light ray emanating from the source "S"' is reflected by a beam splitter "G" and is collimated into a parallel beam by lens "L". After passing the slits "O"1 and "O"2, two rays of light travel through the tubes "A"1 and "A"2, through which water is streaming back and forth as shown by the arrows. The rays reflect off a mirror "m" at the focus of lens L', so that one ray always propagates in the same direction as the water stream, and the other ray opposite to the direction of the water stream. After passing back and forth through the tubes, both rays unite at S, where they produce interference fringes that can be visualized through the illustrated eyepiece. The interference pattern can be analyzed to determine the speed of light traveling along each leg of the tube. Fresnel drag coefficient. Assume that water flows in the pipes at velocity "v". According to the non-relativistic theory of the luminiferous aether, the speed of light should be increased when "dragged" along by the water, and decreased when "overcoming" the resistance of the water. The overall speed of a beam of light should be a simple additive sum of its speed "through" the water plus the speed "of" the water. That is, if "n" is the index of refraction of water, so that "c/n" is the velocity of light in stationary water, then the predicted speed of light "w" in one arm would be and the predicted speed in the other arm would be Hence light traveling against the flow of water should be slower than light traveling with the flow of water. The interference pattern between the two beams when the light is recombined at the observer depends upon the transit times over the two paths, and can be used to calculate the speed of light as a function of the speed of the water. Fizeau found that In other words, light appeared to be dragged by the water, but the magnitude of the dragging was much lower than expected. The Fizeau experiment forced physicists to accept the empirical validity of an old, theoretically unsatisfactory theory of Augustin-Jean Fresnel (1818) that had been invoked to explain an 1810 experiment by Arago, namely, that a medium moving through the stationary aether drags light propagating through it with only a fraction of the medium's speed, with a dragging coefficient "f" given by In 1895, Hendrik Lorentz predicted the existence of an extra term due to dispersion: Repetitions. Albert Michelson and Edward Morley (1886) repeated Fizeau's experiment with improved accuracy, addressing several concerns with Fizeau's original experiment: (1) Deformation of the optical components in Fizeau's apparatus could cause artifactual fringe displacement; (2) observations were rushed, since the pressurized flow of water lasted only a short time; (3) Fizeau's tubes were of small diameter resulting in observational difficulties; (4) there were uncertainties in Fizeau's determination of flow rate. Michelson redesigned Fizeau's apparatus with larger diameter tubes and a large reservoir providing three minutes of steady water flow. His common path interferometer design provided automatic compensation of path length, so that white light fringes were visible at once as soon as the optical elements were aligned. Topologically, the light path was that of a Sagnac interferometer with an even number of reflections in each light path. This offered extremely stable fringes that were, to first order, completely insensitive to any movement of its optical components. The stability was such that it was possible for him to insert a glass plate at h or even to hold a lighted match in the light path without displacing the center of the fringe system. Using this apparatus, Michelson and Morley were able to completely confirm Fizeau's results. Other experiments were conducted by Pieter Zeeman in 1914–1915. Using a scaled-up version of Michelson's apparatus connected directly to Amsterdam's main water conduit, Zeeman was able to perform extended measurements using monochromatic light ranging from violet (4358 Å) through red (6870 Å) to confirm Lorentz's modified coefficient. In 1910, Franz Harress used a "rotating" device and overall confirmed Fresnel's dragging coefficient. However, he additionally found a "systematic bias" in the data, which later turned out to be the Sagnac effect. Since then, many experiments have been conducted measuring such dragging coefficients, often in combination with the Sagnac effect. For instance, in experiments using ring lasers together with rotating disks, or in neutron interferometric experiments. Also a transverse dragging effect was observed, i.e. when the medium is moving at right angles to the direction of the incident light. Hoek experiment. An indirect confirmation of Fresnel's dragging coefficient was provided by Martin Hoek (1868). His apparatus was similar to Fizeau's, though in his version only one arm contained an area filled with resting water, while the other arm was in the air. As seen by an observer resting in the aether, Earth and hence the water is in motion. So the following travel times of two light rays traveling in opposite direction were calculated by Hoek (neglecting the transverse direction, see image): The travel times are not the same, which should be indicated by an interference shift. However, if Fresnel's dragging coefficient is applied to the water in the aether frame, the travel time difference (to first order in v/c) vanishes. Using different setups Hoek actually obtained a null result, confirming Fresnel's dragging coefficient. (For a similar experiment refuting the possibility of "shielding" the aether wind, see Hammar experiment). In the particular version of the experiment shown here, Hoek used a prism "P" to disperse light from a slit into a spectrum which passed through a collimator "C" before entering the apparatus. With the apparatus oriented parallel to the hypothetical aether wind, Hoek expected the light in one circuit to be retarded 7/600 mm with respect to the other. Where this retardation represented an integral number of wavelengths, he expected to see constructive interference; where this retardation represented a half-integral number of wavelengths, he expected to see destructive interference. In the absence of dragging, his expectation was for the observed spectrum to be continuous with the apparatus oriented transversely to the aether wind, and to be banded with the apparatus oriented parallel to the aether wind. His actual experimental results were completely negative. Controversy. Although Fresnel's hypothesis was empirically successful in explaining Fizeau's results, many leading experts in the field, including Fizeau (1851), Éleuthère Mascart (1872), Ketteler (1873), Veltmann (1873), and Lorentz (1886) were united in considering Fresnel's partial aether-dragging hypothesis to be on shaky theoretical grounds. For example, Veltmann (1870) demonstrated that Fresnel's formula implies that the aether would have to be dragged by different amounts for different colors of light, since the index of refraction depends on wavelength; Mascart (1872) demonstrated a similar result for polarized light traveling through a birefringent medium. In other words, the aether must be capable of sustaining different motions at the same time. Fizeau's dissatisfaction with the result of his own experiment is easily discerned in the conclusion to his report: The success of the experiment seems to me to render the adoption of Fresnel's hypothesis necessary, or at least the law which he found for the expression of the alteration of the velocity of light by the effect of motion of a body; for although that law being found true may be a very strong proof in favour of the hypothesis of which it is only a consequence, perhaps the conception of Fresnel may appear so extraordinary, and in some respects so difficult, to admit, that other proofs and a profound examination on the part of geometricians will still be necessary before adopting it as an expression of the real facts of the case. Despite the dissatisfaction of most physicists with Fresnel's partial aether-dragging hypothesis, repetitions and improvements to his experiment (see section above) by others confirmed his results to high accuracy. Besides the problems of the partial aether-dragging hypothesis, another major problem arose with the Michelson-Morley experiment (1887). In Fresnel's theory, the aether is almost stationary, so the experiment should have given a positive result. However, the result of this experiment was negative. Thus from the viewpoint of the aether models at that time, the experimental situation was contradictory: On one hand, the Aberration of light, the Fizeau experiment and the repetition by Michelson and Morley in 1886 appeared to prove the (almost) stationary aether with partial aether-dragging. On the other hand, the Michelson-Morley experiment of 1887 appeared to prove that the aether is at rest with respect to Earth, apparently supporting the idea of complete aether-dragging (see aether drag hypothesis). So the very success of Fresnel's hypothesis in explaining Fizeau's results helped lead to a theoretical crisis, which was not resolved until the development of the theory of special relativity. Lorentz's interpretation. In 1892, Hendrik Lorentz proposed a modification of Fresnel's model, in which the aether is completely stationary. He succeeded in deriving Fresnel's dragging coefficient by the reaction of the moving water upon the interfering waves, without the need of any aether entrainment. He also discovered that the transition from one to another reference frame could be simplified by using an auxiliary time variable which he called "local time": In 1895, Lorentz more generally explained Fresnel's coefficient based on the concept of local time. However, Lorentz's theory had the same fundamental problem as Fresnel's: a stationary aether contradicted the Michelson-Morley experiment. So in 1892 Lorentz proposed that moving bodies contract in the direction of motion (FitzGerald-Lorentz contraction hypothesis, since George FitzGerald had already arrived in 1889 at this conclusion). The equations that he used to describe these effects were further developed by him until 1904. These are now called the Lorentz transformations in his honor, and are identical in form to the equations that Einstein were later to derive from first principles. Unlike Einstein's equations, however, Lorentz's transformations were strictly "ad hoc", their only justification being that they seemed to work. Derivation in special relativity. Einstein showed how Lorentz's equations could be derived as the logical outcome of a set of two simple starting postulates. In addition Einstein recognized that the stationary aether concept has no place in special relativity, and that the Lorentz transformation concerns the nature of space and time. Together with the moving magnet and conductor problem, the negative aether drift experiments, and the aberration of light, the Fizeau experiment was one of the key experimental results that shaped Einstein's thinking about relativity. Robert S. Shankland reported some conversations with Einstein, in which Einstein emphasized the importance of the Fizeau experiment: Max von Laue (1907) demonstrated that the Fresnel drag coefficient can be easily explained as a natural consequence of the relativistic formula for addition of velocities, namely: Fizeau's experiment is hence supporting evidence for the collinear case of Einstein's velocity addition formula [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@61f25b53 [docID]11664784 [hitPos]17 [correct]false [extraScores][F@1f4daef5 , [answer]Rømer's determination of the speed of light was the demonstration in 1676 that light has a finite speed, and so doesn't travel instantaneously. The discovery is usually attributed to Danish astronomer Ole Rømer (1644–1710), who was working at the Royal Observatory in Paris at the time. Rømer estimated that light would take about 22 minutes to travel a distance equal to the diameter of Earth's orbit around the Sun: this is equivalent to about 220,000 kilometres per second in modern units, about 26% lower than the true value. While the exact details of Rømer's calculations have been lost, the error is probably due to an error in the orbital elements of Jupiter, leading Rømer to believe that Jupiter was closer to the Sun than is actually the case. Rømer's theory was controversial at the time he announced it, and he never convinced the director of the Royal Observatory, Giovanni Domenico Cassini, to fully accept it. However, it quickly gained support among other natural philosophers of the period, such as Christiaan Huygens and Isaac Newton. It was finally confirmed nearly two decades after Rømer's death, with the explanation in 1729 of stellar aberration by the English astronomer James Bradley. Background. The determination of longitude was a significant practical problem in cartography and navigation. Philip III of Spain had offered a prize for a method to determine the longitude of a ship out of sight of land, and Galileo proposed a method of establishing the time of day, and thus longitude, based on the times of the eclipses of the moons of Jupiter, in essence using the Jovian system as a cosmic clock; this method was not significantly improved until accurate mechanical clocks were developed in the eighteenth century. Galileo proposed this method to the Spanish crown (1616–17) but it proved to be impractical, not least because of the difficulty of observing the eclipses on a ship. However, with refinements the method could be made to work on land. The Italian astronomer Giovanni Domenico Cassini had pioneered the use of the eclipses of the Galilean moons for longitude measurements, and published tables predicting when eclipses would be visible from a given location. He was invited to France by Louis XIV to set up the Royal Observatory, which opened in 1671 with Cassini as director, a post he would hold for the rest of his life. One of Cassini's first projects in his new post in Paris was to send Frenchman Jean Picard to the site of Tycho Brahe's old observatory at Uraniborg, on the island of Hven near Copenhagen. Picard was to observe and time the eclipses of Jupiter's moons from Uraniborg while Cassini recorded the times they were seen in Paris. If Picard recorded the end of an eclipse at 9 hours 43 minutes 54 seconds after midday in Uraniborg, while Cassini recorded the end of the same eclipse at 9 hours 1 minute 44 seconds after midday in Paris – a difference of 42 minutes 10 seconds – the difference in longitude could be calculated to be 10° 32' 30". Picard was helped in his observations by a young Dane who had recently completed his studies at the University of Copenhagen – Ole Rømer – and he must have been impressed by his assistant's skills, as he arranged for the young man to come to Paris to work at the Royal Observatory there. Eclipses of Io. Io is the innermost of the four moons of Jupiter discovered by Galileo in January 1610. Rømer and Cassini refer to it as the "first satellite of Jupiter". It orbits Jupiter once every 42½ hours, and the plane of its orbit is very close to the plane of Jupiter's orbit around the sun. This means that it passes much of each orbit in the shadow of Jupiter – an eclipse. Viewed from the Earth, an eclipse of Io is seen in one of two ways. From the Earth, it is not possible to view both the immersion and the emergence for the same eclipse of Io, because one or the other will be hidden (occulted) by Jupiter itself. At the point of opposition (point H in the diagram below), both the immersion and the emergence would be hidden by Jupiter. For about four months after the opposition of Jupiter (from L to K in the diagram below), it is possible to view emergences of Io from its eclipses, while for about four months before the opposition (from F to G), it is possible to view immersions of Io into Jupiter's shadow. For about five or six months of the year, around the point of conjunction, it is impossible to observe the eclipses of Io at all because Jupiter is too close (in the sky) to the sun. Even during the periods before and after opposition, not all of the eclipses of Io can be observed from a given location on the Earth's surface: some eclipses will occur during the daytime for a given location, while other eclipses will occur while Jupiter is below the horizon (hidden by the Earth itself). Observations. Most of Rømer's papers were destroyed in the Copenhagen Fire of 1728, but one manuscript that survived contains a listing of about sixty observations of eclipses of Io from 1668 to 1678. In particular, it details two series of observations on either side of the oppositions of 2 March 1672 and 2 April 1673. Rømer comments in a letter to Christiaan Huygens dated 30 September 1677 that these observations from 1671–73 form the basis for his calculations. The surviving manuscript was written some time after January 1678, the date of the last recorded astronomical observation (an emergence of Io on 6 January), and so is posterior to Rømer's letter to Huygens. Rømer appears to be collecting data on eclipses of the Galilean moons in the form of an "aide-mémoire", possibly as he was preparing to return to Denmark in 1681. The document also records the observations around the opposition of 8 July 1676 that formed the basis for the announcement of Rømer's results. Initial announcement. On 22 August 1676, Cassini made an announcement to the Royal Academy of Sciences in Paris that he would be changing the basis of calculation for his tables of eclipses of Io. He may also have stated the reason: "This second inequality appears to be due to light taking some time to reach us from the satellite; light seems to take about ten to eleven minutes cross a distance equal to the half-diameter of the terrestrial orbit". Most importantly, Cassini announced the prediction that the emergence of Io on 16 November 1676 would be observed about ten minutes later than would have been calculated by the previous method. There is no record of any observation of an emergence of Io on 16 November, but an emergence was observed on 9 November. With this experimental evidence in hand, Rømer explained his new method of calculation to the Royal Academy of Sciences on 22 November. The original record of the meeting of the Royal Academy of Sciences has been lost, but Rømer's presentation was recorded as a news report in the "Journal des sçavans" on 7 December. This anonymous report was translated into English and published in "Philosophical Transactions of the Royal Society" in London on 25 July 1677. Rømer's reasoning. Order of magnitude. Rømer starts with an order of magnitude demonstration that the speed of light must be so great that it takes much less than one second to travel a distance equal to Earth's diameter. The point L on the diagram represents the second quadrature of Jupiter, when the angle between Jupiter and the Sun (as seen from Earth) is 90°. Rømer assumes that an observer could see an emergence of Io at the second quadrature (L), and also the emergence which occurs after one orbit of Io around Jupiter (when the Earth is taken to be at point K, the diagram not being to scale), that is 42½ hours later. During those 42½ hours, the Earth has moved further away from Jupiter by the distance LK: this, according to Rømer, is 210 times the Earth's diameter. If light travelled at a speed of one Earth-diameter per second, it would take 3½ minutes to travel the distance LK. And if the period of Io's orbit around Jupiter were taken as the time difference between the emergence at L and the emergence at K, the value would be 3½ minutes longer than the true value. Rømer then applies the same logic to observations around the first quadrature (point G), when Earth is moving "towards" Jupiter. The time difference between an immersion seen from point F and the next immersion seen from point G should be 3½ minutes "shorter" than the true orbital period of Io. Hence, there should be a difference of about 7 minutes between the periods of Io measured at the first quadrature and those measured at the second quadrature. In practice, no difference is observed at all, from which Rømer concludes that the speed of light must be very much greater than one Earth-diameter per second. Cumulative effect. However Rømer also realised that any effect of the finite speed of light would add up over a long series of observations, and it is this cumulative effect that he announced to the Royal Academy of Sciences in Paris. The effect can be illustrated with Rømer's observations from spring 1672. Jupiter was in opposition on 2 March 1672: the first observations of emergences were on 7 March (at 07:58:25) and 14 March (at 09:52:30). Between the two observations, Io had completed four orbits of Jupiter, giving an orbital period of 42 hours 28 minutes 31¼ seconds. The last emergence observed in the series was on 29 April (at 10:30:06). By this time, Io had completed thirty orbits around Jupiter since 7 March: the apparent orbital period is 42 hours 29 minutes 3 seconds. The difference seems minute – 32 seconds – but it meant that the emergence on 29 April was occurring a quarter-hour after it would have been predicted. The only alternative explanation was that the observations on 7 and 14 March were wrong by two minutes. Prediction. Rømer never published the formal description of his method, possibly because of the opposition of Cassini and Picard to his ideas (see below). However, the general nature of his calculation can be inferred from the news report in the "Journal de sçavans" and from Cassini's announcement on 22 August 1676. Cassini announced that the new tables would contain the inequality of the days or the true motion of the Sun the inequality due to the eccentricity of the Earth’s orbit, the eccentric motion of Jupiter the inequality due to the eccentricity of the orbit of Jupiter and this new, not previously detected, inequality due to the finite speed of light. Hence Cassini and Rømer appear to have been calculating the times of each eclipse based on the approximation of circular orbits, and then applying three successive corrections to estimate the time that the eclipse would be observed in Paris. The three "inequalities" (or irregularities) listed by Cassini were not the only ones known, but they were the ones that could be corrected for by calculation. The orbit of Io is also slightly irregular because of orbital resonance with Europa and Ganymede, two of the other Galilean moons of Jupiter, but this would not be fully explained for another century. The only solution available to Cassini and to other astronomers of his time was to issue periodic corrections to the tables of eclipses of Io to take account of its irregular orbital motion: periodically resetting the clock, as it were. The obvious time to reset the clock was just after the opposition of Jupiter to the Sun, when Jupiter is at its closest to Earth and so most easily observable. The opposition of Jupiter to the Sun occurred on or around 8 July 1676. Rømer's "aide-mémoire" lists two observation of emergences of Io after this opposition but before Cassini's announcement: on 7 August at 09:44:50 and on 14 August at 11:45:55. With these data, and knowing the orbital period of Io, Cassini could calculate the times of each of the eclipses over the next four to five months. The next step in applying Rømer's correction would be to calculate the position of Earth and Jupiter in their orbits for each of the eclipses. This sort of coordinate transformation was commonplace in preparing tables of positions of the planets for both astronomy and astrology: it is equivalent to finding each of the positions L (or K) for the various eclipses which might be observable. Finally, the distance between Earth and Jupiter can be calculated using standard trigonometry, in particular the law of cosines, knowing two sides (distance between the Sun and Earth; distance between the Sun and Jupiter) and one angle (the angle between Jupiter and Earth as formed at the Sun) of a triangle. The distance from the Sun to Earth was not well known at the time, but taking it as a fixed value "a", the distance from the Sun to Jupiter can be calculated as some multiple of "a" from Kepler's third law. This model left just one adjustable parameter – the time taken for light to travel a distance equal to "a", the radius of Earth's orbit. Rømer had about thirty observations of eclipses of Io from 1671–73 that he used to find the value which fitted best: eleven minutes. With that value, he could calculate the extra time it would take light to reach Earth from Jupiter in November 1676 compared to August 1676: about ten minutes. Initial reactions. Rømer's explanation of the difference between predicted and observed timings of Io's eclipses was widely, but far from universally, accepted. Huygens was an early supporter, especially as it supported his ideas about refraction, and wrote to the French Controller-General of Finances Jean-Baptiste Colbert in Rømer's defence. However Cassini, Rømer's superior at the Royal Observatory, was an early and tenacious opponent of Rømer's ideas, and it seems that Picard, Rømer's mentor, shared many of Cassini's doubts. Cassini's practical objections took up many debates at the Royal Academy of Sciences (with Huygens participating by letter from London). Cassini noted that the other three Galilean moons did not seem to show the same effect as seen for Io, and that there were other irregularities which could not be explained by Rømer's theory. Rømer replied that it was much more difficult to accurately observe the eclipses of the other moons, and that the unexplained effects were much smaller (for Io) than the effect of the speed of light: however, he admitted to Huygens that the unexplained "irregularities" in the other satellites were larger than the effect of the speed of light. The dispute had something of a philosophical note: Rømer claimed that he had discovered a simple solution to an important practical problem, while Cassini rejected the theory as flawed as it could not explain all the observations. Cassini was forced to include "empirical corrections" in his 1693 tables of eclipses, but never accepted the theoretical basis: indeed, he chose different correction values for the different moons of Jupiter, in direct contradiction with Rømer's theory. Rømer's ideas received a much warmer reception in England. Although Robert Hooke (1635–1703) dismissed the supposed speed of light as so large as to be virtually instantaneous, the Astronomer Royal John Flamsteed (1646–1719) accepted Rømer's hypothesis in his ephemerides of eclipses of Io. Edmond Halley (1656–1742), a future Astronomer Royal, was also an early and enthusiastic supporter. Isaac Newton (1643–1727) also appears to have accepted Rømer's ideas, and gives a value of "seven or eight minutes" for light to travel from the Sun to Earth in his 1704 book "Opticks". Newton also notes that Rømer's observations had been confirmed by others, presumably by Flamsteed and Halley in Greenwich at the very least: the value of 7–8 minutes is closer to the true value (8 minutes 19 seconds) than Rømer's initial estimate of 11 minutes. While it was obviously difficult for many (such as Hooke) to conceive of the enormous speed of light, Rømer's idea suffered a second handicap in that they were based on Kepler's model of the planets orbiting the Sun in elliptical orbits. While Kepler's model had widespread acceptance by the late seventeenth century, it was still considered sufficiently controversial for Newton to spend several pages discussing the observational evidence in favour in his "Philosophiæ Naturalis Principia Mathematica" (1687). Rømer's view that the velocity of light was finite was not fully accepted until measurements of stellar aberration were made in 1727 by James Bradley (1693–1762). Bradley, who would be Halley's successor as Astronomer Royal, calculated a value of 8 minutes 13 seconds for light to travel from the Sun to Earth. Ironically, stellar aberration had first been observed by Cassini and (independently) by Picard in 1671, but neither astronomer was able to give an explanation for the phenomenon. Bradley's work also laid to rest any remaining serious objections to the Keplerian model of the Solar System. Later measurements. Swedish astronomer Pehr Wilhelm Wargentin (1717–83) used Rømer's method in the preparation of his ephemerides of Jupiter's moons (1746), as did Giovanni Domenico Maraldi working in Paris. The remaining irregularities in the orbits of the Galilean moons would not be satisfactorily explained until the work of Joseph Louis Lagrange (1736–1813) and Pierre-Simon Laplace (1749–1827) on orbital resonance. In 1809, again making use of observations of Io, but this time with the benefit of more than a century of increasingly precise observations, the astronomer Jean Baptiste Joseph Delambre (1749–1822) reported the time for light to travel from the Sun to the Earth as 8 minutes 12 seconds. Depending on the value assumed for the astronomical unit, this yields the speed of light as just a little more than 300,000 kilometres per second. The first measurements of the speed of light using completely terrestrial apparatus were published in 1849 by Hippolyte Fizeau (1819–96). Compared to modern values, Fizeau's result (about 313,000 kilometres per second) was too high, and less accurate than those obtained by Rømer's method. It would be another thirty years before A. A. Michelson in the United States published his more precise results (299,910±50 km/s) and Simon Newcomb confirmed the agreement with astronomical measurements, almost exactly two centuries after Rømer's announcement. Modern discussion. Did Rømer measure the speed of light? Several modern discussions have suggested that Rømer should not be credited with the measurement of the speed of light, as he never gave a value in Earth-based units. These authors credit Huygens with the first calculation of the speed of light. Huygens' estimate was a value of 110,000,000 "toises" per second: as the "toise" was later determined to be just under two metres, this gives the value in modern units. However, Huygens' estimate was not a precise calculation but rather an illustration at an order of magnitude level. The relevant passage from "Treatise sur la lumière" reads: "If one considers the vast size of the diameter KL, which according to me is some 24 thousand diameters of the Earth, one will acknowledge the extreme velocity of Light. For, supposing that KL is no more than 22 thousand of these diameters, it appears that being traversed in 22 minutes this makes the speed a thousand diameters in one minute, that is 16-2/3 diameters in one second or in one beat of the pulse, which makes more than 11 hundred times a hundred thousand toises;" Huygens was obviously not concerned about the 9% difference between his preferred value for the distance from the Sun to Earth and the one he uses in his calculation. Nor was there any doubt in Huygens' mind as to Rømer's achievement, as he wrote to Colbert (emphasis added): "I have seen recently, with much pleasure, the beautiful discovery of Mr. Romer, to demonstrate that light takes time in propagating, and even to measure this time"; Neither Newton nor Bradley bothered to calculate the speed of light in Earth-based units. The next recorded calculation was probably made by Fontenelle: claiming to work from Rømer's results, the historical account of Rømer's work written some time after 1707 gives a value of 48203 leagues per second. This is 16.826 Earth-diameters (214,636 km) per second. Doppler method. It has also been suggested that Rømer was measuring a Doppler effect, and this 166 years before Christian Doppler's 1842 discovery. The Doppler effect is the change in observed frequency of an oscillator (in this case, Io orbiting around Jupiter) when the observer (in this case, on Earth's surface) is moving: the frequency is higher when the observer is moving towards the oscillator and lower when the observer is moving away from the oscillator. This apparently anachronistic analysis implies that Rømer was measuring the ratio , where "c" is the speed of light and "v" is the Earth's orbital velocity (strictly, the component of the Earth's orbital velocity parallel to the Earth–Jupiter vector), and indicates that the major inaccuracy of Rømer's calculations was his poor knowledge of the orbit of Jupiter. There is no evidence that Rømer thought that he was measuring : he gives his result as the time of 22 minutes for light to travel a distance equal to the diameter of Earth's orbit or, equivalently, 11 minutes for light to travel from the Sun to Earth. It can be readily shown that the two measurements are equivalent: if we give "τ" as the time taken for light to cross the radius of an orbit (e.g. from the Sun to Earth) and "P" as the orbital period (the time for one complete rotation), then(1) express the orbital velocity in terms of the orbital radius "r" and the orbital period "P": "v" = (2) substitute "τ" = → "v" = (3) rearrange to find .</ref> Bradley, who "was" measuring in his studies of aberration in 1729, was well aware of this relation as he converts his results for into a value for "τ" without any comment [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@12925ecb [docID]24896900 [hitPos]17 [correct]false [extraScores][F@1f89be4 , [answer]A magnetosonic wave (also magnetoacoustic wave) is a longitudinal wave of ions (and electrons) in a magnetized plasma propagating perpendicular to the stationary magnetic field. The wave is dispersionless with a phase velocity ω/"k" given by where "v"s is the speed of the ion acoustic wave, "v"A is the speed of the Alfvén wave, and "c" is the speed of light in vacuum. In the limit of low magnetic field ("v"A→0), the wave turns into an ordinary ion acoustic wave. In the limit of low temperature ("v"s→0), the wave becomes a modified Alfvén wave. Because the phase velocity of the magnetosonic mode is almost always larger than "v"A, the magnetosonic wave is often called the "fast" hydromagnetic wave. Both fast and slow magnetoacoustic waves have been recently discovered in the solar corona, which created an observational foundation for the novel technique for the coronal plasma diagnostics, coronal seismology [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@15ab6789 [docID]1904003 [hitPos]17 [correct]false [extraScores][F@5877f5f6 , [answer]The hartree (symbol: "E"h or Ha), also known as the Hartree energy, is the atomic unit of energy, named after the British physicist Douglas Hartree. It is defined as 2"R"∞"hc", where "R"∞ is the Rydberg constant, "h" is the Planck constant and "c" is the speed of light. The 2010 CODATA recommended value is "E"h = = . The 2006 CODATA recommended value was "E"h = = . The hartree energy is approximately the electric potential energy of the hydrogen atom in its ground state and, by the virial theorem, approximately twice its ionization energy; the relationships are not exact because of the finite mass of the nucleus of the hydrogen atom and relativistic corrections. The hartree is usually used as a unit of energy in atomic physics and computational chemistry: for experimental measurements at the atomic scale, the electronvolt (eV) or the reciprocal centimetre (cm−1) are much more widely used. Other relationships. where [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@15ab6789 [docID]174901 [hitPos]18 [correct]false [extraScores][F@6ffb11bd , [answer]In physics, a velocity-addition formula is an equation that relates the velocities of moving objects in different reference frames. Galilean addition of velocities. As Galileo observed, if a ship is moving relative to the shore at velocity "v", and a fly is moving with velocity "u" as measured on the ship, calculating the velocity of the fly as measured on the shore is what is meant by the addition of the velocities "v" and "u". When both the fly and the ship are moving slowly compared to light, it is accurate enough to use the vector sum where s is the velocity of the fly relative to the shore. Special theory of relativity. According to the theory of special relativity, the frame of the ship has a different clock rate and distance measure, and the notion of simultaneity in the direction of motion is altered, so the addition law for velocities is changed. This change is not noticeable at low velocities but as the velocity increases towards the speed of light it becomes important. The addition law is also called a composition law for velocities. For collinear motions, the velocity of the fly relative to the shore is given by This is also the law of addition of hyperbolic tangents where which shows that the composition of collinear velocities is associative and commutative. The quantities α and β (equal to the artanh of the velocities divided by "c") are known as rapidities. The reason that the velocities are hyperbolic tangents is because the Lorentz transformation can be thought of as the application of a hyperbolic rotation through a hyperbolic angle which is the rapidity. Suppose the velocity of a line in space-time is the slope of the line, which is the hyperbolic tangent of the rapidity, just as the slope of the "x"-axis after a rotation is given by the tangent of the rotation angle. When a plane is successively rotated by two angles, the final rotation is by the sum of the two angles. So the final slope of the "x"-axis is the tangent of the sum of the two angles. In the same way, the slope of the time axis after two boosts is the hyperbolic tangent of the sum of the two rapidities. The composition formula can take an algebraically equivalent form, which can be easily derived by using only the principle of constancy of the speed of light: The colinear law of composition of velocities gave the first test of the kinematics of the special theory of relativity. Using a Michelson interferometer, Fizeau measured the speed of light in a fluid moving parallel to the light. The speed of light in the fluid is slower than the speed of light in vacuum, and it changes if the fluid is moving along with the light. The speed of light in a colinear moving fluid is predicted accurately by the colinear case of the relativistic formula. Derivation. Since a relativistic transformation rotates space and time into each other much as geometric rotations in the plane rotate the "x" and "y" axes, it is convenient to use the same units for space and time, otherwise a unit conversion factor appears throughout relativistic formulae, being the speed of light. In a system where lengths and times are measured in the same units, the speed of light is dimensionless and equal to 1. A velocity is then expressed as fraction of the speed of light. To find the relativistic transformation law, it is useful to introduce the four-velocities and . The four-velocity is defined to be a four vector with relativistic length equal to 1, future-directed and tangent to the spacetime path of the object. Here, "V"0 corresponds to the time component and "V"1 to the x component of the fly's four-velocity as seen by the ship. It is convenient to take the "x"-axis to be the direction of motion of the ship, and the "y"-axis so that the "x"–"y" plane is the plane spanned by the motion of the ship and the fly. This results in several components of the velocities being zero: "V"2 = "V"3 = "U"3 = 0. The ordinary velocity is the ratio of the rate at which the space coordinates are increasing to the rate at which the time coordinate is increasing: Since the relativistic length of "V" is 1, so The Lorentz transformation matrix that boosts the rest frame to four-velocity "V" is then: This matrix rotates the a pure time-axis vector to , and all its columns are relativistically orthogonal to one another, so it defines a Lorentz transformation. If a fly is moving with four-velocity in the rest frame, and it is boosted by multiplying by the matrix above, the new four-velocity is : Dividing by the time component "S"0 and replacing the four-vectors for "U" and "V" by the three-vectors "u" and "v" gives the relativistic composition law: The form of the relativistic composition law can be understood as an effect of the failure of simultaneity at a distance. For the parallel component, the time dilation decreases the speed, the length contraction increases it, and the two effects cancel out. The failure of simultaneity means that the fly is changing simultaneity slices as the projection of "u" onto "v". Since this effect is entirely due to the time slicing, the same factor multiplies the perpendicular component, but for the perpendicular component there is no length contraction, so the time dilation multiplies by a factor of . Vector notation. To translate the formula of the previous section to three-vector notation, replace "u"1 with the component of "U" parallel to "V": Special case: parallel velocities. In the case where the velocities are parallel we have and, expressed in terms of the speeds: Special case: orthogonal velocities. In the case where the velocities are orthogonal we have and, expressed in terms of the speeds: General case (engineering units, replaced V with v / c ). In the general case, the relativistic sum of two velocities v and u is given by where formula_26 and formula_27 are the components of u parallel and perpendicular, respectively, to v, and the equation may easily be transformed to the form used by Ungar Using coordinates this becomes: where formula_31. Einstein velocity addition is commutative "only" when u and v are "parallel". In fact Also it is not associative and where "gyr" is the mathematical abstraction of Thomas precession into an operator called Thomas gyration and given by for all w. The gyr operator forms the foundation of gyrovector spaces. Velocity composition paradox. Since in general u⊕v ≠ v⊕u this raises the question as to which velocity is the real velocity. The paradox is resolved as follows. There are two types of Lorentz transformation: boosts which correspond to a change in velocity, and rotations. The outcome of a boost followed by another boost is not a pure boost but a boost followed by or preceded by a rotation (Thomas precession). So unlike Galilean composite transformations, in special relativity, boost composition is parameterized not by velocities alone, but by velocities and orientations, so u⊕v and v⊕u both describe correctly but partially the boost composition "B"(u)"B"(v). If the 3 × 3 matrix form of the rotation applied to 3-coordinates is given by gyr[u,v], then the 4 × 4 matrix rotation applied to 4-coordinates is given by: If "B"(u)"B"(v) is parameterized by u⊕v, the rotation Gyr[u,v] associated with the composite boost "B"(u)"B"(v) is applied "before" the boost "B"(u⊕v), whereas if "B"(u)"B"(v) is parameterized by v⊕u, the boost B(v⊕u) of v⊕u is "followed" by the rotation Gyr[u,v], so we get: In the above, a boost can be represented as a 4 × 4 matrix. The boost matrix "B"(v) means the boost B that uses the components of v, i.e. "v"1, "v"2, "v"3 in the entries of the matrix, or rather the components of v/c in the representation that is used in the section Matrix forms in the article Lorentz transformation. The matrix entries depend on the components of the 3-velocity v, and that's what the notation "B"(v) means. It could be argued that the entries depend on the components of the 4-velocity because 3 of the entries of the 4-velocity are the same as the entries of the 3-velocity, but the usefulness of parameterizing the boost by 3-velocity is that the resultant boost you get from the composition of two boosts uses the components of the 3-velocity composition u⊕v in the 4 × 4 matrix B(u⊕v). Doppler shift. A notion of velocity addition can also be formulated in the theory of the nonrelativistic, one dimensional Doppler shift. When the source of a wave is moving with nonrelativistic velocity "s" toward the receiver, the frequency of the waves is increased by a factor of 1/(1 − "s"/"c"). If the receiver is moving with velocity "v", the frequency of the waves detected is decreased by a factor of (1 − "v"/"c"). When both the source and the receiver are moving, the frequency measured is given by: If a receiver measures velocities using Doppler shifts, and it determines that an object coming towards it is moving with velocity "u", it is actually determining the shift in frequency, from which it calculates the velocity. Suppose that the receiver itself is moving with velocity "v", but it does not take this into account in the calculation. It calculates the value "u" falsely assuming that it is at rest. The velocity "u" can then be thought of as the inferred velocity relative to the ship from Doppler shifts alone. What, then, is the actual velocity of the object relative to the medium? Since the ship determined "u" from the frequency, the frequency shift factor relative to the ship is But this factor is not the frequency shift relative to a stationary receiver. For a stationary observer, it must be corrected by dividing by the frequency shift of the ship: The velocity of the object relative to the medium is then given by This is the true velocity of the object. Unlike the relativistic addition formula, the velocity "u" is not the physical velocity of the object. There is a group of transformations in one space and one time dimension for which this operation forms the addition law. The group is defined by all matrices: When they act on formula_42, they produce the transformations which is a Galilean boost accompanied by a rescaling of the "x" coordinate. When two of these matrices are multiplied, the quantity "v" (the velocity of the frame), combines according to the Doppler addition law. The physical meaning can be extracted from the transformation. Time is the same for both frames, but the rescaling of the "x" axis keeps the right-moving speed of sound fixed in the moving frame. This means that if the ship uses this transformation to define its frame, the ruler that it uses is the distance that the waves move to the right in one unit of time. The velocity "u" can now be given a physical interpretation, although an unusual one. It is the velocity of the object as measured from the ship using a Doppler contracted ruler. Relativistic Doppler shift. In the theory of the relativistic Doppler shift, the case where the speed of the wave is equal to the speed of light is special, because then there is no preferred rest-frame. In this case the frequency of the received waves can only depend on the relativistic sum of the velocities of the emitter and the receiver. But when the speed of the wave "c" ≠ 1, meaning that the phase velocity of the wave is different from that of light, the relativistic Doppler shift formula does not depend only on the relative velocities of the emitter and receiver, but on their velocities with respect to the medium. In the rest frame of the medium, the frequency emitted by a relativistic source moving with velocity "v" is decreased by the time dilation of the source: If the receiver is moving with a velocity "u" through the fluid perpendicular to the wave fronts, the frequency received is determined by the proper time between the events where the receiver crosses crests. The fluid frame time between crest-crossings does not require changing frames and is the same as in the nonrelativistic case: In this time, the receiver has moved (in the fluid frame) an amount And the proper time between the two crest crossing is And this is the time between crest-crossings as measured by the receiver. From this, the received frequency can be read off: Multiplying the two factors for the emitter and receiver gives the relativistic Doppler shift: When "c" = 1, it simplifies: and then so that the relativistic Doppler shift of light is determined by the relativistic difference of the two velocities. It is also possible to determine, in the relativistic case, the actual velocity of a source, when a moving ship falsely determines it from a Doppler shift without taking its own motion into account. Just as in the non-relativistic case, this is the velocity at which a source would have to be moving in order to make the Doppler shift factor for a moving receiver equal to the Doppler shift factor for the velocity "u". It is the solution of the equation: This is the relativistic analog of the Doppler velocity addition formula. When "c" is not the speed of light, the velocity "u" is not the velocity of anything, just a false inferred velocity from the point of view of the moving ship. In the relativistic case, there is no group of transformations for which this is the velocity addition law, since it is impossible to independently rescale time and distance measurements [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@58777339 [docID]1437696 [hitPos]18 [correct]false [extraScores][F@5ccd9791 , [answer]Various terms are used for passenger rail lines and equipment-the usage of these terms differs substantially between areas: Rapid transit. A rapid transit system is an electric railway characterized by high speed and rapid acceleration. It uses passenger railcars operating singly or in multiple unit trains on fixed rails. It operates on separate rights-of-way from which all other vehicular and foot traffic are excluded (i.e. is fully grade separated from other traffic). It uses sophisticated signaling systems, and high platform loading. Originally, the term rapid transit was used in the 1800s to describe new forms of quick urban public transportation that had a right-of-way separated from street traffic. This set rapid transit apart from horsecars, trams, streetcars, omnibuses, and other forms of public transport. Though the term was almost always used to describe rail transportation, other forms of transit were sometimes described by their proponents as rapid transit, including local ferries in some cases. The term "bus rapid transit" has recently come into use to describe bus lines with features to speed their operation. These usually have more characteristics of light rail than rapid transit. Metro. Metro, short for metropolitan railway refers to an urban, electric rail transport system with high capacity and a high frequency of service. Metros are totally separated from other traffic. They operate in tunnels, on elevated structures, or at surface level but with physical separation from other traffic. Metropolitan railways are used for high capacity public transportation - they can operate in trains of up to 10 cars, carrying 1800 passengers or more. In different parts of the world metro systems are also known as the "underground", "subway", or "tube". In Germany the terms "U-Bahn" and "S-Bahn" are used. Some metro systems run on rubber tires, but are based on the same fixed-guideway principles as steel wheel systems. Subway. Subway used in a transit sense refers to either a rapid transit system or a light rail/streetcar system that goes underground. The term may refer only to the underground parts of the system, or to the full system. "Subway" is most commonly used in the United States and the English-speaking parts of Canada, though the term is also used elsewhere, such as to describe the subway line in Glasgow, Scotland, and in translation of system names or descriptions in some Asian and Latin American cities. Some lines described as "subway" use light rail equipment. Notably, Boston's Green Line and the Newark City Subway, each about half underground, originated from fully surface streetcar lines. Also, the Buffalo Metro Rail is referred to as "the subway", while it uses light rail equipment and operates in a pedestrian mall downtown for half of its route and underground for the remaining section. Sometimes the term is qualified, such as in Philadelphia, where trolleys operate in an actual subway for part of their route and on city streets for the remainder. This is locally styled "subway-surface". In some cities where "subway" is used, it refers to the entire system; in others, only to the portions that actually are underground. Naming practices often select one type of placement in a system where several are used; there are many "subways" with above-ground components, and on the other hand, the Vancouver SkyTrain and Chicago 'L' include underground sections. Interestingly, when the Boston subway was originally built, the "subway" label was only used for sections into which streetcars (trams) operated, and the rapid transit sections were called "tunnels". Also, in some countries, "subway" refers to systems built under roads (such as the Glasgow Subway or London's Metropolitan Line) and the informal term "tube" is used for the deep-underground tunnelled systems (such as London's Piccadilly Line) - in this usage, somewhat technical nowadays and not used much in London, "underground" is regardless the general term for both types of system. Bus subways are uncommon but do exist, though in these cases the non-underground portions of route are not called subways. Seattle, Washington, has a bus subway downtown, in which light rail trains and diesel-electric hybrid buses operate in a shared tunnel, with overhead wires which power the light rail trains, and the hybrid buses running in electrical-only mode while traveling through the tunnel. Bus subways are sometimes built to provide an exclusive right-of-way for bus rapid transit lines, such as the MBTA Silver Line in Boston. These are usually called by the term "bus rapid transit". 'Subway' outside the USA, and especially in Europe often refers to an underground pedestrian passageway linking large road interconnections that are often too difficult or dangerous to cross at ground level. In Canada, the term "subway" may be used in either sense. Underground and Tube. The usage of underground is very similar to that of subway, describing an underground train system. In London the colloquial term tube now refers to the London Underground and is the most common word used for the underground system, and it is used by Transport for London the local government body responsible for most aspects of the transport system throughout Greater London. However, strictly speaking, it should only refer to those deep lines which run in bored circular tunnels as opposed to those constructed near to the surface by 'cut-and-cover' methods. The Glasgow metro system is known as the Glasgow Subway or colloquial as "the subway". The word "Metro" is not usually used in London or Glasgow to refer to those cities' metros, but it is used in and around Newcastle upon Tyne to refer to the Tyne and Wear Metro. Paris, Rome, Madrid, Barcelona, Copenhagen, Helsinki, Warsaw, Saint Petersburg, Rotterdam and Moscow all have metro (from the word metropolitan) systems which are called metro in French, Italian, Spanish, Danish, Finnish, Polish, Dutch and Russian. U-Bahn and S-Bahn. The term metro is not usually used to describe metro systems in German-speaking areas (Germany, Austria and parts of Switzerland), instead using the term "U-Bahn"—a shortening of "Untergrundbahn", meaning "underground railway"—and S-Bahn—an abbreviation for the German ""Stadtschnellbahn"" (fast city train). So for example in Berlin, the mostly underground system is known as the Berlin U-Bahn and it is integrated with the mostly above-ground system, known as the Berlin S-Bahn. BVG, the operators of the Berlin U-Bahn system, describe the U-Bahn as "the largest metro system in Germany" and the S-Bahn system as an "urban rail system". The same applies also to the S-Bahn and U-Bahn in Copenhagen, Denmark, with the only exception that the word "Metro" is used instead of "U-Bahn" and "S-tog" instead of "S-Bahn". (The Danish word "S-tog" applies to the trains ("tog"), rather than the tracks as in Germany; "S-tog" means "S-train".) Otherwise, the S-Bahn of Berlin and the S-tog of Copenhagen are very similar with the exception of the size. Hamburg S-Bahn fulfills all criteria for heavy rail inside the state and city of Hamburg, but some lines go beyond the state border into the state of Niedersachsen and there the S-Bahn runs with lower train frequency. In Switzerland, where there is only one underground railway system in Lausanne, the term metro is generally used, due to the influence from the French language. In Sweden, the metro of Stockholm is called "Tunnelbana" or "T-bana" which applies to the fact that the trains often runs in tunnels. The same applies to Norway and the "T-bane" of Oslo. Elevated and Overhead. Elevated is a shorthand for elevated railway, a railway built on supports over other rights of way, generally city streets. The term "overhead" tends to be used in Europe. The names of elevated railways are sometimes further abbreviate it to El or L. Some examples include: Heavy rail. A heavy rail system is an electric railway with the capacity to handle a heavy volume of traffic. The term is often used to distinguish it from light rail systems, which usually handle a smaller volume of passengers. In Britain, heavy rail refers to regional rail, international rail (Eurostar) and intercity rail services as distinct from other rapid transit or light rail modes, such as when referring to National Rail services in London. In North America, heavy rail can also refer to rapid transit, when referring to systems with heavier passenger loadings than light rail systems, but distinct from commuter rail and intercity rail systems. It is characterized by high-speed, passenger rail cars running in separate rights-of-way from which all other vehicular and foot traffic are excluded. At-grade urban rail transit. Tram, streetcar, trolley. The terms tram, streetcar and trolley refer to most forms of common carrier rail transit that run entirely or partly on streets, providing a local service and picking up and discharging passengers at any street corner, unless otherwise marked. While tram or "tramway" are widely used worldwide, the term used varies in different dialects of English, with "streetcar" and "trolley" most commonly used in North America ("streetcar" being more common in the western and central part of the continent and "trolley" in the eastern part), while "tram" predominates in Europe and elsewhere. Tram is a British word derived from Low German "traam", meaning the "beam (of a wheelbarrow)". The term "tram" was originally used in the coal mines of Scotland and Northern England for a coal cart running on rails, although some sources claim (inaccurately) that it was derived from the name of engineer Benjamin Outram. Streetcar is an American word derived from "street" + "car", where "car" is used in the sense of a vehicle running on rails, i.e. railway car. The first American streetcars, introduced around 1830, were horsecars, and this type of streetcar became ubiquitous because very few of the streets in American cities were paved. Mechanical versions, pulled by cables, were introduced around 1870. Electric streetcars were introduced in the 1880s and soon replaced the horse-drawn streetcar in cities across the United States. Trolley is an American word derived from the electric current pickup mechanism in early systems. The first successful electric streetcars in the United States used a system devised by Frank J. Sprague, in which a spring-loaded trolley pole pushed a small trolley wheel up against an overhead wire to collect electricity for the motors. Although not the first overhead collection system, it was far more reliable than its predecessors, and eventually became used by almost all streetcars. Some authorities believe that the vehicle became known as a "trolley car" because it reminded people (particularly on the West Coast) of a boat trolling for fish. Others believe it derived from a dialect word for a wheeled cart. In the U.S. the word "tram" frequently refers to a tourist bus with the appearance of a heritage streetcar, cable car, or rubber-tired people-mover. They are frequently used for parking lot shuttles at theme parks and major events or transportation within theme parks. "Trolley" can sometimes carry similar meaning, as in the RiverCity Trolley in Minneapolis, Minnesota. Historical systems. Specific terms for some historically important tram technologies include "horsecar", "heritage streetcar", and "cable car". Heritage streetcar (also known as heritage trolley or vintage trolley) is an American term for streetcar systems that use vehicles that were built before 1960, or modern replicas of such vehicles. Cable car is an American word for a passenger rail vehicle attached to a moving cable located below the street surface and powered by engines or motors at a central location, not on board the vehicle. There are cable cars operating in numerous cities, such as San Francisco, California. In the Tennessee Williams play "A Streetcar Named Desire", the term "streetcar" is used allegorically to refer to Blanche DuBois' promiscuousness and inability to form permanent relationships, as in the sarcastic phrase: "Men (or women) are like streetcars. There'll be another one along any minute." There was actually a streetcar line in New Orleans named "Desire Street" and simply signed "Desire". It is mentioned in the book and an actual New Orleans streetcar with that signage is seen at the beginning of the Marlon Brando-Vivien Leigh film. Light railway. A "light railway" is a British English term referring to a railway built at lower costs and to lower standards than typical "heavy rail". These lighter standards allow lower costs of operation at the price of slower operating speeds and lower vehicle capacity. They were permitted under the Light Railways Act 1896 and intended to bring railways to rural areas. The London Docklands Light Railway, has more rapid transit style features than would be typical of "light rail" systems, but fits within the U.K. "light railway" definition. Light rail. A light rail transit (LRT) system is an urban rail transit system with a "light" passenger capacity compared to heavy rail and metro systems. Its operating characteristics are that it uses railcars, called light rail vehicles (LRVs), operating singly or in short multiple unit trains on fixed rails in a right-of-way that is not necessarily grade separated from other traffic for much of the way. Light rail vehicles are almost always electrically driven, with power usually being drawn from an overhead line rather than an electrified third rail, though a few exceptional systems use diesel multiple units (DMUs) instead as a cheaper alternative to an electrically driven light rail system. The phrase light rail was coined in the 1970s during the re-emergence of streetcars/trams with more modern technology. It was devised in 1972 by the U.S. Urban Mass Transportation Administration (UMTA; the precursor to the Federal Transit Administration) to describe new streetcar transformations which were taking place, and was a translation of the German word stadtbahn. However, instead of the literal translation of "city rail", the UMTA used "light rail" instead. In general, it refers to streetcar/tram systems with rapid transit-style features. It is named to distinguish it from "heavy rail", which refers to rapid transit systems as well as heavier regional rail/intercity rail. A few systems such as people movers and personal rapid transit could be considered as even "lighter", at least in terms of how many passengers are moved per vehicle and the speed at which they travel. Monorails are a separate technology. Light rail systems can typically handle steeper inclines than heavy rail, and curves sharp enough to fit within street intersections. They are typically built in urban areas, providing frequent service with multiple-unit trains or single cars. The most difficult distinction to draw is that between light rail and streetcar/tram systems. There is a significant amount of overlap between the technologies, and it is common to classify streetcars/trams as a subtype of light rail rather than as a distinct type of transportation. The two general versions are: Many light rail systems — even fairly old ones — have a combination of the two, with both on-road and off-road sections. In some countries, only the latter is described as "light rail". In those places, trams running on mixed right of way are not regarded as light rail, but considered distinctly as streetcars or trams. However, the requirement for saying that a rail line is "separated" can be quite minimal — sometimes just with concrete "buttons" to discourage automobile drivers from getting onto the tracks. There is a significant difference in cost between these different classes of light rail transit. The traditional style is often less expensive by a factor of two or more. Despite the increased cost, the more modern variation (which can be considered as "heavier" than old streetcar systems, even though it's called "light rail") is the dominant form of new urban rail transit in the United States. The Federal Transit Administration helps to fund many projects, but as of 2004, the rules to determine which projects will be funded are unfavorable toward the simpler streetcar systems (partly because the vehicles tend to be somewhat slower). Some places in the country have set about building the less expensive streetcar lines themselves or with only minimal federal support. Most of these lines have been "heritage" railways, using refurbished or replica streetcars harkening back to the first half of the 20th century. However, a few, such as the Portland Streetcar, use modern vehicles. There is a growing desire to push the Federal Transit Administration to help fund these startup lines as well. Light rail is generally powered by electricity, usually by means of overhead wires, but sometimes by a live rail, also called third rail (a high voltage bar alongside the track), requiring safety measures and warnings to the public not to touch it. In some cases, particularly when initial funds are limited, diesel-powered versions have been used, but it is not a preferred option. Some systems, such as AirTrain JFK in New York City, are automatic, dispensing with the need for a driver; however, such systems are not what is generally thought of as light rail, crossing over into rapid transit. Automatic operation is more common in smaller people mover systems than in light rail systems, where the possibility of grade crossings and street running make driverless operation of the latter inappropriate. Interurban. In the U.S., interurban (German "Überland(straßen)bahn") refers to a higher-speed streetcar (tram) line – i.e. electrical railcars or trains which run both between the cities or towns (often in rural environments) on their own right-of-way, and through the city streets as trams. In the U.S., some interurban railcars constructed in the period 1900-1930 ran at extremely high speed for its time. Several advanced innovations – like streamlining, wind tunnel research and lightweight constructions – have their origin on the interurban scene, or were early adopted by companies like J. G. Brill Company, Cincinnati Car Company, and St. Louis Car Company. The fastest interurbans had a maximum service speed at 145–150 km/h, and an average speed including stops at above 80 km/h. The Cincinnati–Toledo route of Cincinnati and Lake Erie Railroad was . A few interurbans like Philadelphia and Western Railroad adapted to high speeds with double-track, absolute block signalling and without grade crossings. Others ran at (too) high speed on single-track right-of-way without block signalling – and experienced disastrous wrecks.         The U.S. interurbans are all but gone, with two of the remaining (Norristown High Speed Line, IRT Dyre Avenue Line) having been upgraded to rapid transit specifications, and a third system (Cleveland's Blue & Green Lines) now considered to be light rail. The South Shore Line, which runs from Chicago's Millennium Station to South Bend, Indiana, has been converted to modern electric rapid-transit operation on the dense corridor between Chicago and Gary, Indiana but still runs essentially as an interurban through several small towns between Gary and South Bend. The European interurbans, like the Silesian Interurbans (Tramwaje Śląskie S.A.; German "Schlesische Straßenbahn") and Belgium’s "Kusttram", were (and are) more like conventional tramways, as their names indicate. Interurbans sometimes used freight railways rather than building their own track. In Australia, "interurban" refers to long distance commuter trains such as the routes between Newcastle and Sydney, between Brisbane and Gympie, or between Brisbane and the Gold Coast. Some interurban trains may operate from where suburban lines end, such as Southern Higlands services between Campbelltown and Goulburn, or between Ipswich and Rosewood. These do not have the features of "intercity trains" in other parts of the world, such as booked seats and meal services, but are bare commuter trains. They are properly called interurban rather than intercity, although CityRail refers to its interurban services as "intercity" trains. Tram-train. Tram-trains are railcars or trains which run like trams (streetcars) in city streets, and on heavy rail tracks out to the suburbs or between the cities. Usually, this requires two current systems (German "Zweisystemstadtbahn", "Stadtbahn" with two systems), both the tram voltage (600 or 750 V DC) and the heavy rail high voltage (in Germany, 15 kV AC). The vehicles must also be adapted to the heavy rail’s signalling system. This transit mode combines the tram’s availability with stops in the street, and the heavy rail’s higher speed. They are often faster than most rapid transit (metro) systems. The first system was opened in Karlsruhe in 1992. Their top speed is often 100 km/h, in Kassel as much as 114 km/h. Most of the tram-trains fit the definition of an interurban; in reality, this transit mode is a rebirth of the interurban. Regional rail and Commuter rail. Regional rail (also called metropolitan rail, commuter rail, or suburban rail) is an electric or diesel propelled railway for urban passenger train service consisting of local short distance travel operating between adjacent cities and towns, or between a central city and adjacent suburbs, using either locomotive hauled or multiple unit railroad passenger cars. Regional rail usually provides rail services between towns and cities, rather than purely linking major population hubs in the way inter-city rail does. Regional rail operates outside major cities. Unlike Inter-city, it stops at most or all stations. It provides a service between smaller communities along the line, and also connections with long-distance services. Alternative names are "local train" or "stopping train". Examples include the former BR's Regional Railways, France's TER ("Transport express régional") and Germany's DB Regio services. Regional rail operates throughout the day but often at low frequency (once per hour or only a few times a day), whereas commuter rail provides a high-frequency service within a conurbation. Regional rail in this sense does not exist in North America, where the term "regional rail" is synonymous with commuter rail. Regional trains are usually all seated and provide luggage space, although they seldom have all the amenities of inter-city trains such as a buffet or dining car. Since their invention, the distinction between regional and long-distance rail has also been the use of multiple unit propulsion, with longer distance trains being locomotive hauled, although development of trains such as the British Rail Class 390 have blurred this distinction. Shorter regional rail services will still usually be operated exclusively by multiple units where they exist, which have a shorter range and operate at lower average speeds than services on Inter-city rail networks. Not using a locomotive also provides greater passenger capacity in the commuter role at peak periods. British Rail, during sectorisation, did once create a "Regional Railways" subsidiary, however this was so named to differentiate it's 'all other regions' lines from the other sectors Network SouthEast, which heavily focused on commuters services to London terminal stations but operated rail services across the South East region, and the Inter-City sector which operated long distance services. Commuter rail in North America refers to urban passenger train service for local short-distance travel operating between a central city and its suburbs. Such rail service, using either locomotive-hauled or self-propelled railroad passenger cars, is characterized by multi-trip tickets, specific station-to-station fares, and usually only one or two stations in the central business district. It does not include heavy rail, rapid transit, light rail, streetcar, tram, or intercity rail service. Intercity, Corridor and Long-Distance. The Inter-City was a British named train, introduced in 1950, but the term InterCity was adopted by British Rail in 1966 as a brand-name for its long-haul express passenger services. Since then, the terms Inter-city or intercity have been widely adopted to refer to express passenger train services that cover longer distances than commuter or regional trains. In the US, "Corridor" services refer to routes connecting relatively nearby cities, where one city can be visited from another without staying overnight. "Long-Distance" refers to routes which cover vast rural distances. Other types of rail transit. Automated guideway transit refers to guided transit vehicles operating singly or in multi-car trains with fully automated control (no crew on transit units). Service may be on a fixed schedule or in response to a passenger-activated call button. Automated guideway transit includes personal rapid transit, group rapid transit and people mover systems. Personal rapid transit (PRT), also called personal automated transport (PAT), is a public transportation concept that offers on-demand, non-stop transportation, using small, independent vehicles on a network of specially built guideways. People mover or automated people mover (APM) systems are fully automated, grade-separated mass transit systems which serve a relatively small area such as an airport, downtown district or theme park. The term "people mover" has become generic for the type of system, which may use technologies such as monorail, duorail, automated guideway transit or maglev. Monorail means a system of guided transit vehicles operating on or suspended from a single rail, beam, or tube. Usually they operate in trains. Monorails are distinguished from other types of elevated rail system by their use of only a single beam, and from light rail and tram systems by the fact they are always grade separated from other vehicles and pedestrians. Suspension railway is a form of elevated monorail where the vehicle is suspended from a fixed track (as opposed to a cable used in aerial tramways), which is built above street level, over a river or canal, or an existing railway track. Service type. Local service. Means trains stop at every station on a route. For light rail vehicles operating on city streets, local service is analogous to local bus service, where stops are every block or two apart. Regional service. Regional passenger trains may be classified as either slow or stopping trains, or else limited-stop or semi-fast trains, where not all stations and stops are served. For example a pair of closely spaced trains may both stop at the most heavily used stations. For lesser-used stations, the first train stops at alternate stations, while the following train stops at the stations missed by the first train. or else express trains. Express service. Means trains operate for long distances without stopping, skipping some stations between stops. This speeds up longer trips, especially in major urban areas. In major cities, express trains may have separate tracks for at least part of their routes. Passenger boarding. Street-level boarding. Used primarily by light rail and tram lines that stop on the street rather than at stations. No platforms are used, the passengers walk up steps into the vehicles. For wheelchairs, a retractable lift or ramp is required to gain access to the vehicle. Low-level platforms. Generally about above track level and are used primarily by some commuter rail and light rail lines. Wheelchairs can board low-floor vehicles directly from the platform, but high-floor vehicles require retractable lift or ramp. High-level platforms. Generally above track level and are used primarily by heavy rail, automated guideway, and some commuter rail lines. Only high-floor vehicles can be used, but wheelchairs can board directly from platforms if vehicle floors are level with the platform. Rail terminology with regard to speed. Non-high-speed rail: Less than 200 km/h. The vast majority of local, regional and express passenger trains, and almost 100% of freight trains are of this category. High-speed rail: 200 km/h – 400 km/h. There is no globally accepted standard separating high-speed rail from conventional railroads; however a number of widely accepted variables have been acknowledged by the industry in recent years. Generally, high-speed rail is defined as having a top speed in regular use of over . Although almost every form of high-speed rail is electrically driven via overhead lines, this is not necessarily a defining aspect and other forms of propulsion, such as diesel locomotives, may be used. A definitive aspect is the use of continuous welded rail which reduces track vibrations and discrepancies between rail segments enough to allow trains to pass at speeds in excess of . Track radius will often be the ultimate limiting factor in a train's speed, with passenger discomfort often more imminent than the danger of derailment. Depending on design speed, banking and the forces deemed acceptable to the passengers, curves often exceed a 5 kilometer radius. Tilting trains have been developed for achieving greater comfort for passengers, so higher speeds are possible on curvy tracks. Although a few exceptions exist, zero grade crossings is a policy adopted almost worldwide, with advanced switches utilizing very low entry and frog angles. Magnetic levitation trains fall under the category of high-speed rail due to their association with track oriented vehicles; however their inability to operate on conventional railroads often leads to their classification in a separate category. In the US, "high speed rail" is often used to describe services faster than . This is because few regular services in the US exceed (with the exception of Acela Express), which is low by international standards. Very high-speed rail: 310 km/h – 500 km/h. Very high-speed rail is a term used for the fastest trains introduced after 2000, exceeding 300 km/h. It is planned that 350 km/h will be reached in short future with regular trains, and possibly 400 km/h on long term. Ultra high-speed rail: 500 km/h – 1000 km/h. A number of both technological and practical variables begin to influence trains in the vicinity of 500–600 km/h. Technologically the limitations are by no means beyond reach, however conventional trains begin to encounter several physical obstacles, most notably track damage and pantograph limitations. It is important to note that the current world record for rail vehicles is held by the TGV V150 set on 15 April 2007 at 574.8 km/h, and conventional trains may indeed eventually reach into very high-speeds. However, this test has shown that speeds over 500 km/h are unrealistic for regular usage; it wears down the material too much. Based on current and foreseeable technology, these speeds will more than likely be reached predominantly by maglev trains. The two most prominent maglev trains are the Transrapid with a maximum speed of 550 km/h; and the Japanese MLX01, which holds the world land speed record for rail vehicles at 581 km/h. Trains faster than 600 km/h will exceed the speed of most propeller-driven aircraft. Regardless of technological parameters, the track for such a train and anything faster would more than likely require turn radii of significantly higher proportions than current dimensions, essentially preventing anything but a direct line between terminals. Such trains are extremely unlikely in the current or near future. Greater than 1000 km/h. Depending on the aerodynamic design of the vehicle and various ambient atmospheric conditions, a train would begin to exhibit transonic airflow in the vicinity of Mach 0.8 (988 km/h) and higher. From a modern perspective, this is essentially the realistic maximum speed of trains as they are known today. This is because the Prandtl-Glauert singularity would cause catastrophic damage to the vehicle as the sound waves reflected off of the ground, potentially blasting the train into the air. The only trains that could exceed this speed significantly are vactrains. Rail terminology with respect to rail gauge. Approximately 60% of the world's existing railway lines are built to the standard rail gauge where the distance between the inside edges of the rails of the track is (see the list of countries that use the standard gauge). Narrow gauge railways have rail gauges of between and . They are cheaper to build and operate, but tend to be slower and have less capacity. Minimum gauge railway have a gauge of less than or and are primarily used as industrial railways rather than for passenger transit. However many miniature railways use this type of gauge. Broad gauge railways use a rail gauge greater than . Examples include Russian gauge (), Pennsylvania trolley gauge (), and Indian gauge [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@61f25b53 [docID]1780643 [hitPos]19 [correct]false [extraScores][F@d36d8a3 , [answer]The Paczyński–Wiita potential is an approximation of the gravitational potential around a non-rotating black hole. It was introduced by Bohdan Paczyński and Paul Wiita in 1980. The article is one of the 40 most-cited from the first 40 years of the journal Astronomy & Astrophysics. The mathematical form of the potential is where formula_2 is the radial distance from the black hole, formula_3 is the gravitational constant, formula_4 is the mass of the black hole, and formula_5 is its Schwarzschild radius. (formula_6 is the speed of light.) The potential exactly reproduces the locations of the innermost stable circular orbit and the marginally bound orbit. It also exactly reproduces the form of the angular momentum and accurately approximates the Keplerian angular velocity and epicyclic frequency. Because the Paczyński–Wiita potential reproduces these general relativistic effects and is easy to calculate, it is widely used in numerical simulations of black hole accretion [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@15ab6789 [docID]31475901 [hitPos]19 [correct]false [extraScores][F@16c4d6fa , [answer]The speed of light is a physical constant. Lightspeed may also refer to: In arts and literature: Organizations [score]-Infinity [normScore]0.0 [query]info.ephyra.querygeneration.Query@58777339 [docID]6775924 [hitPos]19 [correct]false [extraScores][F@2dfe1fa6 ]
